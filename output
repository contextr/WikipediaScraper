[{"text":"","label":"foo"},{"text":"Physics is the science of all phenomena. Physics may also refer to: Physics (Aristotle), a key text in the philosophy of Aristotle Physics (band), an American rock music group Physics (American Physical Society journal), a scientific journal published by the American Physical Society Physics (Chinese Physical Society journal), a scientific journal published by the Chinese Physical Society Other articles: Geophysics Mathematical physics Newtonian physics in reference to classical mechanics Mesoscopic physics Nobel Prize in Physics Particle physics Physics in medieval Islam Quantum physics is known as quantum mechanics. Theoretical physics Fluid dynamics PhysX for NVIDIA's physics engine for computer games:    See also  All pages beginning with \"physics\" All pages with titles containing \"physics\"","label":"foo"},{"text":"The following outline is provided as an overview of and topical guide to physical science: Physical science – branch of natural science that studies non-living systems, in contrast to life science. It in turn has many branches, each referred to as a \"physical science\", together called the \"physical sciences\". However, the term \"physical\" creates an unintended, somewhat arbitrary distinction, since many branches of physical science also study biological phenomena (organic chemistry, for example).    What is physical science?  Physical science can be described as all of the following: A branch of science – systematic enterprise that builds and organizes knowledge in the form of testable explanations and predictions about the universe.A branch of natural science – natural science is a major branch of science, that tries to explain and predict nature's phenomena, based on empirical evidence. In natural science, hypotheses must be verified scientifically to be regarded as scientific theory. Validity, accuracy, and social mechanisms ensuring quality control, such as peer review and repeatability of findings, are amongst the criteria and methods used for this purpose. Natural science can be broken into 2 main branches: life science, and physical science. Each of these branches, and all of their sub-branches, are referred to as natural sciences.    Branches of physical science  Physics – natural and physical science that involves the study of matter and its motion through space and time, along with related concepts such as energy and force. More broadly, it is the general analysis of nature, conducted in order to understand how the universe behaves.Branches of physics  Astronomy – study of celestial objects (such as stars, galaxies, planets, moons, asteroids, comets and nebulae), the physics, chemistry, and evolution of such objects, and phenomena that originate outside the atmosphere of Earth, including supernovae explosions, gamma ray bursts, and cosmic microwave background radiation. Branches of astronomy  Chemistry – studies the composition, structure, properties and change of matter. In this realm, chemistry deals with such topics as the properties of individual atoms, the manner in which atoms form chemical bonds in the formation of compounds, the interactions of substances through intermolecular forces to give matter its general properties, and the interactions between substances through chemical reactions to form different substances. Branches of chemistry  Earth science – all-embracing term referring to the fields of science dealing with planet Earth. Earth science is the study of how the natural environment (ecosphere or Earth system) works and how it evolved to its current state. It includes the study of the atmosphere, hydrosphere, lithosphere, and biosphere. Branches of earth science    History of physical science  History of physical science – history of the branch of natural science that studies non-living systems, in contrast to the biological sciences. It in turn has many branches, each referred to as a \"physical science\", together called the \"physical sciences\". However, the term \"physical\" creates an unintended, somewhat arbitrary distinction, since many branches of physical science also study biological phenomena (organic chemistry, for example). History of physics – history of the physical science that studies matter and its motion through space-time, and related concepts such as energy and force History of acoustics – history of the study of mechanical waves in solids, liquids, and gases (such as vibration and sound) History of agrophysics – history of the study of physics applied to agroecosystems History of soil physics – history of the study of soil physical properties and processes.  History of astrophysics – history of the study of the physical aspects of celestial objects History of astronomy – history of the studies the universe beyond Earth, including its formation and development, and the evolution, physics, chemistry, meteorology, and motion of celestial objects (such as galaxies, planets, etc.) and phenomena that originate outside the atmosphere of Earth (such as the cosmic background radiation). History of astrodynamics – history of the application of ballistics and celestial mechanics to the practical problems concerning the motion of rockets and other spacecraft. History of astrometry – history of the branch of astronomy that involves precise measurements of the positions and movements of stars and other celestial bodies. History of cosmology – history of the discipline that deals with the nature of the Universe as a whole. History of extragalactic astronomy – history of the branch of astronomy concerned with objects outside our own Milky Way Galaxy History of galactic astronomy – history of the study of our own Milky Way galaxy and all its contents. History of physical cosmology – history of the study of the largest-scale structures and dynamics of the universe and is concerned with fundamental questions about its formation and evolution. History of planetary science – history of the scientific study of planets (including Earth), moons, and planetary systems, in particular those of the Solar System and the processes that form them. History of stellar astronomy – history of the natural science that deals with the study of celestial objects (such as stars, planets, comets, nebulae, star clusters and galaxies) and phenomena that originate outside the atmosphere of Earth (such as cosmic background radiation)  History of atmospheric physics – history of the study of the application of physics to the atmosphere History of atomic, molecular, and optical physics – history of the study of how matter and light interact History of biophysics – history of the study of physical processes relating to biology History of medical physics – history of the application of physics concepts, theories and methods to medicine. History of neurophysics – history of the branch of biophysics dealing with the nervous system.  History of chemical physics – history of the branch of physics that studies chemical processes from the point of view of physics. History of computational physics – history of the study and implementation of numerical algorithms to solve problems in physics for which a quantitative theory already exists. History of condensed matter physics – history of the study of the physical properties of condensed phases of matter. History of cryogenics – history of the cryogenics is the study of the production of very low temperature (below −150 °C, −238 °F or 123K) and the behavior of materials at those temperatures. Dynamics – history of the study of the causes of motion and changes in motion History of econophysics – history of the interdisciplinary research field, applying theories and methods originally developed by physicists in order to solve problems in economics History of electromagnetism – history of the branch of science concerned with the forces that occur between electrically charged particles. History of geophysics – history of the physics of the Earth and its environment in space; also the study of the Earth using quantitative physical methods History of materials physics – history of the use of physics to describe materials in many different ways such as force, heat, light and mechanics. History of mathematical physics – history of the application of mathematics to problems in physics and the development of mathematical methods for such applications and for the formulation of physical theories. History of mechanics – history of the branch of physics concerned with the behavior of physical bodies when subjected to forces or displacements, and the subsequent effects of the bodies on their environment. History of biomechanics – history of the study of the structure and function of biological systems such as humans, animals, plants, organs, and cells by means of the methods of mechanics. History of classical mechanics – history of the one of the two major sub-fields of mechanics, which is concerned with the set of physical laws describing the motion of bodies under the action of a system of forces. History of continuum mechanics – history of the branch of mechanics that deals with the analysis of the kinematics and the mechanical behavior of materials modeled as a continuous mass rather than as discrete particles. History of fluid mechanics – history of the study of fluids and the forces on them. History of quantum mechanics – history of the branch of physics dealing with physical phenomena where the action is on the order of the Planck constant. History of thermodynamics – history of the branch of physical science concerned with heat and its relation to other forms of energy and work.  History of nuclear physics – history of the field of physics that studies the building blocks and interactions of atomic nuclei. History of optics – history of the branch of physics which involves the behavior and properties of light, including its interactions with matter and the construction of instruments that use or detect it. History of particle physics – history of the branch of physics that studies the existence and interactions of particles that are the constituents of what is usually referred to as matter or radiation. History of psychophysics – history of the quantitatively investigates the relationship between physical stimuli and the sensations and perceptions they affect. History of plasma physics – history of the state of matter similar to gas in which a certain portion of the particles are ionized. History of polymer physics – history of the field of physics that studies polymers, their fluctuations, mechanical properties, as well as the kinetics of reactions involving degradation and polymerisation of polymers and monomers respectively. History of quantum physics – history of the branch of physics dealing with physical phenomena where the action is on the order of the Planck constant. Relativity – History of statics – history of the branch of mechanics concerned with the analysis of loads (force, torque/moment) on physical systems in static equilibrium, that is, in a state where the relative positions of subsystems do not vary over time, or where components and structures are at a constant velocity. History of solid state physics – history of the study of rigid matter, or solids, through methods such as quantum mechanics, crystallography, electromagnetism, and metallurgy. History of vehicle dynamics – history of the dynamics of vehicles, here assumed to be ground vehicles.  History of chemistry – history of the physical science of atomic matter (matter that is composed of chemical elements), especially its chemical reactions, but also including its properties, structure, composition, behavior, and changes as they relate the chemical reactions History of analytical chemistry – history of the study of the separation, identification, and quantification of the chemical components of natural and artificial materials. History of astrochemistry – history of the study of the abundance and reactions of chemical elements and molecules in the universe, and their interaction with radiation. History of cosmochemistry – history of the study of the chemical composition of matter in the universe and the processes that led to those compositions  History of atmospheric chemistry – history of the branch of atmospheric science in which the chemistry of the Earth's atmosphere and that of other planets is studied. It is a multidisciplinary field of research and draws on environmental chemistry, physics, meteorology, computer modeling, oceanography, geology and volcanology and other disciplines History of biochemistry – history of the study of chemical processes in living organisms, including, but not limited to, living matter. Biochemistry governs all living organisms and living processes. History of agrochemistry – history of the study of both chemistry and biochemistry which are important in agricultural production, the processing of raw products into foods and beverages, and in environmental monitoring and remediation. History of bioinorganic chemistry – history of the examines the role of metals in biology. History of bioorganic chemistry – history of the rapidly growing scientific discipline that combines organic chemistry and biochemistry. History of biophysical chemistry – history of the new branch of chemistry that covers a broad spectrum of research activities involving biological systems. History of environmental chemistry – history of the scientific study of the chemical and biochemical phenomena that occur in natural places. History of immunochemistry – history of the branch of chemistry that involves the study of the reactions and components on the immune system. History of medicinal chemistry – history of the discipline at the intersection of chemistry, especially synthetic organic chemistry, and pharmacology and various other biological specialties, where they are involved with design, chemical synthesis and development for market of pharmaceutical agents (drugs). History of pharmacology – history of the branch of medicine and biology concerned with the study of drug action. History of natural product chemistry – history of the chemical compound or substance produced by a living organism – history of the found in nature that usually has a pharmacological or biological activity for use in pharmaceutical drug discovery and drug design. History of neurochemistry – history of the specific study of neurochemicals, which include neurotransmitters and other molecules such as neuro-active drugs that influence neuron function.  History of computational chemistry – history of the branch of chemistry that uses principles of computer science to assist in solving chemical problems. History of chemo-informatics – history of the use of computer and informational techniques, applied to a range of problems in the field of chemistry. History of molecular mechanics – history of the uses Newtonian mechanics to model molecular systems.  History of Flavor chemistry – history of the someone who uses chemistry to engineer artificial and natural flavors. History of Flow chemistry – history of the chemical reaction is run in a continuously flowing stream rather than in batch production. History of geochemistry – history of the study of the mechanisms behind major geological systems using chemistry History of aqueous geochemistry – history of the study of the role of various elements in watersheds, including copper, sulfur, mercury, and how elemental fluxes are exchanged through atmospheric-terrestrial-aquatic interactions History of isotope geochemistry – history of the study of the relative and absolute concentrations of the elements and their isotopes using chemistry and geology History of ocean chemistry – history of the studies the chemistry of marine environments including the influences of different variables. History of organic geochemistry – history of the study of the impacts and processes that organisms have had on Earth History of regional, environmental and exploration geochemistry – history of the study of the spatial variation in the chemical composition of materials at the surface of the Earth  History of inorganic chemistry – history of the branch of chemistry concerned with the properties and behavior of inorganic compounds. History of nuclear chemistry – history of the subfield of chemistry dealing with radioactivity, nuclear processes and nuclear properties. History of radiochemistry – history of the chemistry of radioactive materials, where radioactive isotopes of elements are used to study the properties and chemical reactions of non-radioactive isotopes (often within radiochemistry the absence of radioactivity leads to a substance being described as being inactive as the isotopes are stable).  History of organic chemistry – history of the study of the structure, properties, composition, reactions, and preparation (by synthesis or by other means) of carbon-based compounds, hydrocarbons, and their derivatives. History of petrochemistry – history of the branch of chemistry that studies the transformation of crude oil (petroleum) and natural gas into useful products or raw materials.  History of organometallic chemistry – history of the study of chemical compounds containing bonds between carbon and a metal. History of photochemistry – history of the study of chemical reactions that proceed with the absorption of light by atoms or molecules.. History of physical chemistry – history of the study of macroscopic, atomic, subatomic, and particulate phenomena in chemical systems in terms of physical laws and concepts. History of chemical kinetics – history of the study of rates of chemical processes. History of chemical thermodynamics – history of the study of the interrelation of heat and work with chemical reactions or with physical changes of state within the confines of the laws of thermodynamics. History of electrochemistry – history of the branch of chemistry that studies chemical reactions which take place in a solution at the interface of an electron conductor (a metal or a semiconductor) and an ionic conductor (the electrolyte), and which involve electron transfer between the electrode and the electrolyte or species in solution. History of Femtochemistry – history of the Femtochemistry is the science that studies chemical reactions on extremely short timescales, approximately 10−15 seconds (one femtosecond, hence the name). History of mathematical chemistry – history of the area of research engaged in novel applications of mathematics to chemistry; it concerns itself principally with the mathematical modeling of chemical phenomena. History of mechanochemistry – history of the coupling of the mechanical and the chemical phenomena on a molecular scale and includes mechanical breakage, chemical behaviour of mechanically stressed solids (e.g., stress-corrosion cracking), tribology, polymer degradation under shear, cavitation-related phenomena (e.g., sonochemistry and sonoluminescence), shock wave chemistry and physics, and even the burgeoning field of molecular machines. History of physical organic chemistry – history of the study of the interrelationships between structure and reactivity in organic molecules. History of quantum chemistry – history of the branch of chemistry whose primary focus is the application of quantum mechanics in physical models and experiments of chemical systems. History of sonochemistry – history of the study of the effect of sonic waves and wave properties on chemical systems. History of stereochemistry – history of the study of the relative spatial arrangement of atoms within molecules. History of supramolecular chemistry – history of the area of chemistry beyond the molecules and focuses on the chemical systems made up of a discrete number of assembled molecular subunits or components. History of thermochemistry – history of the study of the energy and heat associated with chemical reactions and/or physical transformations.  History of phytochemistry – history of the strict sense of the word the study of phytochemicals. History of polymer chemistry – history of the multidisciplinary science that deals with the chemical synthesis and chemical properties of polymers or macromolecules. History of solid-state chemistry – history of the study of the synthesis, structure, and properties of solid phase materials, particularly, but not necessarily exclusively of, non-molecular solids Multidisciplinary fields involving chemistry History of chemical biology – history of the scientific discipline spanning the fields of chemistry and biology that involves the application of chemical techniques and tools, often compounds produced through synthetic chemistry, to the study and manipulation of biological systems. History of chemical engineering – history of the branch of engineering that deals with physical science (e.g., chemistry and physics), and life sciences (e.g., biology, microbiology and biochemistry) with mathematics and economics, to the process of converting raw materials or chemicals into more useful or valuable forms. History of chemical oceanography – history of the study of the behavior of the chemical elements within the Earth's oceans. History of chemical physics – history of the branch of physics that studies chemical processes from the point of view of physics. History of materials science – history of the interdisciplinary field applying the properties of matter to various areas of science and engineering. History of nanotechnology – history of the study of manipulating matter on an atomic and molecular scale History of oenology – history of the science and study of all aspects of wine and winemaking except vine-growing and grape-harvesting, which is a subfield called viticulture. History of spectroscopy – history of the study of the interaction between matter and radiated energy History of surface science – history of the Surface science is the study of physical and chemical phenomena that occur at the interface of two phases, including solid–liquid interfaces, solid–gas interfaces, solid–vacuum interfaces, and liquid–gas interfaces.  History of earth science – history of the all-embracing term for the sciences related to the planet Earth. Earth science, and all of its branches, are branches of physical science. History of atmospheric sciences – history of the umbrella term for the study of the atmosphere, its processes, the effects other systems have on the atmosphere, and the effects of the atmosphere on these other systems. History of climatology History of meteorology History of atmospheric chemistry  History of biogeography – history of the study of the distribution of species (biology), organisms, and ecosystems in geographic space and through geological time. History of cartography – history of the study and practice of making maps or globes. History of climatology – history of the study of climate, scientifically defined as weather conditions averaged over a period of time History of coastal geography – history of the study of the dynamic interface between the ocean and the land, incorporating both the physical geography (i.e. coastal geomorphology, geology and oceanography) and the human geography (sociology and history) of the coast. History of environmental science – history of the an integrated, quantitative, and interdisciplinary approach to the study of environmental systems. History of ecology – history of the scientific study of the distribution and abundance of living organisms and how the distribution and abundance are affected by interactions between the organisms and their environment. History of Freshwater biology – history of the scientific biological study of freshwater ecosystems and is a branch of limnology History of marine biology – history of the scientific study of organisms in the ocean or other marine or brackish bodies of water History of parasitology – history of the Parasitology is the study of parasites, their hosts, and the relationship between them. History of population dynamics – history of the Population dynamics is the branch of life sciences that studies short-term and long-term changes in the size and age composition of populations, and the biological and environmental processes influencing those changes.  History of environmental chemistry – history of the Environmental chemistry is the scientific study of the chemical and biochemical phenomena that occur in natural places. History of environmental soil science – history of the Environmental soil science is the study of the interaction of humans with the pedosphere as well as critical aspects of the biosphere, the lithosphere, the hydrosphere, and the atmosphere. History of environmental geology – history of the Environmental geology, like hydrogeology, is an applied science concerned with the practical application of the principles of geology in the solving of environmental problems. History of toxicology – history of the branch of biology, chemistry, and medicine concerned with the study of the adverse effects of chemicals on living organisms.  History of geodesy – history of the scientific discipline that deals with the measurement and representation of the Earth, including its gravitational field, in a three-dimensional time-varying space History of geography – history of the science that studies the lands, features, inhabitants, and phenomena of Earth History of geoinformatics – history of the science and the technology which develops and uses information science infrastructure to address the problems of geography, geosciences and related branches of engineering. History of geology – history of the study of the Earth, with the general exclusion of present-day life, flow within the ocean, and the atmosphere. History of planetary geology – history of the planetary science discipline concerned with the geology of the celestial bodies such as the planets and their moons, asteroids, comets, and meteorites.  History of geomorphology – history of the scientific study of landforms and the processes that shape them History of geostatistics – history of the branch of statistics focusing on spatial or spatiotemporal datasets History of geophysics – history of the physics of the Earth and its environment in space; also the study of the Earth using quantitative physical methods. History of glaciology – history of the study of glaciers, or more generally ice and natural phenomena that involve ice. History of hydrology – history of the study of the movement, distribution, and quality of water on Earth and other planets, including the hydrologic cycle, water resources and environmental watershed sustainability. History of hydrogeology – history of the area of geology that deals with the distribution and movement of groundwater in the soil and rocks of the Earth's crust (commonly in aquifers). History of mineralogy – history of the study of chemistry, crystal structure, and physical (including optical) properties of minerals. History of meteorology – history of the interdisciplinary scientific study of the atmosphere which explains and forecasts weather events. History of oceanography – history of the branch of Earth science that studies the ocean History of paleoclimatology – history of the study of changes in climate taken on the scale of the entire history of Earth History of paleontology – history of the study of prehistoric life History of petrology – history of the branch of geology that studies the origin, composition, distribution and structure of rocks. History of limnology – history of the study of inland waters History of seismology – history of the scientific study of earthquakes and the propagation of elastic waves through the Earth or through other planet-like bodies History of soil science – history of the study of soil as a natural resource on the surface of the earth including soil formation, classification and mapping; physical, chemical, biological, and fertility properties of soils; and these properties in relation to the use and management of soils. History of topography – history of the study of surface shape and features of the Earth and other observable astronomical objects including planets, moons, and asteroids. History of volcanology – history of the study of volcanoes, lava, magma, and related geological, geophysical and geochemical phenomena.    General principles of the physical sciences  Principle – law or rule that has to be, or usually is to be followed, or can be desirably followed, or is an inevitable consequence of something, such as the laws observed in nature or the way that a system is constructed. The principles of such a system are understood by its users as the essential characteristics of the system, or reflecting system's designed purpose, and the effective operation or use of which would be impossible if any one of the principles was to be ignored.    Basic principles of physics  Physics – branch of science that studies matter and its motion through space and time, along with related concepts such as energy and force. Physics is one of the \"fundamental sciences\" because the other natural sciences (like biology, geology etc.) deal with systems that seem to obey the laws of physics. According to physics, the physical laws of matter, energy and the fundamental forces of nature govern the interactions between particles and physical entities (such as planets, molecules, atoms or the subatomic particles). Some of the basic pursuits of physics, which include some of the most prominent developments in modern science in the last millennium, include: Describing the nature, measuring and quantifying of bodies and their motion, dynamics etc. Newton's laws of motion Mass, force and weight Momentum and conservation of energy Gravity, theories of gravity Energy, work, and their relationship Motion, position, and energy Different forms of Energy, their interconversion and the inevitable loss of energy in the form of heat (Thermodynamics) Energy conservation, conversion, and transfer. Energy source the transfer of energy from one source to work in another.  Kinetic molecular theory Phases of matter and phase transitions Temperature and thermometers Energy and heat Heat flow: conduction, convection, and radiation The three laws of thermodynamics  The principles of waves and sound The principles of electricity, magnetism, and electromagnetism The principles, sources, and properties of light    Basic principles of astronomy  Astronomy – science of celestial bodies and their interactions in space. Its studies includes the following: The life and characteristics of stars and galaxies Origins of the universe. Physical science uses the Big Bang theory as the commonly accepted scientific theory of the origin of the universe. A heliocentric Solar System. Ancient cultures saw the Earth as the centre of the Solar System or universe (geocentrism). In the 16th century, Nicolaus Copernicus advanced the ideas of heliocentrism, recognizing the Sun as the centre of the Solar System. The structure of solar systems, planets, comets, asteroids, and meteors The shape and structure of Earth (roughly spherical, see also Spherical Earth) Earth in the Solar System Time measurement The composition and features of the Moon Interactions of the Earth and Moon (Note: Astronomy should not be confused with astrology, which assumes that people's destiny and human affairs in general correlate to the apparent positions of astronomical objects in the sky - although the two fields share a common origin, they are quite different; astronomers embrace the scientific method, while astrologers do not.)    Basic principles of chemistry  Chemistry – branch of science that studies the composition, structure, properties and change of matter. Chemistry is chiefly concerned with atoms and molecules and their interactions and transformations, for example, the properties of the chemical bonds formed between atoms to create chemical compounds. As such, chemistry studies the involvement of electrons and various forms of energy in photochemical reactions, oxidation-reduction reactions, changes in phases of matter, and separation of mixtures. Preparation and properties of complex substances, such as alloys, polymers, biological molecules, and pharmaceutical agents are considered in specialized fields of chemistry.  Physical chemistry Chemical thermodynamics Reaction kinetics Molecular structure Quantum chemistry Spectroscopy  Theoretical chemistry Electron configuration Molecular modelling Molecular dynamics Statistical mechanics  Computational chemistry Mathematical chemistry Cheminformatics  Nuclear chemistry The nature of the atomic nucleus Characterization of radioactive decay Nuclear reactions  Organic chemistry Organic compounds Organic reaction Functional groups Organic synthesis  Inorganic chemistry Inorganic compounds Crystal structure Coordination chemistry Solid-state chemistry  Biochemistry Analytical chemistry Instrumental analysis Electroanalytical method Wet chemistry  Electrochemistry Redox reaction  Materials chemistry    Basic principles of earth science  Earth science – the science of the planet Earth, as of 2014 the only identified life-bearing planet. Its studies include the following: The water cycle and the process of transpiration Freshwater Oceanography Weathering and erosion Rocks  Agrophysics Soil science Pedogenesis Soil fertility  Earth's tectonic structure Geomorphology and geophysics Physical geography Seismology: stress, strain, and earthquakes Characteristics of mountains and volcanoes  Characteristics and formation of fossils Atmospheric sciences – the branches of science that study the atmosphere, its processes, the effects other systems have on the atmosphere, and the effects of the atmosphere on these other systems. Atmosphere of Earth Atmospheric pressure and winds Evaporation, condensation, and humidity Fog and clouds  Meteorology, weather, climatology, and climate Hydrology, clouds and precipitation Air masses and weather fronts Major storms: thunderstorms, tornadoes, and hurricanes Major climate groups  Speleology Cave    Notable physical scientists  List of physicists List of astronomers List of chemists    Earth scientists  List of Russian earth scientists    See also  Outline of science Outline of natural science Outline of physical science Outline of earth science  Outline of formal science Outline of social science Outline of applied science    Notes     References     External links  Physical science topics and articles for school curricula (grades K-12)","label":"foo"},{"text":"","label":"foo"},{"text":"","label":"foo"},{"text":"The following outline is provided as an overview of and topical guide to physics: Physics – natural science that involves the study of matter and its motion through spacetime, along with related concepts such as energy and force. More broadly, it is the general analysis of nature, conducted in order to understand how the universe behaves.    What type of thing is physics?Edit  Physics can be described as all of the following: An academic discipline – one with academic departments, curricula and degrees; national and international societies; and specialized journals. A scientific field (a branch of science) – widely-recognized category of specialized expertise within science, and typically embodies its own terminology and nomenclature. Such a field will usually be represented by one or more scientific journals, where peer-reviewed research is published. A natural science – one that seeks to elucidate the rules that govern the natural world using empirical and scientific method. A physical science – one that studies non-living systems. A biological science – one that studies the role of physical processes in living organisms. See Outline of biophysics.    Branches of physicsEdit  Acoustics – study of mechanical waves in solids, liquids, and gases (such as vibration and sound) Agrophysics – study of physics applied to agroecosystems Soil physics – study of soil physical properties and processes.  Astrophysics – study of the physical aspects of celestial objects Astronomy – studies the universe beyond Earth, including its formation and development, and the evolution, physics, chemistry, meteorology, and motion of celestial objects (such as galaxies, planets, etc.) and phenomena that originate outside the atmosphere of Earth (such as the cosmic background radiation). Astrodynamics – application of ballistics and celestial mechanics to the practical problems concerning the motion of rockets and other spacecraft. Astrometry – branch of astronomy that involves precise measurements of the positions and movements of stars and other celestial bodies. Cosmology – discipline that deals with the nature of the Universe as a whole. Extragalactic astronomy – branch of astronomy concerned with objects outside our own Milky Way Galaxy Galactic astronomy – study of our own Milky Way galaxy and all its contents. Physical cosmology – study of the largest-scale structures and dynamics of the universe and is concerned with fundamental questions about its formation and evolution. Planetary science – scientific study of planets (including Earth), moons, and planetary systems, in particular those of the Solar System and the processes that form them. Stellar astronomy – natural science that deals with the study of celestial objects (such as stars, planets, comets, nebulae, star clusters and galaxies) and phenomena that originate outside the atmosphere of Earth (such as cosmic background radiation)  Atmospheric physics – study of the application of physics to the atmosphere Atomic, molecular, and optical physics – study of how matter and light interact Biophysics – study of physical processes relating to biology Medical physics – application of physics concepts, theories and methods to medicine. Neurophysics – branch of biophysics dealing with the nervous system.  Chemical physics – branch of physics that studies chemical processes from the point of view of physics. Classical physics – Computational physics – study and implementation of numerical algorithms to solve problems in physics for which a quantitative theory already exists. Condensed matter physics – study of the physical properties of condensed phases of matter. Cryogenics – cryogenics is the study of the production of very low temperature (below −150 °C, −238 °F or 123K) and the behavior of materials at those temperatures. Dynamics – study of the causes of motion and changes in motion Thermodynamics - the study of the relationships between heat and mechanical energy  Econophysics – interdisciplinary research field, applying theories and methods originally developed by physicists in order to solve problems in economics Electromagnetism – branch of science concerned with the forces that occur between electrically charged particles. Geophysics – the physics of the Earth and its environment in space; also the study of the Earth using quantitative physical methods Materials physics – use of physics to describe materials in many different ways such as force, heat, light and mechanics. Mathematical physics – application of mathematics to problems in physics and the development of mathematical methods for such applications and for the formulation of physical theories. Mechanics – branch of physics concerned with the behavior of physical bodies when subjected to forces or displacements, and the subsequent effects of the bodies on their environment. Aerodynamics – Biomechanics – study of the structure and function of biological systems such as humans, animals, plants, organs, and cells by means of the methods of mechanics. Classical mechanics – one of the two major sub-fields of mechanics, which is concerned with the set of physical laws describing the motion of bodies under the action of a system of forces. Kinematics – branch of classical mechanics that describes the motion of points, bodies (objects) and systems of bodies (groups of objects) without consideration of the causes of motion.  Continuum mechanics – branch of mechanics that deals with the analysis of the kinematics and the mechanical behavior of materials modeled as a continuous mass rather than as discrete particles. Fluid mechanics – study of fluids and the forces on them. Fluid statics – study of fluids at rest Fluid kinematics – study of fluids in motion Fluid dynamics – study of the effect of forces on fluid motion  Quantum mechanics – branch of physics which describes tiny discrete quantities of matter and energy, where action is on the order of Planck's constant. Thermodynamics – branch of physical science concerned with heat and its relation to other forms of energy and work.  Nuclear physics – field of physics that studies the building blocks and interactions of atomic nuclei. Optics – branch of physics which involves the behavior and properties of light, including its interactions with matter and the construction of instruments that use or detect it. Particle physics – branch of physics that studies the properties and interactions of the fundamental constituents of matter and energy. Psychophysics – quantitatively investigates the relationship between physical stimuli and the sensations and perceptions they affect. Plasma physics – the study of plasma, a state of matter similar to gas in which a certain portion of the particles are ionized. Polymer physics – field of physics that studies polymers, their fluctuations, mechanical properties, as well as the kinetics of reactions involving degradation and polymerisation of polymers and monomers respectively. Quantum physics – branch of physics dealing with physical phenomena where the action is on the order of the Planck constant. Relativity – theory of physics which describes the relationship between space and time. Statics – branch of mechanics concerned with the analysis of loads (force, torque/moment) on physical systems in static equilibrium, that is, in a state where the relative positions of subsystems do not vary over time, or where components and structures are at a constant velocity. Solid state physics – study of rigid matter, or solids, through methods such as quantum mechanics, crystallography, electromagnetism, and metallurgy. Theoretical physics – the pursuit of describing physical phenomena with rigorous mathematical models and physical abstractions in order to analyze, explain, and predict natural processes. Vehicle dynamics – dynamics of vehicles, here assumed to be ground vehicles.    History of physicsEdit  History of physics – history of the physical science that studies matter and its motion through space-time, and related concepts such as energy and force History of acoustics – history of the study of mechanical waves in solids, liquids, and gases (such as vibration and sound) History of agrophysics – history of the study of physics applied to agroecosystems History of soil physics – history of the study of soil physical properties and processes.  History of astrophysics – history of the study of the physical aspects of celestial objects History of astronomy – history of the studies the universe beyond Earth, including its formation and development, and the evolution, physics, chemistry, meteorology, and motion of celestial objects (such as galaxies, planets, etc.) and phenomena that originate outside the atmosphere of Earth (such as the cosmic background radiation). History of astrodynamics – history of the application of ballistics and celestial mechanics to the practical problems concerning the motion of rockets and other spacecraft. History of astrometry – history of the branch of astronomy that involves precise measurements of the positions and movements of stars and other celestial bodies. History of cosmology – history of the discipline that deals with the nature of the Universe as a whole. History of extragalactic astronomy – history of the branch of astronomy concerned with objects outside our own Milky Way Galaxy History of galactic astronomy – history of the study of our own Milky Way galaxy and all its contents. History of physical cosmology – history of the study of the largest-scale structures and dynamics of the universe and is concerned with fundamental questions about its formation and evolution. History of planetary science – history of the scientific study of planets (including Earth), moons, and planetary systems, in particular those of the Solar System and the processes that form them. History of stellar astronomy – history of the natural science that deals with the study of celestial objects (such as stars, planets, comets, nebulae, star clusters and galaxies) and phenomena that originate outside the atmosphere of Earth (such as cosmic background radiation)  History of atmospheric physics – history of the study of the application of physics to the atmosphere History of atomic, molecular, and optical physics – history of the study of how matter and light interact History of biophysics – history of the study of physical processes relating to biology History of medical physics – history of the application of physics concepts, theories and methods to medicine. History of neurophysics – history of the branch of biophysics dealing with the nervous system.  History of chemical physics – history of the branch of physics that studies chemical processes from the point of view of physics. History of computational physics – history of the study and implementation of numerical algorithms to solve problems in physics for which a quantitative theory already exists. History of condensed matter physics – history of the study of the physical properties of condensed phases of matter. History of cryogenics – history of the cryogenics is the study of the production of very low temperature (below −150 °C, −238 °F or 123K) and the behavior of materials at those temperatures. Dynamics – history of the study of the causes of motion and changes in motion History of econophysics – history of the interdisciplinary research field, applying theories and methods originally developed by physicists in order to solve problems in economics History of electromagnetism – history of the branch of science concerned with the forces that occur between electrically charged particles. History of geophysics – history of the physics of the Earth and its environment in space; also the study of the Earth using quantitative physical methods History of materials physics – history of the use of physics to describe materials in many different ways such as force, heat, light and mechanics. History of mathematical physics – history of the application of mathematics to problems in physics and the development of mathematical methods for such applications and for the formulation of physical theories. History of mechanics – history of the branch of physics concerned with the behavior of physical bodies when subjected to forces or displacements, and the subsequent effects of the bodies on their environment. History of biomechanics – history of the study of the structure and function of biological systems such as humans, animals, plants, organs, and cells by means of the methods of mechanics. History of classical mechanics – history of the one of the two major sub-fields of mechanics, which is concerned with the set of physical laws describing the motion of bodies under the action of a system of forces. History of continuum mechanics – history of the branch of mechanics that deals with the analysis of the kinematics and the mechanical behavior of materials modeled as a continuous mass rather than as discrete particles. History of fluid mechanics – history of the study of fluids and the forces on them. History of quantum mechanics – history of the branch of physics dealing with physical phenomena where the action is on the order of the Planck constant. History of thermodynamics – history of the branch of physical science concerned with heat and its relation to other forms of energy and work.  History of nuclear physics – history of the field of physics that studies the building blocks and interactions of atomic nuclei. History of optics – history of the branch of physics which involves the behavior and properties of light, including its interactions with matter and the construction of instruments that use or detect it. History of particle physics – history of the branch of physics that studies the existence and interactions of particles that are the constituents of what is usually referred to as matter or radiation. History of psychophysics – history of the quantitatively investigates the relationship between physical stimuli and the sensations and perceptions they affect. History of plasma physics – history of the state of matter similar to gas in which a certain portion of the particles are ionized. History of polymer physics – history of the field of physics that studies polymers, their fluctuations, mechanical properties, as well as the kinetics of reactions involving degradation and polymerisation of polymers and monomers respectively. History of quantum physics – history of the branch of physics dealing with physical phenomena where the action is on the order of the Planck constant. Relativity – History of statics – history of the branch of mechanics concerned with the analysis of loads (force, torque/moment) on physical systems in static equilibrium, that is, in a state where the relative positions of subsystems do not vary over time, or where components and structures are at a constant velocity. History of solid state physics – history of the study of rigid matter, or solids, through methods such as quantum mechanics, crystallography, electromagnetism, and metallurgy. History of vehicle dynamics – history of the dynamics of vehicles, here assumed to be ground vehicles.    General concepts of physicsEdit     Basic principles of physicsEdit  Physics – branch of science that studies matter and its motion through space and time, along with related concepts such as energy and force. Physics is one of the \"fundamental sciences\" because the other natural sciences (like biology, geology etc.) deal with systems that seem to obey the laws of physics. According to physics, the physical laws of matter, energy and the fundamental forces of nature govern the interactions between particles and physical entities (such as planets, molecules, atoms or the subatomic particles). Some of the basic pursuits of physics, which include some of the most prominent developments in modern science in the last millennium, include: Describing the nature, measuring and quantifying of bodies and their motion, dynamics etc. Newton's laws of motion Mass, force and weight Momentum and conservation of energy Gravity, theories of gravity Energy, work, and their relationship Motion, position, and energy Different forms of Energy, their interconversion and the inevitable loss of energy in the form of heat (Thermodynamics) Energy conservation, conversion, and transfer. Energy source the transfer of energy from one source to work in another.  Kinetic molecular theory Phases of matter and phase transitions Temperature and thermometers Energy and heat Heat flow: conduction, convection, and radiation The three laws of thermodynamics  The principles of waves and sound The principles of electricity, magnetism, and electromagnetism The principles, sources, and properties of light Basic quantities Acceleration Electric charge Energy Entropy Force Length Mass Matter Momentum Potential energy Space Temperature Time Velocity  Gravity, light, physical system, physical observation, physical quantity, physical state, physical unit, physical theory, physical experiment Theoretical concepts Mass–energy equivalence, particle, physical field, physical interaction, physical law, fundamental force, physical constant, wave    OverviewEdit   This is a list of the primary theories in physics, major subtopics, and concepts. Note: the Theory column below contains links to articles with infoboxes at the top of their respective pages which list the major concepts.    Concepts by fieldEdit     Famous physicistsEdit   Archimedes – discovered the laws of flotation and developed Archimedes' principle. Ibn al-Haytham – Father of optics and discovered reflection and refraction. Galileo Galilei – \"Father of modern physics.\". Isaac Newton – Laid the groundwork for classical mechanics, made significant contributions to the field of optics and co-invented calculus. Often considered the greatest physicist of all time. James Clerk Maxwell - Formulated a set of equations that united previously unrelated observations, experiments, and equations of electricity, magnetism, and optics into a consistent theory. Albert Einstein – Generally considered greatest scientist of the 20th century. Developed both the Special and General theories of relativity and proved the existence of atoms beyond doubt. Niels Bohr – made fundamental contributions to understanding atomic structure and quantum mechanics. Widely considered one of the greatest physicists of the 20th century. Robert Oppenheimer – \"Father of the atomic bomb.\" Richard Feynman – Expanded the theory of quantum electrodynamics, and developed the tool known as Feynman diagrams. Stephen Hawking – made fundamental contributions to black hole physics and cosmology. Also authored popular books on these subjects.    ListsEdit   List of common physics notations List of equations in classical mechanics List of important publications in physics List of laws in science List of letters used in mathematics and science List of noise topics List of optical topics List of physicists List of physics journals List of scientific units named after people Index of wave articles Variables commonly used in physics    See alsoEdit   Category:Concepts in physics Category:Physics-related lists Elementary physics formulae Glossary of classical physics List of physics concepts in primary and secondary education curricula    NotesEdit     External linksEdit  AIP.org is the website of the American Institute of Physics IOP.org is the website of the Institute of Physics APS.org is the website of the American Physical Society SPS National is the website of the American Society of Physics Students CAP.ca is the website of the Canadian Association of Physicists EPS.org is the website of the European Physical Society Meta Institute for Computational Physics - Popular Talks Physics | Channel | MIT Video Theoretical Physics as a Challenge (website with outline of physics) by Gerard 't Hooft The Feynman Lectures on Physics, 3 vols., free online, Caltech & The Feynman Lectures Website resource recommendations - List of freely available physics books - Physics Stack Exchange","label":"foo"},{"text":"Ancient Greek includes the forms of Greek used in ancient Greece and the ancient world from around the 9th century BC to the 6th century AD. It is often roughly divided into the Archaic period (9th to 6th centuries BC), Classical period (5th and 4th centuries BC), and Hellenistic period (3rd century BC to 6th century AD). It is antedated in the second millennium BC by Mycenaean Greek. The language of the Hellenistic phase is known as Koine (common), while the language from the late period onward features no considerable differences from Medieval Greek. Koine is regarded as a separate historical stage of its own, although in its earlier form, it closely resembled the Classical. Prior to the Koine period, Greek of the classic and earlier periods included several regional dialects. Ancient Greek was the language of Homer and of classical Athenian historians, playwrights, and philosophers. It has contributed many words to English vocabulary and has been a standard subject of study in educational institutions of the West since the Renaissance. This article primarily contains information about the Epic and Classical phases of the language.    DialectsEdit   Ancient Greek was a pluricentric language, divided into many dialects. The main dialect groups are Attic and Ionic, Aeolic, Arcadocypriot, and Doric, many of them with several subdivisions. Some dialects are found in standardized literary forms used in literature, while others are attested only in inscriptions. There are also several historical forms. Homeric Greek is a literary form of Archaic Greek (derived primarily from Ionic and Aeolic) used in the epic poems, the \"Iliad\" and \"Odyssey\", and in later poems by other authors. Homeric Greek had significant differences in grammar and pronunciation from Classical Attic and other Classical-era dialects.    HistoryEdit  The origins, early form and development of the Hellenic language family are not well understood because of a lack of contemporaneous evidence. Several theories exist about what Hellenic dialect groups may have existed between the divergence of early Greek-like speech from the common Proto-Indo-European language. They have the same general outline, but differ in some of the detail. The only attested dialect from this period is Mycenaean, but its relationship to the historical dialects and the historical circumstances of the times imply that the overall groups already existed in some form. Scholars assume that major Ancient Greek period dialect groups developed not later than 1120 BC, at the time of the Dorian invasion(s)—and that their first appearances as precise alphabetic writing began in the 8th century BC. The invasion would not be \"Dorian\" unless the invaders had some cultural relationship to the historical Dorians. The invasion is known to have displaced population to the later Attic-Ionic regions, who regarded themselves as descendants of the population displaced by or contending with the Dorians. The Greeks of this period believed there were three major divisions of all Greek people—Dorians, Aeolians, and Ionians (including Athenians), each with their own defining and distinctive dialects. Allowing for their oversight of Arcadian, an obscure mountain dialect, and Cypriot, far from the center of Greek scholarship, this division of people and language is quite similar to the results of modern archaeological-linguistic investigation. One standard formulation for the dialects is: West vs. non-west Greek is the strongest marked and earliest division, with non-west in subsets of Ionic-Attic (or Attic-Ionic) and Aeolic vs. Arcado-Cypriot, or Aeolic and Arcado-Cypriot vs. Ionic-Attic. Often non-west is called East Greek. The Arcado-Cypriot group apparently descended more closely from the Mycenaean Greek of the Bronze Age. Boeotian had come under a strong Northwest Greek influence, and can in some respects be considered a transitional dialect. Thessalian likewise had come under Northwest Greek influence, though to a lesser degree. Pamphylian, spoken in a small area on the south-western coast of Asia Minor and little preserved in inscriptions, may be either a fifth major dialect group, or it is Mycenaean Greek overlaid by Doric, with a non-Greek native influence. Most of the dialect sub-groups listed above had further subdivisions, generally equivalent to a city-state and its surrounding territory, or to an island. Doric notably had several intermediate divisions as well, into Island Doric (including Cretan Doric), Southern Peloponnesus Doric (including Laconian, the dialect of Sparta), and Northern Peloponnesus Doric (including Corinthian). The Lesbian dialect was a member of the Aegean/Asiatic Aeolic sub-group. All the groups were represented by colonies beyond Greece proper as well, and these colonies generally developed local characteristics, often under the influence of settlers or neighbors speaking different Greek dialects. The dialects outside the Ionic group are known mainly from inscriptions, notable exceptions being fragments of the works of the poetess Sappho from the island of Lesbos and the poems of the Boeotian poet, Pindar. After the conquests of Alexander the Great in the late 4th century BC, a new international dialect known as Koine or Common Greek developed, largely based on Attic Greek, but with influence from other dialects. This dialect slowly replaced most of the older dialects, although Doric dialect has survived to the present in the form of the Tsakonian dialect of Modern Greek, spoken in the region of modern Sparta. Doric has also passed down its aorist terminations into most verbs of Demotic Greek. By about the 6th century AD, the Koine had slowly metamorphosized into Medieval Greek.    Other languagesEdit  Ancient Macedonian was an Indo-European language closely related to Greek, but its exact relationship is unclear because of insufficient data: possibly a dialect of Greek; a sibling language to Greek; or a close cousin to Greek, and perhaps related to some extent, to Thracian and Phrygian languages. The Pella curse tablet is one of many finds that support the idea that the Ancient Macedonian language is closely related to the Doric Greek dialect.    PhonologyEdit     Differences from Proto-Indo-EuropeanEdit   Ancient Greek differs from Proto-Indo-European and other Indo-European languages in certain ways. In phonotactics, Ancient Greek words could only end in a vowel or /n s r/; final stops were lost, as in γάλα \"milk\", compared with γάλακτος \"of milk\" (genitive). Ancient Greek of the classical period also differed in phonemic inventory: PIE *s became /h/ at the beginning of a word (debuccalization): Latin sex, English six, Ancient Greek ἕξ /héks/. PIE *s was lost between vowels via an intermediate step of debuccalization: Sanskrit janasas, Latin generis (where s > r by rhotacism), Greek *genesos > *genehos > Ancient Greek γένεος (/géneos/), Attic γένους (/génoːs/) \"of a kind\". PIE *y /j/ became /h/ (debuccalization) or /(d)z/ (fortition): Sanskrit yas, Ancient Greek ὅς \"who\" (relative pronoun); Latin iugum, English yoke, Ancient Greek ζυγός /zygós/. PIE *w, which occurred in Mycenaean and some non-Attic dialects, was lost: early Doric ϝέργον, English work, Attic Greek ἔργον /érgon/. PIE and Mycenaean labiovelars changed to plain stops (labials, dentals, and velars) in the later Greek dialects: for instance, PIE *kʷ became /p/ or /t/ in Attic: Attic Greek ποῦ /pôː/ \"where?\", Latin quō; Attic Greek τίς /tís/, Latin quis \"who?\". PIE \"voiced aspirated\" stops *bʰ dʰ ǵʰ gʰ gʷʰ were devoiced and became the aspirated stops φ θ χ /pʰ tʰ kʰ/ in Ancient Greek.    Phonemic inventoryEdit   The pronunciation of Ancient Greek was very different from that of Modern Greek. Ancient Greek had long and short vowels; many diphthongs; double and single consonants; voiced, voiceless, and aspirated stops; and a pitch accent. In Modern Greek, all vowels and consonants are short—many vowels and diphthongs once pronounced distinctly are pronounced as /i/ (iotacism). Some of the stops and glides in diphthongs have become fricatives, and the pitch accent has changed to a stress accent. Many of these changes took place in the Koine Greek period. The writing system of Modern Greek, however, does not reflect all pronunciation changes. The examples below represent Attic Greek in the 5th century BC. Ancient pronunciation can never be reconstructed with certainty, but Greek from this period is well documented and there is little disagreement among linguists as to the general nature of the sounds that the letters represent.   = ConsonantsEdit = [ŋ] occurred as an allophone of /n/ used before velars and as an allophone of /ɡ/ before nasals. /r/ was probably voiceless when word-initial (written ῥ). /s/ was assimilated to [z] before voiced consonants.   = VowelsEdit = /oː/ raised to [uː], probably by the 4th century BC.    MorphologyEdit   Greek, like all of the older Indo-European languages, is highly inflected. It is highly archaic in its preservation of Proto-Indo-European forms. In Ancient Greek, nouns (including proper nouns) have five cases (nominative, genitive, dative, accusative, and vocative), three genders (masculine, feminine, and neuter), and three numbers (singular, dual, and plural). Verbs have four moods (indicative, imperative, subjunctive, and optative) and three voices (active, middle, and passive), as well as three persons (first, second, and third) and various other forms. Verbs are conjugated through seven combinations of tenses and aspect (generally simply called \"tenses\"): the present, future, and imperfect are imperfective in aspect; the aorist (perfective aspect); a present perfect, pluperfect and future perfect. Most tenses display all four moods and three voices, although there is no future subjunctive or imperative. Also, there is no imperfect subjunctive, optative or imperative. The infinitives and participles correspond to the finite combinations of tense, aspect, and voice.    AugmentEdit  The indicative of past tenses adds (conceptually, at least) a prefix /e-/, called the augment. This was probably originally a separate word, meaning something like \"then\", added because tenses in PIE had primarily aspectual meaning. The augment is added to the indicative of the aorist, imperfect, and pluperfect, but not to any of the other forms of the aorist (no other forms of the imperfect and pluperfect exist). The two kinds of augment in Greek are syllabic and quantitative. The syllabic augment is added to stems beginning with consonants, and simply prefixes e (stems beginning with r, however, add er). The quantitative augment is added to stems beginning with vowels, and involves lengthening the vowel: a, ā, e, ē → ē i, ī → ī o, ō → ō u, ū → ū ai → ēi ei → ēi or ei oi → ōi au → ēu or au eu → ēu or eu ou → ou Some verbs augment irregularly; the most common variation is e → ei. The irregularity can be explained diachronically by the loss of s between vowels. In verbs with a prefix, the augment is placed not at the start of the word, but between the prefix and the original verb. For example, προσ(-)βάλλω (I attack) goes to προσέβαλoν in the aorist. Following Homer's practice, the augment is sometimes not made in poetry, especially epic poetry. The augment sometimes substitutes for reduplication; see below.    ReduplicationEdit  Almost all forms of the perfect, pluperfect, and future perfect reduplicate the initial syllable of the verb stem. (Note that a few irregular forms of perfect do not reduplicate, whereas a handful of irregular aorists reduplicate.) The three types of reduplication are: Syllabic reduplication: Most verbs beginning with a single consonant, or a cluster of a stop with a sonorant, add a syllable consisting of the initial consonant followed by e. An aspirated consonant, however, reduplicates in its unaspirated equivalent: Grassmann's law. Augment: Verbs beginning with a vowel, as well as those beginning with a cluster other than those indicated previously (and occasionally for a few other verbs) reduplicate in the same fashion as the augment. This remains in all forms of the perfect, not just the indicative. Attic reduplication: Some verbs beginning with an a, e or o, followed by a sonorant (or occasionally d or g), reduplicate by adding a syllable consisting of the initial vowel and following consonant, and lengthening the following vowel. Hence er → erēr, an → anēn, ol → olōl, ed → edēd. This is not actually specific to Attic Greek, despite its name, but it was generalized in Attic. This originally involved reduplicating a cluster consisting of a laryngeal and sonorant, hence h₃l → h₃leh₃l → olōl with normal Greek development of laryngeals. (Forms with a stop were analogous.) Irregular duplication can be understood diachronically. For example, lambanō (root lab) has the perfect stem eilēpha (not *lelēpha) because it was originally slambanō, with perfect seslēpha, becoming eilēpha through compensatory lengthening. Reduplication is also visible in the present tense stems of certain verbs. These stems add a syllable consisting of the root's initial consonant followed by i. A nasal stop appears after the reduplication in some verbs.    Writing systemEdit   Ancient Greek was written in the Greek alphabet, with some variation among dialects. Early texts are written in boustrophedon style, but left-to-right became standard during the classic period. Modern editions of Ancient Greek texts are usually written with accents and breathing marks, interword spacing, modern punctuation, and sometimes mixed case, but these were all introduced later.    Example textEdit  The beginning of Homer's Iliad exemplifies the Archaic period of Ancient Greek (see Homeric Greek for more details):  The beginning of Apology by Plato exemplifies Attic Greek from the Classical period of Ancient Greek: Ὅτι μὲν ὑμεῖς, ὦ ἄνδρες Ἀθηναῖοι, πεπόνθατε ὑπὸ τῶν ἐμῶν κατηγόρων, οὐκ οἶδα: ἐγὼ δ' οὖν καὶ αὐτὸς ὑπ' αὐτῶν ὀλίγου ἐμαυτοῦ ἐπελαθόμην, οὕτω πιθανῶς ἔλεγον. Καίτοι ἀληθές γε ὡς ἔπος εἰπεῖν οὐδὲν εἰρήκασιν. Transliterated into the Latin alphabet using a modern version of the Erasmian scheme: Hóti mèn hūmeîs, ô ándres Athēnaîoi, pepónthate hupò tôn emôn katēgórōn, ouk oîda: egṑ d' oûn kaì autòs hup' autōn olígou emautoû epelathómēn, hoútō pithanôs élegon. Kaítoi alēthés ge hōs épos eipeîn oudèn eirḗkāsin. Using the IPA: /hóti men hyːmêːs | ɔ̂ː ándres atʰɛːnaî̯i̯oi | pepóntʰate | hypo tɔ̂ːn emɔ̂ːn katɛːɡórɔːn | uːk oî̯da ‖ éɡɔː dûːn kai̯ au̯tos | hyp au̯tɔ̂ːn olíɡuː emau̯tûː | epelatʰómɛːn | hǔːtɔː pitʰanɔ̂ːs éleɡon ‖ kaí̯toi̯ alɛːtʰéz ɡe | hɔːs épos eːpêːn | uːden eːrɛ̌ːkaːsin ‖/ Translated into English: What you, men of Athens, have learned from my accusers, I do not know: but I, for my part, nearly forgot who I was thanks to them, since they spoke so persuasively. And yet, of the truth, they have spoken, one might say, nothing at all.    Modern useEdit   The study of Ancient Greek in European countries in addition to Latin occupied an important place in the syllabus from the Renaissance until the beginning of the 20th century. Ancient Greek is still taught as a compulsory or optional subject especially at traditional or elite schools throughout Europe, such as public schools and grammar schools in the United Kingdom. It is compulsory in the Liceo classico in Italy, in the gymnasium in the Netherlands, in some classes in Austria, in Croatia in klasična gimnazija and it is optional in the Humanistisches Gymnasium in Germany (usually as a third language after Latin and English, from the age of 14 to 18). In 2006/07, 15,000 pupils studied Ancient Greek in Germany according to the Federal Statistical Office of Germany, and 280,000 pupils studied it in Italy. It is a compulsory subject alongside Latin in the Humanities branch of Spanish Bachillerato. Ancient Greek is also taught at most major universities worldwide, often combined with Latin as part of Classics. It will also be taught in state primary schools in the UK, to boost children’s language skills, and will be offered as a foreign language to pupils in all primary schools from 2014 as part of a major drive to boost education standards, together with Latin, Mandarin, French, German, Spanish, and Italian. Ancient Greek is also taught as a compulsory subject in Gymnasia and Lykia in Greece. Modern authors rarely write in Ancient Greek, though Jan Křesadlo wrote some poetry and prose in the language, and some volumes of Asterix and Harry Potter and the Philosopher's Stone have been translated into Ancient Greek. Oνόματα Kεχιασμένα (Onomata Kechiasmena) is the first magazine of crosswords and puzzles in Ancient Greek. Its first issue appeared in April 2015 as an annex to Hebdomada Aenigmatum. Alfred Rahlfs included a preface, a short history of the Septuagint text, and other front matter translated into Ancient Greek in his 1935 edition of the Septuagint; Robert Hanhart also included the introductory remarks to the 2006 revised Rahlfs–Hanhart edition in the language as well. Ancient Greek is also used by organizations and individuals, mainly Greek, who wish to denote their respect, admiration or preference for the use of this language. This use is sometimes considered graphical, nationalistic or funny. In any case, the fact that modern Greeks can still wholly or partly understand texts written in non-archaic forms of ancient Greek shows the affinity of modern Greek language to its ancestral predecessor. An isolated community near Trabzon, Turkey, an area where Pontic Greek is spoken, has been found to speak a variety of Greek that has parallels, both structurally and in its vocabulary, to Ancient Greek not present in other varieties. As few as 5,000 people speak the dialect but linguists believe that it is the closest living language to Ancient Greek. Ancient Greek is often used in the coinage of modern technical terms in the European languages: see English words of Greek origin. Latinized forms of Ancient Greek roots are used in many of the scientific names of species and in scientific terminology.    See alsoEdit  Ancient Greek grammar Proto-Greek language Ancient Greek dialects Mycenaean Greek Koine Greek Medieval Greek Modern Greek Varieties of Modern Greek Greek language Hellenic languages Exploring the Ancient Greek Language and Culture (competition) Greek alphabet Greek diacritics List of Greek phrases (mostly Ancient Greek) List of Greek words with English derivatives    ReferencesEdit     Further readingEdit  P. Chantraine (1968), Dictionnaire étymologique de la langue grecque, Klincksieck, Paris. Athenaze A series of textbooks on Ancient Greek published for school use Hansen, Hardy and Quinn, Gerald M. (1992) Greek: An Intensive Course, Fordham University Press Easterling, P & Handley, C. Greek Scripts: An illustrated introduction. London: Society for the Promotion of Hellenic Studies, 2001. ISBN 0-902984-17-9    External linksEdit  Online Greek resources – Dictionaries, grammar, virtual libraries, fonts, etc. Alpheios – Combines LSJ, Autenrieth, Smyth's grammar and inflection tables in a browser add-on for use on any web site Ancient Greek basic lexicon at the Global Lexicostatistical Database Ancient Greek Swadesh list of basic vocabulary words (from Wiktionary's Swadesh list appendix)    Grammar learningEdit  A more extensive grammar of the Ancient Greek language written by J. Rietveld Recitation of classics books Perseus Greek dictionaries Greek-Language.com – Information on the history of the Greek language, application of modern Linguistics to the study of Greek, and tools for learning Greek Free Lessons in Ancient Greek, Bilingual Libraries, Forum A critical survey of websites devoted to Ancient Greek Ancient Greek Tutorials – Berkeley Language Center of the University of California A Digital Tutorial For Ancient Greek Based on White's First Greek Book New Testament Greek Acropolis World News – A summary of the latest world news in Ancient Greek, Juan Coderch, University of St Andrews    Classical textsEdit  Perseus – Greek and Roman Materials Ancient Greek Texts","label":"foo"},{"text":"","label":"foo"},{"text":"","label":"foo"},{"text":"","label":"foo"},{"text":"Natural science is a branch of science concerned with the description, prediction, and understanding of natural phenomena, based on observational and empirical evidence. Validity, accuracy, and social mechanisms ensuring quality control, such as peer review and repeatability of findings, are amongst the criteria and methods used for this purpose. Natural science can be broken into two main branches: life science (or biological science) and physical science. Physical science is further broken down into branches, including physics, astronomy, chemistry, and Earth science. All of these branches of natural science are divided into many further specialized branches (also known as fields), and each of these is known as a \"natural science\". In Western society's analytic tradition, the empirical and especially natural sciences use tools from formal sciences, such as mathematics and logic, converting information about nature into measurements which can be explained as clear statements about the \"laws of nature\". The social sciences also use such methods, but rely more on qualitative research, so that they are sometimes called \"soft science\", whereas natural sciences, insofar as emphasizing quantifiable data produced, tested, and confirmed through the scientific method are sometimes called \"hard science\". Modern natural science succeeded more classical approaches to natural philosophy, usually traced to ancient Greece. Galileo, Descartes, Francis Bacon, and Newton debated the benefits of using approaches which were more mathematical and more experimental in a methodical way. Still, philosophical perspectives, conjectures, and presuppositions, often overlooked, remain requisite in natural science. Systematic data collection, including discovery science, succeed natural history, which emerged in the 16th century by describing and classifying plants, animals, minerals, and so on. Yet today, natural history suggests observational descriptions aimed at popular audiences.    CriteriaEdit   Philosophers of science have suggested a number of criteria, including the Karl Popper's controversial falsifiability criterion, to help them differentiate scientific endeavors from a non-scientific ones. Validity, accuracy, and social mechanisms ensuring quality control, such as peer review and repeatability of findings, are amongst the most respected criteria in present-day global scientific community.    Branches of natural scienceEdit     BiologyEdit   This field encompasses a set of disciplines that examines phenomena related to living organisms. The scale of study can range from sub-component biophysics up to complex ecologies. Biology is concerned with the characteristics, classification and behaviors of organisms, as well as how species were formed and their interactions with each other and the environment. The biological fields of botany, zoology, and medicine date back to early periods of civilization, while microbiology was introduced in the 17th century with the invention of the microscope. However, it was not until the 19th century that biology became a unified science. Once scientists discovered commonalities between all living things, it was decided they were best studied as a whole. Some key developments in biology were the discovery of genetics; Darwin's theory of evolution through natural selection; the germ theory of disease and the application of the techniques of chemistry and physics at the level of the cell or organic molecule. Modern biology is divided into subdisciplines by the type of organism and by the scale being studied. Molecular biology is the study of the fundamental chemistry of life, while cellular biology is the examination of the cell; the basic building block of all life. At a higher level, physiology looks at the internal structure of organism, while ecology looks at how various organisms interrelate.    ChemistryEdit   Constituting the scientific study of matter at the atomic and molecular scale, chemistry deals primarily with collections of atoms, such as gases, molecules, crystals, and metals. The composition, statistical properties, transformations and reactions of these materials are studied. Chemistry also involves understanding the properties and interactions of individual atoms and molecules for use in larger-scale applications. Most chemical processes can be studied directly in a laboratory, using a series of (often well-tested) techniques for manipulating materials, as well as an understanding of the underlying processes. Chemistry is often called \"the central science\" because of its role in connecting the other natural sciences. Early experiments in chemistry had their roots in the system of Alchemy, a set of beliefs combining mysticism with physical experiments. The science of chemistry began to develop with the work of Robert Boyle, the discoverer of gas, and Antoine Lavoisier, who developed the theory of the Conservation of mass. The discovery of the chemical elements and atomic theory began to systematize this science, and researchers developed a fundamental understanding of states of matter, ions, chemical bonds and chemical reactions. The success of this science led to a complementary chemical industry that now plays a significant role in the world economy.    PhysicsEdit   Physics embodies the study of the fundamental constituents of the universe, the forces and interactions they exert on one another, and the results produced by these interactions. In general, physics is regarded as the fundamental science, because all other natural sciences use and obey the principles and laws set down by the field. Physics relies heavily on mathematics as the logical framework for formulation and quantification of principles. The study of the principles of the universe has a long history and largely derives from direct observation and experimentation. The formulation of theories about the governing laws of the universe has been central to the study of physics from very early on, with philosophy gradually yielding to systematic, quantitative experimental testing and observation as the source of verification. Key historical developments in physics include Isaac Newton's theory of universal gravitation and classical mechanics, an understanding of electricity and its relation to magnetism, Einstein's theories of special and general relativity, the development of thermodynamics, and the quantum mechanical model of atomic and subatomic physics. The field of physics is extremely broad, and can include such diverse studies as quantum mechanics and theoretical physics, applied physics and optics. Modern physics is becoming increasingly specialized, where researchers tend to focus on a particular area rather than being \"universalists\" like Isaac Newton, Albert Einstein and Lev Landau, who worked in multiple areas.   = AstronomyEdit =  This discipline is the science of celestial objects and phenomena that originate outside the Earth's atmosphere. It is concerned with the evolution, physics, chemistry, meteorology, and motion of celestial objects, as well as the formation and development of the universe. Astronomy includes the examination, study and modeling of stars, planets, comets, galaxies and the cosmos. Most of the information used by astronomers is gathered by remote observation, although some laboratory reproduction of celestial phenomena has been performed (such as the molecular chemistry of the interstellar medium). While the origins of the study of celestial features and phenomena can be traced back to antiquity, the scientific methodology of this field began to develop in the middle of the 17th century. A key factor was Galileo's introduction of the telescope to examine the night sky in more detail. The mathematical treatment of astronomy began with Newton's development of celestial mechanics and the laws of gravitation, although it was triggered by earlier work of astronomers such as Kepler. By the 19th century, astronomy had developed into a formal science, with the introduction of instruments such as the spectroscope and photography, along with much-improved telescopes and the creation of professional observatories.    Earth scienceEdit   Earth science (also known as geoscience), is an all-embracing term for the sciences related to the planet Earth, including geology, geophysics, hydrology, meteorology, physical geography, oceanography, and soil science. Although mining and precious stones have been human interests throughout the history of civilization, the development of the related sciences of economic geology and mineralogy did not occur until the 18th century. The study of the earth, particularly palaeontology, blossomed in the 19th century. The growth of other disciplines, such as geophysics, in the 20th century led to the development of the theory of plate tectonics in the 1960s, which has had a similar effect on the Earth sciences as the theory of evolution had on biology. Earth sciences today are closely linked to petroleum and mineral resources, climate research and to environmental assessment and remediation.   = Atmospheric scienceEdit =  Though sometimes considered in conjunction with the earth sciences, due to the independent development of its concepts, techniques and practices and also the fact of it having a wide range of sub disciplines under its wing, the atmospheric science is also considered a separate branch of natural science. This field studies the characteristics of different layers of the atmosphere from ground level to the edge of the time. The timescale of study also varies from days to centuries. Sometimes the field also includes the study of climatic patterns on planets other than earth.   = OceanographyEdit =  The serious study of oceans began in the early to mid-20th century. As a field of natural science, it is relatively young but stand-alone programs offer specializations in the subject. Though some controversies remain as to the categorization of the field under earth sciences, interdisciplinary sciences or as a separate field in its own right, most modern workers in the field agree that it has matured to a state that it has its own paradigms and practices. As such a big family of related studies spanning every aspect of the oceans is now classified under this field.    Interdisciplinary studiesEdit  The distinctions between the natural science disciplines are not always sharp, and they share a number of cross-discipline fields. Physics plays a significant role in the other natural sciences, as represented by astrophysics, geophysics, chemical physics and biophysics. Likewise chemistry is represented by such fields as biochemistry, chemical biology, geochemistry and astrochemistry. A particular example of a scientific discipline that draws upon multiple natural sciences is environmental science. This field studies the interactions of physical, chemical, geological, and biological components of the environment, with a particular regard to the effect of human activities and the impact on biodiversity and sustainability. This science also draws upon expertise from other fields such as economics, law and social sciences. A comparable discipline is oceanography, as it draws upon a similar breadth of scientific disciplines. Oceanography is sub-categorized into more specialized cross-disciplines, such as physical oceanography and marine biology. As the marine ecosystem is very large and diverse, marine biology is further divided into many subfields, including specializations in particular species. There are also a subset of cross-disciplinary fields which, by the nature of the problems that they address, have strong currents that run counter to specialization. Put another way: In some fields of integrative application, specialists in more than one field are a key part of most dialog. Such integrative fields, for example, include nanoscience, astrobiology, and complex system informatics.    Materials scienceEdit   Materials science is a relatively new, interdisciplinary field which deals with the study of matter and their properties; as well as the discovery and design of new materials. Originally developed through the field of metallurgy, the study of the properties of materials and solids has now expanded into all materials. The field covers the chemistry, physics and engineering applications of materials including metals, ceramics, artificial polymers, and many others. The core of the field deals with relating structure of material with it properties. It is at the forefront of research in science and engineering. It is an important part of forensic engineering (the investigation of materials, products, structures or components that fail or do not operate or function as intended, causing personal injury or damage to property) and failure analysis, the latter being the key to understanding, for example, the cause of various aviation accidents. Many of the most pressing scientific problems that are faced today are due to the limitations of the materials that are available and, as a result, breakthroughs in this field are likely to have a significant impact on the future of technology. The basis of materials science involves studying the structure of materials, and relating them to their properties. Once, a materials scientists knows about this structure-property correlation, he/she can then go on to study the relative performance of a material in a certain application. The major determinants of the structure of a material and thus of its properties are its constituent chemical elements and the way in which it has been processed into its final form. These characteristics, taken together and related through the laws of thermodynamics and kinetics, govern a material's microstructure, and thus its properties.    HistoryEdit   Some scholars trace the origins of natural science as far back as pre-literate human societies, where understanding the natural world was necessary for survival. People observed and built up knowledge about the behavior of animals and the usefulness of plants as food and medicine, which was passed down from generation to generation. These primitive understandings gave way to more formalized inquiry around 3500 to 3000 BC in Mesopotamian and Ancient Egyptian cultures, which produced the first known written evidence of natural philosophy, the precursor of natural science. While the writings show an interest in astronomy, mathematics and other aspects of the physical world, the ultimate aim of inquiry about nature's workings was in all cases religious or mythological, not scientific. A tradition of scientific inquiry also emerged in Ancient China, where Taoist alchemists and philosophers experimented with elixirs to extend life and cure ailments. They focused on the yin and yang, or contrasting elements in nature; the yin was associated with femininity and coldness, while yang was associated with masculinity and warmth. The five phases – fire, earth, metal, wood and water – described a cycle of transformations in nature. Water turned into wood, which turned into fire when it burned. The ashes left by fire were earth. Using these principles, Chinese philosophers and doctors explored human anatomy, characterizing organs as predominantly yin or yang and understood the relationship between the pulse, the heart and the flow of blood in the body centuries before it became accepted in the West. Little evidence survives of how Ancient Indian cultures around the Indus River understood nature, but some of their perspectives may be reflected in the Vedas, a set of sacred Hindu texts. They reveal a conception of the universe as ever-expanding and constantly being recycled and reformed. Surgeons in the Ayurvedic tradition saw health and illness as a combination of three humors: wind, bile and phlegm. A healthy life was the result of a balance between these humors. In Ayurvedic thought, the body consisted of five elements: earth, water, fire, wind and empty space. Ayurvedic surgeons performed complex surgeries and developed a detailed understanding of human anatomy. Pre-Socratic philosophers in Ancient Greek culture brought natural philosophy a step closer to direct inquiry about cause and effect in nature between 600 and 400 BC, although an element of magic and mythology remained. Natural phenomena such as earthquakes and eclipses were explained increasingly in the context of nature itself instead of being attributed to angry gods. Thales of Miletus, an early philosopher who lived from 625 to 546 BC, explained earthquakes by theorizing that the world floated on water and that water was the fundamental element in nature. In the 5th century BC, Leucippus was an early exponent of atomism, the idea that the world is made up of fundamental indivisible particles. Pythagoras applied Greek innovations in mathematics to astronomy, and suggested that the earth was spherical. Specific practical discoveries, useful and impressive as they may be, neither qualify as science nor do they succeed in establishing a paradigm. Awareness of the sine qua non relation of formal science to observation and, mainly, quantitative studies has proven to be difficult to emerge. Pythagoreans came close to it when they applied numbers to the study of natural phenomena, however they do not appear to have been aware of the importance of methodology as distinct from concrete manipulations. A recent interpretation of Parmenides presents evidence that he was the philosopher who first proposed a method for doing natural science. Although 'peri physeos' is a poem, it may be viewed as an epistemological essay, an essay on method. Parmenides' ἐὸν may refer to a formal system, a calculus which, if 'superimposed' on empirical observations, can describe nature more precisely than natural languages. 'Physis' may be identical to ἐὸν.    Aristotelian natural philosophy (400 BC–1100 AD)Edit  Later Socratic and Platonic thought focused on ethics, morals and art and did not attempt an investigation of the physical world; Plato criticized pre-Socratic thinkers as materialists and anti-religionists. Aristotle, however, a student of Plato who lived from 384 to 322 BC, paid closer attention to the natural world in his philosophy. In his History of Animals, he described the inner workings of 110 species, including the stingray, catfish and bee. He investigated chick embryos by breaking open eggs and observing them at various stages of development. Aristotle's works were influential through the 19th century, and he is considered to be the father of biology. He also presented philosophies about physics, nature and astronomy using inductive reasoning in his works Physics and Meteorology.  While Aristotle considered natural philosophy more seriously than his predecessors, he approached it as a theoretical branch of science. Still, inspired by his work, Ancient Roman philosophers of the early 1st century AD, including Lucretius, Seneca and Pliny the Elder, wrote treatises that dealt with the rules of the natural world in varying degrees of depth. Many Ancient Roman Neoplatonists of the 3rd to the 6th centuries also adapted Aristotle's teachings on the physical world to a philosophy that emphasized spiritualism. Early medieval philosophers including Macrobius, Calcidius and Martianus Capella also examined the physical world, largely from a cosmological and cosmographical perspective, putting forth theories on the arrangement of celestial bodies and the heavens, which were posited as being composed of aether. Aristotle's works on natural philosophy continued to be translated and studied amid the rise of the Byzantine Empire and Islam in the Middle East. A revival in mathematics and science took place during the time of the Abbasid Caliphate from the 9th century onward, when Muslim scholars expanded upon Greek and Indian natural philosophy. The words alcohol, algebra and zenith all have Arabic roots.    Medieval natural philosophy (1100–1600)Edit  Aristotle's works and other Greek natural philosophy did not reach the West until about the middle of the 12th century, when works were translated from Greek and Arabic into Latin. The development of European civilization later in the Middle Ages brought with it further advances in natural philosophy. European inventions such as the horseshoe, horse collar and crop rotation allowed for rapid population growth, eventually giving way to urbanization and the foundation of schools connected to monasteries and cathedrals in modern-day France and England. Aided by the schools, an approach to Christian theology developed that sought to answer questions about nature and other subjects using logic. This approach, however, was seen by some detractors as heresy. By the 12th century, Western European scholars and philosophers came into contact with a body of knowledge of which they had previously been ignorant: a large corpus of works in Greek and Arabic that were preserved by Islamic scholars. Through translation into Latin, Western Europe was introduced to Aristotle and his natural philosophy. These works were taught at new universities in Paris and Oxford by the early 13th century, although the practice was frowned upon by the Catholic church. A 1210 decree from the Synod of Paris ordered that \"no lectures are to be held in Paris either publicly or privately using Aristotle's books on natural philosophy or the commentaries, and we forbid all this under pain of excommunication.\" In the late Middle Ages, Spanish philosopher Dominicus Gundissalinus translated a treatise by the earlier Arab scholar Al-Farabi called On the Sciences into Latin, calling the study of the mechanics of nature scientia naturalis, or natural science. Gundissalinus also proposed his own classification of the natural sciences in his 1150 work On the Division of Philosophy. This was the first detailed classification of the sciences based on Greek and Arab philosophy to reach Western Europe. Gundissalinus defined natural science as \"the science considering only things unabstracted and with motion,\" as opposed to mathematics and sciences that rely on mathematics. Following Al-Farabi, he then separated the sciences into eight parts, including physics, cosmology, meteorology, minerals science and plant and animal science. Later philosophers made their own classifications of the natural sciences. Robert Kilwardby wrote On the Order of the Sciences in the 13th century that classed medicine as a mechanical science, along with agriculture, hunting and theater while defining natural science as the science that deals with bodies in motion. Roger Bacon, an English friar and philosopher, wrote that natural science dealt with \"a principle of motion and rest, as in the parts of the elements of fire, air, earth and water, and in all inanimate things made from them.\" These sciences also covered plants, animals and celestial bodies. Later in the 13th century, Catholic priest and theologian Thomas Aquinas defined natural science as dealing with \"mobile beings\" and \"things which depend on matter not only for their existence, but also for their definition.\" There was wide agreement among scholars in medieval times that natural science was about bodies in motion, although there was division about the inclusion of fields including medicine, music and perspective. Philosophers pondered questions including the existence of a vacuum, whether motion could produce heat, the colors of rainbows, the motion of the earth, whether elemental chemicals exist and where in the atmosphere rain is formed. In the centuries up through the end of the Middle Ages, natural science was often mingled with philosophies about magic and the occult. Natural philosophy appeared in a wide range of forms, from treatises to encyclopedias to commentaries on Aristotle. The interaction between natural philosophy and Christianity was complex during this period; some early theologians, including Tatian and Eusebius, considered natural philosophy an outcropping of pagan Greek science and were suspicious of it. Although some later Christian philosophers, including Aquinas, came to see natural science as a means of interpreting scripture, this suspicion persisted until the 12th and 13th centuries. The Condemnation of 1277, which forbade setting philosophy on a level equal with theology and the debate of religious constructs in a scientific context, showed the persistence with which Catholic leaders resisted the development of natural philosophy even from a theological perspective. Aquinas and Albertus Magnus, another Catholic theologian of the era, sought to distance theology from science in their works. \"I don't see what one's interpretation of Aristotle has to do with the teaching of the faith,\" he wrote in 1271.    Newton and the scientific revolution (1600–1800)Edit  By the 16th and 17th centuries, natural philosophy underwent an evolution beyond commentary on Aristotle as more early Greek philosophy was uncovered and translated. The invention of the printing press in the 15th century, the invention of the microscope and telescope, and the Protestant Reformation fundamentally altered the social context in which scientific inquiry evolved in the West. Christopher Columbus's discovery of a new world changed perceptions about the physical makeup of the world, while observations by Copernicus, Tyco Brahe and Galileo brought a more accurate picture of the solar system as heliocentric and proved many of Aristotle's theories about the heavenly bodies false. A number of 17th-century philosophers, including Thomas Hobbes, John Locke and Francis Bacon made a break from the past by rejecting Aristotle and his medieval followers outright, calling their approach to natural philosophy as superficial. The titles of Galileo's work Two New Sciences and Johannes Kepler's New Astronomy underscored the atmosphere of change that took hold in the 17th century as Aristotle was dismissed in favor of novel methods of inquiry into the natural world. Bacon was instrumental in popularizing this change; he argued that people should use the arts and sciences to gain dominion over nature. To achieve this, he wrote that \"human life [must] be endowed with new discoveries and powers.\" He defined natural philosophy as \"the knowledge of Causes and secret motions of things; and enlarging the bounds of Human Empire, to the effecting of all things possible.\" Bacon proposed scientific inquiry supported by the state and fed by the collaborative research of scientists, a vision that was unprecedented in its scope, ambition and form at the time. Natural philosophers came to view nature increasingly as a mechanism that could be taken apart and understood, much like a complex clock. Natural philosophers including Isaac Newton, Evangelista Torricelli and Francesco Redi conducted experiments focusing on the flow of water, measuring atmospheric pressure using a barometer and disproving spontaneous generation. Scientific societies and scientific journals emerged and were spread widely through the printing press, touching off the scientific revolution. Newton in 1687 published his The Mathematical Principles of Natural Philosophy, or Principia Mathematica, which set the groundwork for physical laws that remained current until the 19th century. Some modern scholars, including Andrew Cunningham, Perry Williams and Floris Cohen, argue that natural philosophy is not properly called a science, and that genuine scientific inquiry began only with the scientific revolution. According to Cohen, \"the emancipation of science from an overarching entity called 'natural philosophy' is one defining characteristic of the Scientific Revolution.\" Other historians of science, including Edward Grant, contend that the scientific revolution that blossomed in the 17th, 18th and 19th centuries occurred when principles learned in the exact sciences of optics, mechanics and astronomy began to be applied to questions raised by natural philosophy. Grant argues that Newton attempted to expose the mathematical basis of nature – the immutable rules it obeyed – and in doing so joined natural philosophy and mathematics for the first time, producing an early work of modern physics.  The scientific revolution, which began to take hold in the 17th century, represented a sharp break from Aristotelian modes of inquiry. One of its principal advances was the use of the scientific method to investigate nature. Data was collected and repeatable measurements made in experiments. Scientists then formed hypotheses to explain the results of these experiments. The hypothesis was then tested using the principle of falsifiability to prove or disprove its accuracy. The natural sciences continued to be called natural philosophy, but the adoption of the scientific method took science beyond the realm of philosophical conjecture and introduced a more structured way of examining nature. Newton, an English mathematician and physicist, was the seminal figure in the scientific revolution. Drawing on advances made in astronomy by Copernicus, Brahe and Kepler, Newton derived the universal law of gravitation and laws of motion. These laws applied both on earth and in outer space, uniting two spheres of the physical world previously thought to function independently of each other, according to separate physical rules. Newton, for example, showed that the tides were caused by the gravitational pull of the moon. Another of Newton's advances was to make mathematics a powerful explanatory tool for natural phenomena. While natural philosophers had long used mathematics as a means of measurement and analysis, its principles were not used as a means of understanding cause and effect in nature until Newton. In the 18th century and 19th century, scientists including Charles-Augustin de Coulomb, Alessandro Volta, and Michael Faraday built upon Newtonian mechanics by exploring electromagnetism, or the interplay of forces with positive and negative charges on electrically charged particles. Faraday proposed that forces in nature operated in \"fields\" that filled space. The idea of fields contrasted with the Newtonian construct of gravitation as simply \"action at a distance\", or the attraction of objects with nothing in the space between them to intervene. James Clerk Maxwell in the 19th century unified these discoveries in a coherent theory of electrodynamics. Using mathematical equations and experimentation, Maxwell discovered that space was filled with charged particles that could act upon themselves and each other, and that they were a medium for the transmission of charged waves. Significant advances in chemistry also took place during the scientific revolution. Antoine Lavoisier, a French chemist, refuted the phlogiston theory, which posited that things burned by releasing \"phlogiston\" into the air. Joseph Priestley had discovered oxygen in the 18th century, but Lavoisier discovered that combustion was the result of oxidation. He also constructed a table of 33 elements and invented modern chemical nomenclature. Formal biological science remained in its infancy in the 18th century, when the focus lay upon the classification and categorization of natural life. This growth in natural history was led by Carolus Linnaeus, whose 1735 taxonomy of the natural world is still in use. Linnaeus in the 1750s introduced scientific names for all his species.    19th-century developments (1800–1900)Edit   By the 19th century, the study of science had come into the purview of professionals and institutions. In so doing, it gradually acquired the more modern name of natural science. The term scientist was coined by William Whewell in an 1834 review of Mary Somerville's On the Connexion of the Sciences. But the word did not enter general use until nearly the end of the same century.    Modern natural science (1900–present)Edit  According to a famous 1923 textbook Thermodynamics and the Free Energy of Chemical Substances by the American chemist Gilbert N. Lewis and the American physical chemist Merle Randall, the natural sciences contain three great branches:  Aside from the logical and mathematical sciences, there are three great branches of natural science which stand apart by reason of the variety of far reaching deductions drawn from a small number of primary postulates — they are mechanics, electrodynamics, and thermodynamics.  Today, natural sciences are more commonly divided into life sciences, such as botany and zoology; and physical sciences, which include physics, chemistry, geology, astronomy and materials science.    See alsoEdit   Empiricism Branches of science List of academic disciplines and sub-disciplines    ReferencesEdit     BibliographyEdit     Further readingEdit  Defining Natural Sciences Ledoux,S. F., 2002: Defining Natural Sciences, Behaviorology Today, 5(1), 34-36.    External linksEdit  The History of Recent Science and Technology Natural Sciences Information on the Natural Sciences degree programme at Durham University. Natural Sciences Contains updated information on research in the Natural Sciences including biology, geography and the applied life and earth sciences. Natural Sciences Information on the Natural Sciences degree programme at the University of Bath which includes the Biological Sciences, Chemistry, Pharmacology, Physics and Environmental Studies. Reviews of Books About Natural Science This site contains over 50 previously published reviews of books about natural science, plus selected essays on timely topics in natural science. Scientific Grant Awards Database Contains details of over 2,000,000 scientific research projects conducted over the past 25 years. Natural Sciences Tripos Provides information on the framework within which most of the natural science is taught at the University of Cambridge. E!Science Up-to-date science news aggregator from major sources including universities.","label":"foo"},{"text":"Before the 20th century, the term matter included ordinary matter composed of atoms and excluded other energy phenomena such as light or sound. This concept of matter may be generalized from atoms to include any objects having mass even when at rest, but this is ill-defined because an object's mass can arise from its (possibly massless) constituents' motion and interaction energies. Thus, matter does not have a universal definition, nor is it a fundamental concept in physics today. Matter is also used loosely as a general term for the substance that makes up all observable physical objects. All the objects from everyday life that we can bump into, touch or squeeze are composed of atoms. This atomic matter is in turn made up of interacting subatomic particles—usually a nucleus of protons and neutrons, and a cloud of orbiting electrons. Typically, science considers these composite particles matter because they have both rest mass and volume. By contrast, massless particles, such as photons, are not considered matter, because they have neither rest mass nor volume. However, not all particles with rest mass have a classical volume, since fundamental particles such as quarks and leptons (sometimes equated with matter) are considered \"point particles\" with no effective size or volume. Nevertheless, quarks and leptons together make up \"ordinary matter\", and their interactions contribute to the effective volume of the composite particles that make up ordinary matter. Matter commonly exists in four states (or phases): solid, liquid and gas, and plasma. However, advances in experimental techniques have revealed other previously theoretical phases, such as Bose–Einstein condensates and fermionic condensates. A focus on an elementary-particle view of matter also leads to new phases of matter, such as the quark–gluon plasma. For much of the history of the natural sciences people have contemplated the exact nature of matter. The idea that matter was built of discrete building blocks, the so-called particulate theory of matter, was first put forward by the Greek philosophers Leucippus (~490 BC) and Democritus (~470–380 BC). Matter should not be confused with mass, as the two are not quite the same in modern physics. For example, mass is a conserved quantity, which means that its value is unchanging through time, within closed systems. However, matter is not conserved in such systems, although this is not obvious in ordinary conditions on Earth, where matter is approximately conserved. Still, special relativity shows that matter may disappear by conversion into energy, even inside closed systems, and it can also be created from energy, within such systems. However, because mass (like energy) can neither be created nor destroyed, the quantity of mass and the quantity of energy remain the same during a transformation of matter (which represents a certain amount of energy) into non-material (i.e., non-matter) energy. This is also true in the reverse transformation of energy into matter. Different fields of science use the term matter in different, and sometimes incompatible, ways. Some of these ways are based on loose historical meanings, from a time when there was no reason to distinguish mass and matter. As such, there is no single universally agreed scientific meaning of the word \"matter\". Scientifically, the term \"mass\" is well-defined, but \"matter\" is not. Sometimes in the field of physics \"matter\" is simply equated with particles that exhibit rest mass (i.e., that cannot travel at the speed of light), such as quarks and leptons. However, in both physics and chemistry, matter exhibits both wave-like and particle-like properties, the so-called wave–particle duality.    Definition     Common definition   The common definition of matter is anything that has mass and volume (occupies space). For example, a car would be said to be made of matter, as it occupies space, and has mass. The observation that matter occupies space goes back to antiquity. However, an explanation for why matter occupies space is recent, and is argued to be a result of the phenomenon described in the Pauli exclusion principle. Two particular examples where the exclusion principle clearly relates matter to the occupation of space are white dwarf stars and neutron stars, discussed further below.    Relativity   In the context of relativity, mass is not an additive quantity, in the sense that one can add the rest masses of particles in a system to get the total rest mass of the system. Thus, in relativity usually a more general view is that it is not the sum of rest masses, but the energy–momentum tensor that quantifies the amount of matter. This tensor gives the rest mass for the entire system. \"Matter\" therefore is sometimes considered as anything that contributes to the energy–momentum of a system, that is, anything that is not purely gravity. This view is commonly held in fields that deal with general relativity such as cosmology. In this view, light and other massless particles and fields are part of matter. The reason for this is that in this definition, electromagnetic radiation (such as light) as well as the energy of electromagnetic fields contributes to the mass of systems, and therefore appears to add matter to them. For example, light radiation (or thermal radiation) trapped inside a box would contribute to the mass of the box, as would any kind of energy inside the box, including the kinetic energy of particles held by the box. Nevertheless, isolated individual particles of light (photons) and the isolated kinetic energy of massive particles, are normally not considered to be matter. A difference between matter and mass therefore may seem to arise when single particles are examined. In such cases, the mass of single photons is zero. For particles with rest mass, such as leptons and quarks, isolation of the particle in a frame where it is not moving, removes its kinetic energy. A source of definition difficulty in relativity arises from two definitions of mass in common use, one of which is formally equivalent to total energy (and is thus observer dependent), and the other of which is referred to as rest mass or invariant mass and is independent of the observer. Only \"rest mass\" is loosely equated with matter (since it can be weighed). Invariant mass is usually applied in physics to unbound systems of particles. However, energies which contribute to the \"invariant mass\" may be weighed also in special circumstances, such as when a system that has invariant mass is confined and has no net momentum (as in the box example above). Thus, a photon with no mass may (confusingly) still add mass to a system in which it is trapped. The same is true of the kinetic energy of particles, which by definition is not part of their rest mass, but which does add rest mass to systems in which these particles reside (an example is the mass added by the motion of gas molecules of a bottle of gas, or by the thermal energy of any hot object). Since such mass (kinetic energies of particles, the energy of trapped electromagnetic radiation and stored potential energy of repulsive fields) is measured as part of the mass of ordinary matter in complex systems, the \"matter\" status of \"massless particles\" and fields of force becomes unclear in such systems. These problems contribute to the lack of a rigorous definition of matter in science, although mass is easier to define as the total stress–energy above (this is also what is weighed on a scale, and what is the source of gravity).    Atoms definition  A definition of \"matter\" based on its physical and chemical structure is: matter is made up of atoms. As an example, deoxyribonucleic acid molecules (DNA) are matter under this definition because they are made of atoms. This definition can extend to include charged atoms and molecules, so as to include plasmas (gases of ions) and electrolytes (ionic solutions), which are not obviously included in the atoms definition. Alternatively, one can adopt the protons, neutrons, and electrons definition.    Protons, neutrons and electrons definition  A definition of \"matter\" more fine-scale than the atoms and molecules definition is: matter is made up of what atoms and molecules are made of, meaning anything made of positively charged protons, neutral neutrons, and negatively charged electrons. This definition goes beyond atoms and molecules, however, to include substances made from these building blocks that are not simply atoms or molecules, for example white dwarf matter—typically, carbon and oxygen nuclei in a sea of degenerate electrons. At a microscopic level, the constituent \"particles\" of matter such as protons, neutrons, and electrons obey the laws of quantum mechanics and exhibit wave–particle duality. At an even deeper level, protons and neutrons are made up of quarks and the force fields (gluons) that bind them together (see Quarks and leptons definition below).    Quarks and leptons definition   As seen in the above discussion, many early definitions of what can be called ordinary matter were based upon its structure or building blocks. On the scale of elementary particles, a definition that follows this tradition can be stated as: ordinary matter is everything that is composed of elementary fermions, namely quarks and leptons. The connection between these formulations follows. Leptons (the most famous being the electron), and quarks (of which baryons, such as protons and neutrons, are made) combine to form atoms, which in turn form molecules. Because atoms and molecules are said to be matter, it is natural to phrase the definition as: ordinary matter is anything that is made of the same things that atoms and molecules are made of. (However, notice that one also can make from these building blocks matter that is not atoms or molecules.) Then, because electrons are leptons, and protons, and neutrons are made of quarks, this definition in turn leads to the definition of matter as being quarks and leptons, which are the two types of elementary fermions. Carithers and Grannis state: Ordinary matter is composed entirely of first-generation particles, namely the [up] and [down] quarks, plus the electron and its neutrino. (Higher generations particles quickly decay into first-generation particles, and thus are not commonly encountered.) This definition of ordinary matter is more subtle than it first appears. All the particles that make up ordinary matter (leptons and quarks) are elementary fermions, while all the force carriers are elementary bosons. The W and Z bosons that mediate the weak force are not made of quarks or leptons, and so are not ordinary matter, even if they have mass. In other words, mass is not something that is exclusive to ordinary matter. The quark–lepton definition of ordinary matter, however, identifies not only the elementary building blocks of matter, but also includes composites made from the constituents (atoms and molecules, for example). Such composites contain an interaction energy that holds the constituents together, and may constitute the bulk of the mass of the composite. As an example, to a great extent, the mass of an atom is simply the sum of the masses of its constituent protons, neutrons and electrons. However, digging deeper, the protons and neutrons are made up of quarks bound together by gluon fields (see dynamics of quantum chromodynamics) and these gluons fields contribute significantly to the mass of hadrons. In other words, most of what composes the \"mass\" of ordinary matter is due to the binding energy of quarks within protons and neutrons. For example, the sum of the mass of the three quarks in a nucleon is approximately 7001125000000000000♠12.5 MeV/c2, which is low compared to the mass of a nucleon (approximately 7002938000000000000♠938 MeV/c2). The bottom line is that most of the mass of everyday objects comes from the interaction energy of its elementary components.    Smaller building blocks issue  The Standard Model groups matter particles into three generations, where each generation consists of two quarks and two leptons. The first generation is the up and down quarks, the electron and the electron neutrino; the second includes the charm and strange quarks, the muon and the muon neutrino; the third generation consists of the top and bottom quarks and the tau and tau neutrino. The most natural explanation for this would be that quarks and leptons of higher generations are excited states of the first generations. If this turns out to be the case, it would imply that quarks and leptons are composite particles, rather than elementary particles.    Structure  In particle physics, fermions are particles that obey Fermi–Dirac statistics. Fermions can be elementary, like the electron—or composite, like the proton and neutron. In the Standard Model, there are two types of elementary fermions: quarks and leptons, which are discussed next.    Quarks   Quarks are particles of spin-1⁄2, implying that they are fermions. They carry an electric charge of −1⁄3 e (down-type quarks) or +2⁄3 e (up-type quarks). For comparison, an electron has a charge of −1 e. They also carry colour charge, which is the equivalent of the electric charge for the strong interaction. Quarks also undergo radioactive decay, meaning that they are subject to the weak interaction. Quarks are massive particles, and therefore are also subject to gravity.   = Baryonic matter =  Baryons are strongly interacting fermions, and so are subject to Fermi–Dirac statistics. Amongst the baryons are the protons and neutrons, which occur in atomic nuclei, but many other unstable baryons exist as well. The term baryon usually refers to triquarks—particles made of three quarks. \"Exotic\" baryons made of four quarks and one antiquark are known as the pentaquarks, but their existence is not generally accepted. Baryonic matter is the part of the universe that is made of baryons (including all atoms). This part of the universe does not include dark energy, dark matter, black holes or various forms of degenerate matter, such as compose white dwarf stars and neutron stars. Microwave light seen by Wilkinson Microwave Anisotropy Probe (WMAP), suggests that only about 4.6% of that part of the universe within range of the best telescopes (that is, matter that may be visible because light could reach us from it), is made of baryonic matter. About 23% is dark matter, and about 72% is dark energy.   = Degenerate matter =  In physics, degenerate matter refers to the ground state of a gas of fermions at a temperature near absolute zero. The Pauli exclusion principle requires that only two fermions can occupy a quantum state, one spin-up and the other spin-down. Hence, at zero temperature, the fermions fill up sufficient levels to accommodate all the available fermions—and in the case of many fermions, the maximum kinetic energy (called the Fermi energy) and the pressure of the gas becomes very large, and depends on the number of fermions rather than the temperature, unlike normal states of matter. Degenerate matter is thought to occur during the evolution of heavy stars. The demonstration by Subrahmanyan Chandrasekhar that white dwarf stars have a maximum allowed mass because of the exclusion principle caused a revolution in the theory of star evolution. Degenerate matter includes the part of the universe that is made up of neutron stars and white dwarfs.   = Strange matter =  Strange matter is a particular form of quark matter, usually thought of as a liquid of up, down, and strange quarks. It is contrasted with nuclear matter, which is a liquid of neutrons and protons (which themselves are built out of up and down quarks), and with non-strange quark matter, which is a quark liquid that contains only up and down quarks. At high enough density, strange matter is expected to be color superconducting. Strange matter is hypothesized to occur in the core of neutron stars, or, more speculatively, as isolated droplets that may vary in size from femtometers (strangelets) to kilometers (quark stars).    Two meanings of the term \"strange matter\"  In particle physics and astrophysics, the term is used in two ways, one broader and the other more specific. The broader meaning is just quark matter that contains three flavors of quarks: up, down, and strange. In this definition, there is a critical pressure and an associated critical density, and when nuclear matter (made of protons and neutrons) is compressed beyond this density, the protons and neutrons dissociate into quarks, yielding quark matter (probably strange matter). The narrower meaning is quark matter that is more stable than nuclear matter. The idea that this could happen is the \"strange matter hypothesis\" of Bodmer and Witten. In this definition, the critical pressure is zero: the true ground state of matter is always quark matter. The nuclei that we see in the matter around us, which are droplets of nuclear matter, are actually metastable, and given enough time (or the right external stimulus) would decay into droplets of strange matter, i.e. strangelets.    Leptons   Leptons are particles of spin-1⁄2, meaning that they are fermions. They carry an electric charge of −1 e (charged leptons) or 0 e (neutrinos). Unlike quarks, leptons do not carry colour charge, meaning that they do not experience the strong interaction. Leptons also undergo radioactive decay, meaning that they are subject to the weak interaction. Leptons are massive particles, therefore are subject to gravity.    Phases   In bulk, matter can exist in several different forms, or states of aggregation, known as phases, depending on ambient pressure, temperature and volume. A phase is a form of matter that has a relatively uniform chemical composition and physical properties (such as density, specific heat, refractive index, and so forth). These phases include the three familiar ones (solids, liquids, and gases), as well as more exotic states of matter (such as plasmas, superfluids, supersolids, Bose–Einstein condensates, ...). A fluid may be a liquid, gas or plasma. There are also paramagnetic and ferromagnetic phases of magnetic materials. As conditions change, matter may change from one phase into another. These phenomena are called phase transitions, and are studied in the field of thermodynamics. In nanomaterials, the vastly increased ratio of surface area to volume results in matter that can exhibit properties entirely different from those of bulk material, and not well described by any bulk phase (see nanomaterials for more details). Phases are sometimes called states of matter, but this term can lead to confusion with thermodynamic states. For example, two gases maintained at different pressures are in different thermodynamic states (different pressures), but in the same phase (both are gases).    Antimatter   In particle physics and quantum chemistry, antimatter is matter that is composed of the antiparticles of those that constitute ordinary matter. If a particle and its antiparticle come into contact with each other, the two annihilate; that is, they may both be converted into other particles with equal energy in accordance with Einstein's equation E = mc2. These new particles may be high-energy photons (gamma rays) or other particle–antiparticle pairs. The resulting particles are endowed with an amount of kinetic energy equal to the difference between the rest mass of the products of the annihilation and the rest mass of the original particle–antiparticle pair, which is often quite large. Antimatter is not found naturally on Earth, except very briefly and in vanishingly small quantities (as the result of radioactive decay, lightning or cosmic rays). This is because antimatter that came to exist on Earth outside the confines of a suitable physics laboratory would almost instantly meet the ordinary matter that Earth is made of, and be annihilated. Antiparticles and some stable antimatter (such as antihydrogen) can be made in tiny amounts, but not in enough quantity to do more than test a few of its theoretical properties. There is considerable speculation both in science and science fiction as to why the observable universe is apparently almost entirely matter, and whether other places are almost entirely antimatter instead. In the early universe, it is thought that matter and antimatter were equally represented, and the disappearance of antimatter requires an asymmetry in physical laws called the charge parity (or CP symmetry) violation. CP symmetry violation can be obtained from the Standard Model, but at this time the apparent asymmetry of matter and antimatter in the visible universe is one of the great unsolved problems in physics. Possible processes by which it came about are explored in more detail under baryogenesis.    Other types   Ordinary matter, in the quarks and leptons definition, constitutes about 4% of the energy of the observable universe. The remaining energy is theorized to be due to exotic forms, of which 23% is dark matter and 73% is dark energy.    Dark matter   In astrophysics and cosmology, dark matter is matter of unknown composition that does not emit or reflect enough electromagnetic radiation to be observed directly, but whose presence can be inferred from gravitational effects on visible matter. Observational evidence of the early universe and the big bang theory require that this matter have energy and mass, but is not composed of either elementary fermions (as above) OR gauge bosons. The commonly accepted view is that most of the dark matter is non-baryonic in nature. As such, it is composed of particles as yet unobserved in the laboratory. Perhaps they are supersymmetric particles, which are not Standard Model particles, but relics formed at very high energies in the early phase of the universe and still floating about.    Dark energy   In cosmology, dark energy is the name given to the antigravitating influence that is accelerating the rate of expansion of the universe. It is known not to be composed of known particles like protons, neutrons or electrons, nor of the particles of dark matter, because these all gravitate.    Exotic matter   Exotic matter is a hypothetical concept of particle physics. It covers any material that violates one or more classical conditions or is not made of known baryonic particles. Such materials would possess qualities like negative mass or being repelled rather than attracted by gravity.    Historical development     Origins  The pre-Socratics were among the first recorded speculators about the underlying nature of the visible world. Thales (c. 624 BC–c. 546 BC) regarded water as the fundamental material of the world. Anaximander (c. 610 BC–c. 546 BC) posited that the basic material was wholly characterless or limitless: the Infinite (apeiron). Anaximenes (flourished 585 BC, d. 528 BC) posited that the basic stuff was pneuma or air. Heraclitus (c. 535–c. 475 BC) seems to say the basic element is fire, though perhaps he means that all is change. Empedocles (c. 490–430 BC) spoke of four elements of which everything was made: earth, water, air, and fire. Meanwhile, Parmenides argued that change does not exist, and Democritus argued that everything is composed of minuscule, inert bodies of all shapes called atoms, a philosophy called atomism. All of these notions had deep philosophical problems. Aristotle (384 BC – 322 BC) was the first to put the conception on a sound philosophical basis, which he did in his natural philosophy, especially in Physics book I. He adopted as reasonable suppositions the four Empedoclean elements, but added a fifth, aether. Nevertheless, these elements are not basic in Aristotle's mind. Rather they, like everything else in the visible world, are composed of the basic principles matter and form. The word Aristotle uses for matter, ὑλη (hyle or hule), can be literally translated as wood or timber, that is, \"raw material\" for building. Indeed, Aristotle's conception of matter is intrinsically linked to something being made or composed. In other words, in contrast to the early modern conception of matter as simply occupying space, matter for Aristotle is definitionally linked to process or change: matter is what underlies a change of substance. For example, a horse eats grass: the horse changes the grass into itself; the grass as such does not persist in the horse, but some aspect of it—its matter—does. The matter is not specifically described (e.g., as atoms), but consists of whatever persists in the change of substance from grass to horse. Matter in this understanding does not exist independently (i.e., as a substance), but exists interdependently (i.e., as a \"principle\") with form and only insofar as it underlies change. It can be helpful to conceive of the relationship of matter and form as very similar to that between parts and whole. For Aristotle, matter as such can only receive actuality from form; it has no activity or actuality in itself, similar to the way that parts as such only have their existence in a whole (otherwise they would be independent wholes).    Early modernity  René Descartes (1596–1650) originated the modern conception of matter. He was primarily a geometer. Instead of, like Aristotle, deducing the existence of matter from the physical reality of change, Descartes arbitrarily postulated matter to be an abstract, mathematical substance that occupies space:  For Descartes, matter has only the property of extension, so its only activity aside from locomotion is to exclude other bodies: this is the mechanical philosophy. Descartes makes an absolute distinction between mind, which he defines as unextended, thinking substance, and matter, which he defines as unthinking, extended substance. They are independent things. In contrast, Aristotle defines matter and the formal/forming principle as complementary principles that together compose one independent thing (substance). In short, Aristotle defines matter (roughly speaking) as what things are actually made of (with a potential independent existence), but Descartes elevates matter to an actual independent thing in itself. The continuity and difference between Descartes' and Aristotle's conceptions is noteworthy. In both conceptions, matter is passive or inert. In the respective conceptions matter has different relationships to intelligence. For Aristotle, matter and intelligence (form) exist together in an interdependent relationship, whereas for Descartes, matter and intelligence (mind) are definitionally opposed, independent substances. Descartes' justification for restricting the inherent qualities of matter to extension is its permanence, but his real criterion is not permanence (which equally applied to color and resistance), but his desire to use geometry to explain all material properties. Like Descartes, Hobbes, Boyle, and Locke argued that the inherent properties of bodies were limited to extension, and that so-called secondary qualities, like color, were only products of human perception. Isaac Newton (1643–1727) inherited Descartes' mechanical conception of matter. In the third of his \"Rules of Reasoning in Philosophy\", Newton lists the universal qualities of matter as \"extension, hardness, impenetrability, mobility, and inertia\". Similarly in Optics he conjectures that God created matter as \"solid, massy, hard, impenetrable, movable particles\", which were \"...even so very hard as never to wear or break in pieces\". The \"primary\" properties of matter were amenable to mathematical description, unlike \"secondary\" qualities such as color or taste. Like Descartes, Newton rejected the essential nature of secondary qualities. Newton developed Descartes' notion of matter by restoring to matter intrinsic properties in addition to extension (at least on a limited basis), such as mass. Newton's use of gravitational force, which worked \"at a distance\", effectively repudiated Descartes' mechanics, in which interactions happened exclusively by contact. Though Newton's gravity would seem to be a power of bodies, Newton himself did not admit it to be an essential property of matter. Carrying the logic forward more consistently, Joseph Priestley argued that corporeal properties transcend contact mechanics: chemical properties require the capacity for attraction. He argued matter has other inherent powers besides the so-called primary qualities of Descartes, et al. Since Priestley's time, there has been a massive expansion in knowledge of the constituents of the material world (viz., molecules, atoms, subatomic particles), but there has been no further development in the definition of matter. Rather the question has been set aside. Noam Chomsky summarizes the situation that has prevailed since that time:  So matter is whatever physics studies and the object of study of physics is matter: there is no independent general definition of matter, apart from its fitting into the methodology of measurement and controlled experimentation. In sum, the boundaries between what constitutes matter and everything else remains as vague as the demarcation problem of delimiting science from everything else.    Late nineteenth and early twentieth centuries  In the 19th century, following the development of the periodic table, and of atomic theory, atoms were seen as being the fundamental constituents of matter; atoms formed molecules and compounds. The common definition in terms of occupying space and having mass is in contrast with most physical and chemical definitions of matter, which rely instead upon its structure and upon attributes not necessarily related to volume and mass. At the turn of the nineteenth century, the knowledge of matter began a rapid evolution. Aspects of the Newtonian view still held sway. James Clerk Maxwell discussed matter in his work Matter and Motion. He carefully separates \"matter\" from space and time, and defines it in terms of the object referred to in Newton's first law of motion. However, the Newtonian picture was not the whole story. In the 19th century, the term \"matter\" was actively discussed by a host of scientists and philosophers, and a brief outline can be found in Levere. A textbook discussion from 1870 suggests matter is what is made up of atoms:  Three divisions of matter are recognized in science: masses, molecules and atoms. A Mass of matter is any portion of matter appreciable by the senses. A Molecule is the smallest particle of matter into which a body can be divided without losing its identity. An Atom is a still smaller particle produced by division of a molecule.  Rather than simply having the attributes of mass and occupying space, matter was held to have chemical and electrical properties. The famous physicist J. J. Thomson wrote about the \"constitution of matter\" and was concerned with the possible connection between matter and electrical charge.    Later developments  There is an entire literature concerning the \"structure of matter\", ranging from the \"electrical structure\" in the early 20th century, to the more recent \"quark structure of matter\", introduced today with the remark: Understanding the quark structure of matter has been one of the most important advances in contemporary physics. In this connection, physicists speak of matter fields, and speak of particles as \"quantum excitations of a mode of the matter field\". And here is a quote from de Sabbata and Gasperini: \"With the word \"matter\" we denote, in this context, the sources of the interactions, that is spinor fields (like quarks and leptons), which are believed to be the fundamental components of matter, or scalar fields, like the Higgs particles, which are used to introduced mass in a gauge theory (and that, however, could be composed of more fundamental fermion fields).\" The modern conception of matter has been refined many times in history, in light of the improvement in knowledge of just what the basic building blocks are, and in how they interact. In the late 19th century with the discovery of the electron, and in the early 20th century, with the discovery of the atomic nucleus, and the birth of particle physics, matter was seen as made up of electrons, protons and neutrons interacting to form atoms. Today, we know that even protons and neutrons are not indivisible, they can be divided into quarks, while electrons are part of a particle family called leptons. Both quarks and leptons are elementary particles, and are currently seen as being the fundamental constituents of matter. These quarks and leptons interact through four fundamental forces: gravity, electromagnetism, weak interactions, and strong interactions. The Standard Model of particle physics is currently the best explanation for all of physics, but despite decades of efforts, gravity cannot yet be accounted for at the quantum level; it is only described by classical physics (see quantum gravity and graviton). Interactions between quarks and leptons are the result of an exchange of force-carrying particles (such as photons) between quarks and leptons. The force-carrying particles are not themselves building blocks. As one consequence, mass and energy (which cannot be created or destroyed) cannot always be related to matter (which can be created out of non-matter particles such as photons, or even out of pure energy, such as kinetic energy). Force carriers are usually not considered matter: the carriers of the electric force (photons) possess energy (see Planck relation) and the carriers of the weak force (W and Z bosons) are massive, but neither are considered matter either. However, while these particles are not considered matter, they do contribute to the total mass of atoms, subatomic particles, and all systems that contain them.    Summary  The term \"matter\" is used throughout physics in a bewildering variety of contexts: for example, one refers to \"condensed matter physics\", \"elementary matter\", \"partonic\" matter, \"dark\" matter, \"anti\"-matter, \"strange\" matter, and \"nuclear\" matter. In discussions of matter and antimatter, normal matter has been referred to by Alfvén as koinomatter (Gk. common matter). It is fair to say that in physics, there is no broad consensus as to a general definition of matter, and the term \"matter\" usually is used in conjunction with a specifying modifier.    See also     References     Further reading  Lillian Hoddeson; Michael Riordan, eds. (1997). The Rise of the Standard Model. Cambridge University Press. ISBN 0-521-57816-7.  Timothy Paul Smith (2004). \"The search for quarks in ordinary matter\". Hidden Worlds. Princeton University Press. p. 1. ISBN 0-691-05773-7.  Harald Fritzsch (2005). Elementary Particles: Building blocks of matter. World Scientific. p. 1. ISBN 981-256-141-2.  Bertrand Russell (1992). \"The philosophy of matter\". A Critical Exposition of the Philosophy of Leibniz (Reprint of 1937 2nd ed.). Routledge. p. 88. ISBN 0-415-08296-X.  Stephen Toulmin and June Goodfield, The Architecture of Matter (Chicago: University of Chicago Press, 1962). Richard J. Connell, Matter and Becoming (Chicago: The Priory Press, 1966). Ernan McMullin, The Concept of Matter in Greek and Medieval Philosophy (Notre Dame, Indiana: Univ. of Notre Dame Press, 1965). Ernan McMullin, The Concept of Matter in Modern Philosophy (Notre Dame, Indiana: University of Notre Dame Press, 1978).    External links  Visionlearning Module on Matter Matter in the universe How much Matter is in the Universe? NASA on superfluid core of neutron star Matter and Energy: A False Dichotomy – Conversations About Science with Theoretical Physicist Matt Strassler","label":"foo"},{"text":"","label":"foo"},{"text":"In physics, motion is a change in position of an object with respect to time. Motion is typically described in terms of displacement, distance (scalar), velocity, acceleration, time and speed. Motion of a body is observed by attaching a frame of reference to an observer and measuring the change in position of the body relative to that frame. If the position of a body is not changing with the time with respect to a given frame of reference the body is said to be at rest, motionless, immobile, stationary, or to have constant (time-invariant) position. An object's motion cannot change unless it is acted upon by a force, as described by Newton's first law. Momentum is a quantity which is used for measuring motion of an object. An object's momentum is directly related to the object's mass and velocity, and the total momentum of all objects in an isolated system (one not affected by external forces) does not change with time, as described by the law of conservation of momentum. As there is no absolute frame of reference, absolute motion cannot be determined. Thus, everything in the universe can be considered to be moving. More generally, motion is a concept that applies to objects, bodies, and matter particles, to radiation, radiation fields and radiation particles, and to space, its curvature and space-time. One can also speak of motion of shapes and boundaries. So, the term motion in general signifies a continuous change in the configuration of a physical system. For example, one can talk about motion of a wave or about motion of a quantum particle, where the configuration consists of probabilities of occupying specific positions.    Laws of motionEdit   In physics, motion is described through two sets of apparently contradictory laws of mechanics. Motions of all large scale and familiar objects in the universe (such as projectiles, planets, cells, and humans) are described by classical mechanics. Whereas the motion of very small atomic and sub-atomic objects is described by quantum mechanics.    Classical mechanicsEdit  Classical mechanics is used for describing the motion of macroscopic objects, from projectiles to parts of machinery, as well as astronomical objects, such as spacecraft, planets, stars, and galaxies. It produces very accurate results within these domains, and is one of the oldest and largest subjects in science, engineering, and technology. Classical mechanics is fundamentally based on Newton's laws of motion. These laws describe the relationship between the forces acting on a body and the motion of that body. They were first compiled by Sir Isaac Newton in his work Philosophiæ Naturalis Principia Mathematica, first published on July 5, 1687. His three laws are: A body either is at rest or moves with constant velocity, until and unless an outer force is applied to it. An object will travel in one direction only until an outer force changes its direction. Whenever one body exerts a force F onto a second body,(in some cases, which is standing still) the second body exerts the force −F on the first body. F and −F are equal in magnitude and opposite in sense. So, the body which exerts F will go backwards. Newton's three laws of motion, along with his Newton's law of motion, which were the first to accurately provide a mathematical model for understanding orbiting bodies in outer space. This explanation unified the motion of celestial bodies and motion of objects on earth. Classical mechanics was later further enhanced by Albert Einstein's special relativity and general relativity. Motion of objects with a high velocity, approaching the speed of light; general relativity is employed to handle gravitational motion at a deeper level.    Quantum mechanicsEdit   Quantum mechanics is a set of principles describing physical reality at the atomic level of matter (molecules and atoms) and the subatomic (electrons, protons, and even smaller particles). These descriptions include the simultaneous wave-like and particle-like behavior of both matter and radiation energy, this is described in the wave–particle duality. In classical mechanics, accurate measurements and predictions of the state of objects can be calculated, such as location and velocity. In the quantum mechanics, due to the Heisenberg uncertainty principle), the complete state of a subatomic particle, such as its location and velocity, cannot be simultaneously determined. In addition to describing the motion of atomic level phenomena, quantum mechanics is useful in understanding some large scale phenomenon such as superfluidity, superconductivity, and biological systems, including the function of smell receptors and the structures of proteins.    List of \"imperceptible\" human motionsEdit  Humans, like all known things in the universe, are in constant motion, however, aside from obvious movements of the various external body parts and locomotion, humans are in motion in a variety of ways which are more difficult to perceive. Many of these \"imperceptible motions\" are only perceivable with the help of special tools and careful observation. The larger scales of \"imperceptible motions\" are difficult for humans to perceive for two reasons: 1) Newton's laws of motion (particularly Inertia) which prevent humans from feeling motions of a mass to which they are connected, and 2) the lack of an obvious frame of reference which would allow individuals to easily see that they are moving. The smaller scales of these motions are too small for humans to sense.    UniverseEdit  Spacetime (the fabric of the universe) is actually expanding. Essentially, everything in the universe is stretching like a rubber band. This motion is the most obscure as it is not physical motion as such, but rather a change in the very nature of the universe. The primary source of verification of this expansion was provided by Edwin Hubble who demonstrated that all galaxies and distant astronomical objects were moving away from us (\"Hubble's law\") as predicted by a universal expansion.    GalaxyEdit  The Milky Way Galaxy, is moving through space. Many astronomers believe the Milky Way is moving at approximately 600 km/s relative to the observed locations of other nearby galaxies. Another reference frame is provided by the Cosmic microwave background. This frame of reference indicates that The Milky Way is moving at around 552 km/s.    Sun and solar systemEdit  The Milky Way is rotating around its dense galactic center, thus the sun is moving in a circle within the galaxy's gravity. Away from the central bulge or outer rim, the typical stellar velocity is between 210 and 240 km/s. All planets and their moons move with the sun. Thus the solar system is moving.    EarthEdit  The Earth is rotating or spinning around its axis, this is evidenced by day and night, at the equator the earth has an eastward velocity of 0.4651 km/s (1040 mi/h). The Earth is orbiting around the Sun in an orbital revolution. A complete orbit around the sun takes one year or about 365 days; it averages a speed of about 30 km/s (67,000 mi/h).    ContinentsEdit  The Theory of Plate tectonics tells us that the continents are drifting on convection currents within the mantle causing them to move across the surface of the planet at the slow speed of approximately 1 inch (2.54 cm) per year. However, the velocities of plates range widely. The fastest-moving plates are the oceanic plates, with the Cocos Plate advancing at a rate of 75 mm/yr (3.0 in/yr) and the Pacific Plate moving 52–69 mm/yr (2.1–2.7 in/yr). At the other extreme, the slowest-moving plate is the Eurasian Plate, progressing at a typical rate of about 21 mm/yr (0.8 in/yr).    Internal bodyEdit  The human heart is constantly contracting to move blood throughout the body. Through larger veins and arteries in the body blood has been found to travel at approximately 0.33 m/s. Though considerable variation exists, and peak flows in the venae cavae have been found between 0.1 m/s and 0.45 m/s. The smooth muscles of hollow internal organs are moving. The most familiar would be peristalsis which is where digested food is forced throughout the digestive tract. Though different foods travel through the body at rates, an average speed through the human small intestine is 2.16 m/h (0.036 m/s). Typically some sound is audible at any given moment, when the vibration of these sound waves reaches the ear drum it moves in response and allows the sense of hearing. The human lymphatic system is constantly moving excess fluids, lipids, and immune system related products around the body. The lymph fluid has been found to move through a lymph capillary of the skin at approximately 0.0000097 m/s.    CellsEdit  The cells of the human body have many structures which move throughout them. Cytoplasmic streaming is a way which cells move molecular substances throughout the cytoplasm. Various motor proteins work as molecular motors within a cell and move along the surface of various cellular substrates such as microtubuless. Motor proteins are typically powered by the hydrolysis of adenosine triphosphate (ATP), and convert chemical energy into mechanical work. Vesicles propelled by motor proteins have been found to have a velocity of approximately 0.00000152 m/s.    ParticlesEdit  According to the laws of thermodynamics all particles of matter are in constant random motion as long as the temperature is above absolute zero. Thus the molecules and atoms which make up the human body are vibrating, colliding, and moving. This motion can be detected as temperature; higher temperatures, which represent greater kinetic energy in the particles, feel warm to humans whom sense the thermal energy transferring from the object being touched to their nerves. Similarly, when lower temperature objects are touched, the senses perceive the transfer of heat away from the body as feeling cold.    Subatomic particlesEdit  Within each atom, electrons exist in an area around the nucleus. This area is called the electron cloud. According to Bohr's model of the atom, electrons have a high velocity, and the larger the nucleus they are orbiting the faster they would need to move. If electrons 'move' about the electron cloud in strict paths the same way planets orbit the sun, then electrons would be required to do so at speeds which far exceed the speed of light. However, there is no reason that one must confine one's self to this strict conceptualization, that electrons move in paths the same way macroscopic objects do. Rather one can conceptualize electrons to be 'particles' that capriciously exist within the bounds of the electron cloud. Inside the atomic nucleus the protons and neutrons are also probably moving around due the electrical repulsion of the protons and the presence of angular momentum of both particles.    LightEdit   Light propagates at 299,792,458 m/s, often approximated as 300,000 kilometres per second or 186,000 miles per second. The speed of light (or c) is also the speed of all massless particles and associated fields in a vacuum, and it is the upper limit on the speed at which energy, matter, and information can travel. The speed of light is the limit speed for physical systems. In addition, the speed of light is an invariant quantity: it has the same value, irrespective of the position or speed of the observer. This property makes the speed of light c the natural measurement unit for speed.    Types of motionEdit  Simple harmonic motion – (e.g., that of a pendulum). Anharmonic motion Periodic motion Rectilinear motion (Linear motion) – motion which follows a straight linear path, and whose displacement is exactly the same as its trajectory. Reciprocal motion (e.g. vibration) Random motion (e.g. vibration) Brownian motion (i.e. the random movement of particles) Circular motion (e.g. the orbits of planets) Rotary motion – a motion about a fixed point. (e.g. Ferris wheel). Curvilinear motion – It is defined as the motion along a curved path that may be planar or in three dimensions. Rotational motion Rolling motion - (as of the wheel of a bicycle) Oscillation Combination (or simultaneous) motions - Combination of two or more above listed motions Projectile motion - uniform horizontal motion + vertical accelerated motion Half projectile motion    See alsoEdit   Simple machines Kinematic chain Power Machine Motion (geometry) Displacement Translatory motion    ReferencesEdit","label":"foo"},{"text":"In physics, spacetime (also space–time, space time or space–time continuum) is any mathematical model that combines space and time into a single interwoven continuum. The spacetime of our universe is usually interpreted from a Euclidean space perspective, which regards space as consisting of three dimensions, and time as consisting of one dimension, the \"fourth dimension\". By combining space and time into a single manifold called Minkowski space, physicists have significantly simplified a large number of physical theories, as well as described in a more uniform way the workings of the universe at both the supergalactic and subatomic levels.    ExplanationEdit  In non-relativistic classical mechanics, the use of Euclidean space instead of spacetime is appropriate, because time is treated as universal with a constant rate of passage that is independent of the state of motion of an observer. In relativistic contexts, time cannot be separated from the three dimensions of space, because the observed rate at which time passes for an object depends on the object's velocity relative to the observer and also on the strength of gravitational fields, which can slow the passage of time for an object as seen by an observer outside the field. In cosmology, the concept of spacetime combines space and time to a single abstract universe. Mathematically it is a manifold consisting of \"events\" which are described by some type of coordinate system. Typically three spatial dimensions (length, width, height), and one temporal dimension (time) are required. Dimensions are independent components of a coordinate grid needed to locate a point in a certain defined \"space\". For example, on the globe the latitude and longitude are two independent coordinates which together uniquely determine a location. In spacetime, a coordinate grid that spans the 3+1 dimensions locates events (rather than just points in space), i.e., time is added as another dimension to the coordinate grid. This way the coordinates specify where and when events occur. However, the unified nature of spacetime and the freedom of coordinate choice it allows imply that to express the temporal coordinate in one coordinate system requires both temporal and spatial coordinates in another coordinate system. Unlike in normal spatial coordinates, there are still restrictions for how measurements can be made spatially and temporally (see Spacetime intervals). These restrictions correspond roughly to a particular mathematical model which differs from Euclidean space in its manifest symmetry. Until the beginning of the 20th century, time was believed to be independent of motion, progressing at a fixed rate in all reference frames; however, later experiments revealed that time slows at higher speeds of the reference frame relative to another reference frame. Such slowing, called time dilation, is explained in special relativity theory. Many experiments have confirmed time dilation, such as the relativistic decay of muons from cosmic ray showers and the slowing of atomic clocks aboard a Space Shuttle relative to synchronized Earth-bound inertial clocks. The duration of time can therefore vary according to events and reference frames. When dimensions are understood as mere components of the grid system, rather than physical attributes of space, it is easier to understand the alternate dimensional views as being simply the result of coordinate transformations. The term spacetime has taken on a generalized meaning beyond treating spacetime events with the normal 3+1 dimensions. It is really the combination of space and time. Other proposed spacetime theories include additional dimensions—normally spatial but there exist some speculative theories that include additional temporal dimensions and even some that include dimensions that are neither temporal nor spatial (e.g., superspace). How many dimensions are needed to describe the universe is still an open question. Speculative theories such as string theory predict 10 or 26 dimensions (with M-theory predicting 11 dimensions: 10 spatial and 1 temporal), but the existence of more than four dimensions would only appear to make a difference at the subatomic level.    Spacetime in literatureEdit  Incas regarded space and time as a single concept, referred to as pacha (Quechua: pacha, Aymara: pacha). The peoples of the Andes maintain a similar understanding. Arthur Schopenhauer wrote in §18 of On the Fourfold Root of the Principle of Sufficient Reason (1813): \"the representation of coexistence is impossible in Time alone; it depends, for its completion, upon the representation of Space; because, in mere Time, all things follow one another, and in mere Space all things are side by side; it is accordingly only by the combination of Time and Space that the representation of coexistence arises\". The idea of a unified spacetime is stated by Edgar Allan Poe in his essay on cosmology titled Eureka (1848) that \"Space and duration are one\". In 1895, in his novel The Time Machine, H. G. Wells wrote, \"There is no difference between time and any of the three dimensions of space except that our consciousness moves along it\", and that \"any real body must have extension in four directions: it must have Length, Breadth, Thickness, and Duration\". Marcel Proust, in his novel Swann's Way (published 1913), describes the village church of his childhood's Combray as \"a building which occupied, so to speak, four dimensions of space—the name of the fourth being Time\".    Mathematical conceptEdit  In Encyclopedie under the term dimension Jean le Rond d'Alembert speculated that duration (time) might be considered a fourth dimension if the idea was not too novel. Another early venture was by Joseph Louis Lagrange in his Theory of Analytic Functions (1797, 1813). He said, \"One may view mechanics as a geometry of four dimensions, and mechanical analysis as an extension of geometric analysis\". The ancient idea of the cosmos gradually was described mathematically with differential equations, differential geometry, and abstract algebra. These mathematical articulations blossomed in the nineteenth century as electrical technology stimulated men like Michael Faraday and James Clerk Maxwell to describe the reciprocal relations of electric and magnetic fields. Daniel Siegel phrased Maxwell's role in relativity as follows:  [...] the idea of the propagation of forces at the velocity of light through the electromagnetic field as described by Maxwell's equations—rather than instantaneously at a distance—formed the necessary basis for relativity theory.  Maxwell used vortex models in his papers on On Physical Lines of Force, but ultimately gave up on any substance but the electromagnetic field. Pierre Duhem wrote:  [Maxwell] was not able to create the theory that he envisaged except by giving up the use of any model, and by extending by means of analogy the abstract system of electrodynamics to displacement currents.  In Siegel's estimation, \"this very abstract view of the electromagnetic fields, involving no visualizable picture of what is going on out there in the field, is Maxwell's legacy.\" Describing the behaviour of electric fields and magnetic fields led Maxwell to view the combination as an electromagnetic field. These fields have a value at every point of spacetime. It is the intermingling of electric and magnetic manifestations, described by Maxwell's equations, that give spacetime its structure. In particular, the rate of motion of an observer determines the electric and magnetic profiles of the electromagnetic field. The propagation of the field is determined by the electromagnetic wave equation, which requires spacetime for description. Spacetime was described as an affine space with quadratic form in Minkowski space of 1908. In his 1914 textbook The Theory of Relativity, Ludwik Silberstein used biquaternions to represent events in Minkowski space. He also exhibited the Lorentz transformations between observers of differing velocities as biquaternion mappings. Biquaternions were described in 1853 by W. R. Hamilton, so while the physical interpretation was new, the mathematics was well known in English literature, making relativity an instance of applied mathematics. The first inkling of general relativity in spacetime was articulated by W. K. Clifford. Description of the effect of gravitation on space and time was found to be most easily visualized as a \"warp\" or stretching in the geometrical fabric of space and time, in a smooth and continuous way that changed smoothly from point-to-point along the spacetime fabric. In 1947 James Jeans provided a concise summary of the development of spacetime theory in his book The Growth of Physical Science.    Basic conceptsEdit  The basic elements of spacetime are events. In any given spacetime, an event is a unique position at a unique time. Because events are spacetime points, an example of an event in classical relativistic physics is , the location of an elementary (point-like) particle at a particular time. A spacetime itself can be viewed as the union of all events in the same way that a line is the union of all of its points, formally organized into a manifold, a space which can be described at small scales using coordinate systems. A spacetime is independent of any observer. However, in describing physical phenomena (which occur at certain moments of time in a given region of space), each observer chooses a convenient metrical coordinate system. Events are specified by four real numbers in any such coordinate system. The trajectories of elementary (point-like) particles through space and time are thus a continuum of events called the world line of the particle. Extended or composite objects (consisting of many elementary particles) are thus a union of many world lines twisted together by virtue of their interactions through spacetime into a \"world-braid\". However, in physics, it is common to treat an extended object as a \"particle\" or \"field\" with its own unique (e.g., center of mass) position at any given time, so that the world line of a particle or light beam is the path that this particle or beam takes in the spacetime and represents the history of the particle or beam. The world line of the orbit of the Earth (in such a description) is depicted in two spatial dimensions x and y (the plane of the Earth's orbit) and a time dimension orthogonal to x and y. The orbit of the Earth is an ellipse in space alone, but its world line is a helix in spacetime. The unification of space and time is exemplified by the common practice of selecting a metric (the measure that specifies the interval between two events in spacetime) such that all four dimensions are measured in terms of units of distance: representing an event as  (in the Lorentz metric) or  (in the original Minkowski metric) where  is the speed of light. The metrical descriptions of Minkowski Space and spacelike, lightlike, and timelike intervals given below follow this convention, as do the conventional formulations of the Lorentz transformation.    Spacetime intervals in flat spaceEdit  In a Euclidean space, the separation between two points is measured by the distance between the two points. The distance is purely spatial, and is always positive. In spacetime, the displacement four-vector ΔR is given by the space displacement vector Δr and the time difference Δt between the events. The spacetime interval, also called invariant interval, between the two events, s2, is defined as:    (spacetime interval), where c is the speed of light. The choice of signs for  above follows the space-like convention (−+++). Spacetime intervals may be classified into three distinct types, based on whether the temporal separation () or the spatial separation () of the two events is greater: time-like, light-like or space-like. Certain types of world lines are called geodesics of the spacetime – straight lines in the case of Minkowski space and their closest equivalent in the curved spacetime of general relativity. In the case of purely time-like paths, geodesics are (locally) the paths of greatest separation (spacetime interval) as measured along the path between two events, whereas in Euclidean space and Riemannian manifolds, geodesics are paths of shortest distance between two points. The concept of geodesics becomes central in general relativity, since geodesic motion may be thought of as \"pure motion\" (inertial motion) in spacetime, that is, free from any external influences.   = Time-like intervalEdit =  For two events separated by a time-like interval, enough time passes between them that there could be a cause–effect relationship between the two events. For a particle traveling through space at less than the speed of light, any two events which occur to or by the particle must be separated by a time-like interval. Event pairs with time-like separation define a negative spacetime interval () and may be said to occur in each other's future or past. There exists a reference frame such that the two events are observed to occur in the same spatial location, but there is no reference frame in which the two events can occur at the same time. The measure of a time-like spacetime interval is described by the proper time interval, :    (proper time interval). The proper time interval would be measured by an observer with a clock traveling between the two events in an inertial reference frame, when the observer's path intersects each event as that event occurs. (The proper time interval defines a real number, since the interior of the square root is positive.)   = Light-like intervalEdit =  In a light-like interval, the spatial distance between two events is exactly balanced by the time between the two events. The events define a spacetime interval of zero (). Light-like intervals are also known as \"null\" intervals. Events which occur to or are initiated by a photon along its path (i.e., while traveling at , the speed of light) all have light-like separation. Given one event, all those events which follow at light-like intervals define the propagation of a light cone, and all the events which preceded from a light-like interval define a second (graphically inverted, which is to say \"pastward\") light cone.   = Space-like intervalEdit =  When a space-like interval separates two events, not enough time passes between their occurrences for there to exist a causal relationship crossing the spatial distance between the two events at the speed of light or slower. Generally, the events are considered not to occur in each other's future or past. There exists a reference frame such that the two events are observed to occur at the same time, but there is no reference frame in which the two events can occur in the same spatial location. For these space-like event pairs with a positive spacetime interval (), the measurement of space-like separation is the proper distance, :    (proper distance). Like the proper time of time-like intervals, the proper distance of space-like spacetime intervals is a real number value.    Interval as areaEdit  The interval has been presented as the area of an oriented rectangle formed by two events and isotropic lines through them. Time-like or space-like separations correspond to oppositely oriented rectangles, one type considered to have rectangles of negative area. The case of two events separated by light corresponds to the rectangle degenerating to the segment between the events and zero area. The transformations leaving interval-length invariant are the area-preserving squeeze mappings. The parameters traditionally used rely on quadrature of the hyperbola, which is the natural logarithm. This transcendental function is essential in mathematical analysis as its inverse unites circular functions and hyperbolic functions: The exponential function, et,  t a real number, used in the hyperbola (et, e–t ), generates hyperbolic sectors and the hyperbolic angle parameter. The functions cosh and sinh, used with rapidity as hyperbolic angle, provide the common representation of squeeze in the form  or as the split-complex unit     Mathematics of spacetimesEdit  For physical reasons, a spacetime continuum is mathematically defined as a four-dimensional, smooth, connected Lorentzian manifold . This means the smooth Lorentz metric  has signature . The metric determines the geometry of spacetime, as well as determining the geodesics of particles and light beams. About each point (event) on this manifold, coordinate charts are used to represent observers in reference frames. Usually, Cartesian coordinates  are used. Moreover, for simplicity's sake, units of measurement are usually chosen such that the speed of light  is equal to 1. A reference frame (observer) can be identified with one of these coordinate charts; any such observer can describe any event . Another reference frame may be identified by a second coordinate chart about . Two observers (one in each reference frame) may describe the same event  but obtain different descriptions. Usually, many overlapping coordinate charts are needed to cover a manifold. Given two coordinate charts, one containing  (representing an observer) and another containing  (representing another observer), the intersection of the charts represents the region of spacetime in which both observers can measure physical quantities and hence compare results. The relation between the two sets of measurements is given by a non-singular coordinate transformation on this intersection. The idea of coordinate charts as local observers who can perform measurements in their vicinity also makes good physical sense, as this is how one actually collects physical data—locally. For example, two observers, one of whom is on Earth, but the other one who is on a fast rocket to Jupiter, may observe a comet crashing into Jupiter (this is the event ). In general, they will disagree about the exact location and timing of this impact, i.e., they will have different 4-tuples  (as they are using different coordinate systems). Although their kinematic descriptions will differ, dynamical (physical) laws, such as momentum conservation and the first law of thermodynamics, will still hold. In fact, relativity theory requires more than this in the sense that it stipulates these (and all other physical) laws must take the same form in all coordinate systems. This introduces tensors into relativity, by which all physical quantities are represented. Geodesics are said to be time-like, null, or space-like if the tangent vector to one point of the geodesic is of this nature. Paths of particles and light beams in spacetime are represented by time-like and null (light-like) geodesics, respectively.    TopologyEdit   The assumptions contained in the definition of a spacetime are usually justified by the following considerations. The connectedness assumption serves two main purposes. First, different observers making measurements (represented by coordinate charts) should be able to compare their observations on the non-empty intersection of the charts. If the connectedness assumption were dropped, this would not be possible. Second, for a manifold, the properties of connectedness and path-connectedness are equivalent, and one requires the existence of paths (in particular, geodesics) in the spacetime to represent the motion of particles and radiation. Every spacetime is paracompact. This property, allied with the smoothness of the spacetime, gives rise to a smooth linear connection, an important structure in general relativity. Some important theorems on constructing spacetimes from compact and non-compact manifolds include the following: A compact manifold can be turned into a spacetime if, and only if, its Euler characteristic is 0. (Proof idea: the existence of a Lorentzian metric is shown to be equivalent to the existence of a nonvanishing vector field.) Any non-compact 4-manifold can be turned into a spacetime.    Spacetime symmetriesEdit   Often in relativity, spacetimes that have some form of symmetry are studied. As well as helping to classify spacetimes, these symmetries usually serve as a simplifying assumption in specialized work. Some of the most popular ones include: Axisymmetric spacetimes Spherically symmetric spacetimes Static spacetimes Stationary spacetimes    Causal structureEdit   The causal structure of a spacetime describes causal relationships between pairs of points in the spacetime based on the existence of certain types of curves joining the points.    Spacetime in special relativityEdit   The geometry of spacetime in special relativity is described by the Minkowski metric on R4. This spacetime is called Minkowski space. The Minkowski metric is usually denoted by  and can be written as a four-by-four matrix:  where the Landau–Lifshitz space-like convention is being used. A basic assumption of relativity is that coordinate transformations must leave spacetime intervals invariant. Intervals are invariant under Lorentz transformations. This invariance property leads to the use of four-vectors (and other tensors) in describing physics. Strictly speaking, one can also consider events in Newtonian physics as a single spacetime. This is Galilean–Newtonian relativity, and the coordinate systems are related by Galilean transformations. However, since these preserve spatial and temporal distances independently, such a spacetime can be decomposed into spatial coordinates plus temporal coordinates, which is not possible in the general case.    Spacetime in general relativityEdit  In general relativity, it is assumed that spacetime is curved by the presence of matter (energy), this curvature being represented by the Riemann tensor. In special relativity, the Riemann tensor is identically zero, and so this concept of \"non-curvedness\" is sometimes expressed by the statement Minkowski spacetime is flat. The earlier discussed notions of time-like, light-like and space-like intervals in special relativity can similarly be used to classify one-dimensional curves through curved spacetime. A time-like curve can be understood as one where the interval between any two infinitesimally close events on the curve is time-like, and likewise for light-like and space-like curves. Technically the three types of curves are usually defined in terms of whether the tangent vector at each point on the curve is time-like, light-like or space-like. The world line of a slower-than-light object will always be a time-like curve, the world line of a massless particle such as a photon will be a light-like curve, and a space-like curve could be the world line of a hypothetical tachyon. In the local neighborhood of any event, time-like curves that pass through the event will remain inside that event's past and future light cones, light-like curves that pass through the event will be on the surface of the light cones, and space-like curves that pass through the event will be outside the light cones. One can also define the notion of a three-dimensional \"spacelike hypersurface\", a continuous three-dimensional \"slice\" through the four-dimensional property with the property that every curve that is contained entirely within this hypersurface is a space-like curve. Many spacetime continua have physical interpretations which most physicists would consider bizarre or unsettling. For example, a compact spacetime has closed timelike curves, which violate our usual ideas of causality (that is, future events could affect past ones). For this reason, mathematical physicists usually consider only restricted subsets of all the possible spacetimes. One way to do this is to study \"realistic\" solutions of the equations of general relativity. Another way is to add some additional \"physically reasonable\" but still fairly general geometric restrictions and try to prove interesting things about the resulting spacetimes. The latter approach has led to some important results, most notably the Penrose–Hawking singularity theorems.    Quantized spacetimeEdit   In general relativity, spacetime is assumed to be smooth and continuous—and not just in the mathematical sense. In the theory of quantum mechanics, there is an inherent discreteness present in physics. In attempting to reconcile these two theories, it is sometimes postulated that spacetime should be quantized at the very smallest scales. Current theory is focused on the nature of spacetime at the Planck scale. Causal sets, loop quantum gravity, string theory, causal dynamical triangulation, and black hole thermodynamics all predict a quantized spacetime with agreement on the order of magnitude. Loop quantum gravity makes precise predictions about the geometry of spacetime at the Planck scale.    See alsoEdit     ReferencesEdit     Further ReadingEdit  Albert Einstein on Space-Time 13th edition Encyclopedia Britannica Albert Einstein's 1926 article Ehrenfest, Paul (1920) \"How do the fundamental laws of physics make manifest that Space has 3 dimensions?\" Annalen der Physik 366: 440. George F. Ellis and Ruth M. Williams (1992) Flat and curved space–times. Oxford Univ. Press. ISBN 0-19-851164-7 Space-time and gravitation Scholarpedia Expert articles    External linksEdit  http://universaltheory.org Barrow, John D.; Tipler, Frank J. (1988). The Anthropic Cosmological Principle. Oxford University Press. ISBN 978-0-19-282147-8. LCCN 87028148.  Isenberg, J. A. (1981). \"Wheeler–Einstein–Mach spacetimes\". Phys. Rev. D 24 (2): 251–256. Bibcode:1981PhRvD..24..251I. doi:10.1103/PhysRevD.24.251.  Kant, Immanuel (1929) \"Thoughts on the true estimation of living forces\" in J. Handyside, trans., Kant's Inaugural Dissertation and Early Writings on Space. Univ. of Chicago Press. Lorentz, H. A., Einstein, Albert, Minkowski, Hermann, and Weyl, Hermann (1952) The Principle of Relativity: A Collection of Original Memoirs. Dover. Lucas, John Randolph (1973) A Treatise on Time and Space. London: Methuen. Penrose, Roger (2004). The Road to Reality. Oxford: Oxford University Press. ISBN 0-679-45443-8.  Chpts. 17–18. Poe, Edgar A. (1848). Eureka; An Essay on the Material and Spiritual Universe. Hesperus Press Limited. ISBN 1-84391-009-8.  Robb, A. A. (1936). Geometry of Time and Space. University Press.  Erwin Schrödinger (1950) Space–time structure. Cambridge Univ. Press. Schutz, J. W. (1997). Independent axioms for Minkowski Space–time. Addison-Wesley Longman. ISBN 0-582-31760-6.  Tangherlini, F. R. (1963). \"Schwarzschild Field in n Dimensions and the Dimensionality of Space Problem\". Nuovo Cimento 14 (27): 636.  Taylor, E. F.; Wheeler, John A. (1963). Spacetime Physics. W. H. Freeman. ISBN 0-7167-2327-1.  Wells, H.G. (2004). The Time Machine. New York: Pocket Books. ISBN 0-671-57554-6.  (pp. 5–6) Stanford Encyclopedia of Philosophy: \"Space and Time: Inertial Frames\" by Robert DiSalle.","label":"foo"},{"text":"In physics, energy is a property of objects which can be transferred to other objects or converted into different forms, but cannot be created or destroyed. The \"ability of a system to perform work\" is a common description, but it is difficult to give one single comprehensive definition of energy because of its many forms. For instance, in SI units, energy is measured in joules, and one joule is defined \"mechanically\", being the energy transferred to an object by the mechanical work of moving it a distance of 1 metre against a force of 1 newton. However, there are many other definitions of energy, depending on the context, such as thermal energy, radiant energy, electromagnetic, nuclear, etc., where definitions are derived that are the most convenient. Common energy forms include the kinetic energy of a moving object, the radiant energy carried by light, the potential energy stored by an object's position in a force field (gravitational, electric or magnetic), elastic energy stored by stretching solid objects, chemical energy released when a fuel burns, and the thermal energy due to an object's temperature. All of the many forms of energy are convertible to other kinds of energy, and obey the law of conservation of energy which says that energy can be neither created nor be destroyed; however, it can change from one form to another. For \"closed systems\" with no external source or sink of energy, the first law of thermodynamics states that a system's energy is constant unless energy is transferred in or out by mechanical work or heat, and that no energy is lost in transfer. This means that it is impossible to create or destroy energy. The second law of thermodynamics states that all systems doing work always lose some energy as waste heat. This creates a limit to the amount of energy that can do work by a heating process, a limit called the available energy. Mechanical and other forms of energy can be transformed in the other direction into thermal energy without such limitations. The total energy of a system can be calculated by adding up all forms of energy in the system. Examples of energy transformation include generating electric energy from heat energy via a steam turbine, or lifting an object against gravity using electrical energy driving a crane motor. Lifting against gravity performs mechanical work on the object and stores gravitational potential energy In the object. If the object falls to ground, gravity does mechanical work on the object which transforms the potential energy in the gravitational field to the kinetic energy released as heat on impact with the ground. Our Sun transforms nuclear potential energy to other forms of energy; its total mass does not decrease due to that in itself (since it still contains the same total energy even if in different forms), but its mass does decrease when the energy escapes out to its surroundings, largely as radiant energy. Mass and energy are closely related. According to the theory of mass–energy equivalence, any object that has mass when stationary in a frame of reference (called rest mass) also has an equivalent amount of energy whose form is called rest energy in that frame, and any additional energy acquired by the object above that rest energy will increase an object's mass. For example, if you had a sensitive enough scale, you could measure an increase in mass after heating an object. Living organisms require available energy to stay alive, such as the energy humans get from food. Civilisation gets the energy it needs from energy resources such as fossil fuels. The processes of Earth's climate and ecosystem are driven by the radiant energy Earth receives from the sun and the geothermal energy contained within the earth.    Forms   The total energy of a system can be subdivided and classified in various ways. For example, classical mechanics distinguishes between kinetic energy, which is determined by an object's movement through space, and potential energy, which is a function of the position of an object within a field. It may also be convenient to distinguish gravitational energy, thermal energy, several types of nuclear energy (which utilize potentials from the nuclear force and the weak force), electric energy (from the electric field), and magnetic energy (from the magnetic field), among others. Many of these classifications overlap; for instance, thermal energy usually consists partly of kinetic and partly of potential energy. Some types of energy are a varying mix of both potential and kinetic energy. An example is mechanical energy which is the sum of (usually macroscopic) kinetic and potential energy in a system. Elastic energy in materials is also dependent upon electrical potential energy (among atoms and molecules), as is chemical energy, which is stored and released from a reservoir of electrical potential energy between electrons, and the molecules or atomic nuclei that attract them..The list is also not necessarily complete. Whenever physical scientists discover that a certain phenomenon appears to violate the law of energy conservation, new forms are typically added that account for the discrepancy. Heat and work are special cases in that they are not properties of systems, but are instead properties of processes that transfer energy. In general we cannot measure how much heat or work are present in an object, but rather only how much energy is transferred among objects in certain ways during the occurrence of a given process. Heat and work are measured as positive or negative depending on which side of the transfer we view them from. Potential energies are often measured as positive or negative depending on whether they are greater or less than the energy of a specified base state or configuration such as two interacting bodies being infinitely far apart. Wave energies (such as radiant or sound energy), kinetic energy, and rest energy are each greater than or equal to zero because they are measured in comparison to a base state of zero energy: \"no wave\", \"no motion\", and \"no inertia\", respectively. The distinctions between different kinds of energy is not always clear-cut. As Richard Feynman points out:  Some examples of different kinds of energy:    History   The word energy derives from the Ancient Greek: ἐνέργεια energeia \"activity, operation\", which possibly appears for the first time in the work of Aristotle in the 4th century BC. In contrast to the modern definition, energeia was a qualitative philosophical concept, broad enough to include ideas such as happiness and pleasure. In the late 17th century, Gottfried Leibniz proposed the idea of the Latin: vis viva, or living force, which defined as the product of the mass of an object and its velocity squared; he believed that total vis viva was conserved. To account for slowing due to friction, Leibniz theorized that thermal energy consisted of the random motion of the constituent parts of matter, a view shared by Isaac Newton, although it would be more than a century until this was generally accepted. The modern analog of this property, kinetic energy, differs from vis viva only by a factor of two. In 1807, Thomas Young was possibly the first to use the term \"energy\" instead of vis viva, in its modern sense. Gustave-Gaspard Coriolis described \"kinetic energy\" in 1829 in its modern sense, and in 1853, William Rankine coined the term \"potential energy\". The law of conservation of energy, was also first postulated in the early 19th century, and applies to any isolated system. It was argued for some years whether heat was a physical substance, dubbed the caloric, or merely a physical quantity, such as momentum. In 1845 James Prescott Joule discovered the link between mechanical work and the generation of heat. These developments led to the theory of conservation of energy, formalized largely by William Thomson (Lord Kelvin) as the field of thermodynamics. Thermodynamics aided the rapid development of explanations of chemical processes by Rudolf Clausius, Josiah Willard Gibbs, and Walther Nernst. It also led to a mathematical formulation of the concept of entropy by Clausius and to the introduction of laws of radiant energy by Jožef Stefan. According to Noether's theorem, the conservation of energy is a consequence of the fact that the laws of physics do not change over time. Thus, since 1918, theorists have understood that the law of conservation of energy is the direct mathematical consequence of the translational symmetry of the quantity conjugate to energy, namely time.    Units of measure   In 1843 James Prescott Joule independently discovered the mechanical equivalent in a series of experiments. The most famous of them used the \"Joule apparatus\": a descending weight, attached to a string, caused rotation of a paddle immersed in water, practically insulated from heat transfer. It showed that the gravitational potential energy lost by the weight in descending was equal to the internal energy gained by the water through friction with the paddle. In the International System of Units (SI), the unit of energy is the joule, named after James Prescott Joule. It is a derived unit. It is equal to the energy expended (or work done) in applying a force of one newton through a distance of one metre. However energy is also expressed in many other units not part of the SI, such as ergs, calories, British Thermal Units, kilowatt-hours and kilocalories, which require a conversion factor when expressed in SI units. The SI unit of energy rate (energy per unit time) is the watt, which is a joule per second. Thus, one joule is one watt-second, and 3600 joules equal one watt-hour. The CGS energy unit is the erg and the imperial and US customary unit is the foot pound. Other energy units such as the electronvolt, food calorie or thermodynamic kcal (based on the temperature change of water in a heating process), and BTU are used in specific areas of science and commerce.    Scientific use     Classical mechanics   In classical mechanics, energy is a conceptually and mathematically useful property, as it is a conserved quantity. Several formulations of mechanics have been developed using energy as a core concept. Work, a form of energy, is force times distance.  This says that the work () is equal to the line integral of the force F along a path C; for details see the mechanical work article. Work and thus energy is frame dependent. For example, consider a ball being hit by a bat. In the center-of-mass reference frame, the bat does no work on the ball. But, in the reference frame of the person swinging the bat, considerable work is done on the ball. The total energy of a system is sometimes called the Hamiltonian, after William Rowan Hamilton. The classical equations of motion can be written in terms of the Hamiltonian, even for highly complex or abstract systems. These classical equations have remarkably direct analogs in nonrelativistic quantum mechanics. Another energy-related concept is called the Lagrangian, after Joseph-Louis Lagrange. This formalism is as fundamental as the Hamiltonian, and both can be used to derive the equations of motion or be derived from them. It was invented in the context of classical mechanics, but is generally useful in modern physics. The Lagrangian is defined as the kinetic energy minus the potential energy. Usually, the Lagrange formalism is mathematically more convenient than the Hamiltonian for non-conservative systems (such as systems with friction). Noether's theorem (1918) states that any differentiable symmetry of the action of a physical system has a corresponding conservation law. Noether's theorem has become a fundamental tool of modern theoretical physics and the calculus of variations. A generalisation of the seminal formulations on constants of motion in Lagrangian and Hamiltonian mechanics (1788 and 1833, respectively), it does not apply to systems that cannot be modeled with a Lagrangian; for example, dissipative systems with continuous symmetries need not have a corresponding conservation law.    Chemistry  In the context of chemistry, energy is an attribute of a substance as a consequence of its atomic, molecular or aggregate structure. Since a chemical transformation is accompanied by a change in one or more of these kinds of structure, it is invariably accompanied by an increase or decrease of energy of the substances involved. Some energy is transferred between the surroundings and the reactants of the reaction in the form of heat or light; thus the products of a reaction may have more or less energy than the reactants. A reaction is said to be exergonic if the final state is lower on the energy scale than the initial state; in the case of endergonic reactions the situation is the reverse. Chemical reactions are invariably not possible unless the reactants surmount an energy barrier known as the activation energy. The speed of a chemical reaction (at given temperature T) is related to the activation energy E, by the Boltzmann's population factor e−E/kT – that is the probability of molecule to have energy greater than or equal to E at the given temperature T. This exponential dependence of a reaction rate on temperature is known as the Arrhenius equation.The activation energy necessary for a chemical reaction can be in the form of thermal energy.    Biology   In biology, energy is an attribute of all biological systems from the biosphere to the smallest living organism. Within an organism it is responsible for growth and development of a biological cell or an organelle of a biological organism. Energy is thus often said to be stored by cells in the structures of molecules of substances such as carbohydrates (including sugars), lipids, and proteins, which release energy when reacted with oxygen in respiration. In human terms, the human equivalent (H-e) (Human energy conversion) indicates, for a given amount of energy expenditure, the relative quantity of energy needed for human metabolism, assuming an average human energy expenditure of 12,500kJ per day and a basal metabolic rate of 80 watts. For example, if our bodies run (on average) at 80 watts, then a light bulb running at 100 watts is running at 1.25 human equivalents (100 ÷ 80) i.e. 1.25 H-e. For a difficult task of only a few seconds' duration, a person can put out thousands of watts, many times the 746 watts in one official horsepower. For tasks lasting a few minutes, a fit human can generate perhaps 1,000 watts. For an activity that must be sustained for an hour, output drops to around 300; for an activity kept up all day, 150 watts is about the maximum. The human equivalent assists understanding of energy flows in physical and biological systems by expressing energy units in human terms: it provides a \"feel\" for the use of a given amount of energy Sunlight is also captured by plants as chemical potential energy in photosynthesis, when carbon dioxide and water (two low-energy compounds) are converted into the high-energy compounds carbohydrates, lipids, and proteins. Plants also release oxygen during photosynthesis, which is utilized by living organisms as an electron acceptor, to release the energy of carbohydrates, lipids, and proteins. Release of the energy stored during photosynthesis as heat or light may be triggered suddenly by a spark, in a forest fire, or it may be made available more slowly for animal or human metabolism, when these molecules are ingested, and catabolism is triggered by enzyme action. Any living organism relies on an external source of energy—radiation from the Sun in the case of green plants; chemical energy in some form in the case of animals—to be able to grow and reproduce. The daily 1500–2000 Calories (6–8 MJ) recommended for a human adult are taken as a combination of oxygen and food molecules, the latter mostly carbohydrates and fats, of which glucose (C6H12O6) and stearin (C57H110O6) are convenient examples. The food molecules are oxidised to carbon dioxide and water in the mitochondria  C6H12O6 + 6O2 → 6CO2 + 6H2O C57H110O6 + 81.5O2 → 57CO2 + 55H2O  and some of the energy is used to convert ADP into ATP  ADP + HPO42− → ATP + H2O  The rest of the chemical energy in the carbohydrate or fat is converted into heat: the ATP is used as a sort of \"energy currency\", and some of the chemical energy it contains when split and reacted with water, is used for other metabolism (at each stage of a metabolic pathway, some chemical energy is converted into heat). Only a tiny fraction of the original chemical energy is used for work: gain in kinetic energy of a sprinter during a 100 m race: 4 kJ gain in gravitational potential energy of a 150 kg weight lifted through 2 metres: 3kJ Daily food intake of a normal adult: 6–8 MJ It would appear that living organisms are remarkably inefficient (in the physical sense) in their use of the energy they receive (chemical energy or radiation), and it is true that most real machines manage higher efficiencies. In growing organisms the energy that is converted to heat serves a vital purpose, as it allows the organism tissue to be highly ordered with regard to the molecules it is built from. The second law of thermodynamics states that energy (and matter) tends to become more evenly spread out across the universe: to concentrate energy (or matter) in one specific place, it is necessary to spread out a greater amount of energy (as heat) across the remainder of the universe (\"the surroundings\"). Simpler organisms can achieve higher energy efficiencies than more complex ones, but the complex organisms can occupy ecological niches that are not available to their simpler brethren. The conversion of a portion of the chemical energy to heat at each step in a metabolic pathway is the physical reason behind the pyramid of biomass observed in ecology: to take just the first step in the food chain, of the estimated 124.7 Pg/a of carbon that is fixed by photosynthesis, 64.3 Pg/a (52%) are used for the metabolism of green plants, i.e. reconverted into carbon dioxide and heat.    Earth sciences  In geology, continental drift, mountain ranges, volcanoes, and earthquakes are phenomena that can be explained in terms of energy transformations in the Earth's interior., while meteorological phenomena like wind, rain, hail, snow, lightning, tornadoes and hurricanes, are all a result of energy transformations brought about by solar energy on the atmosphere of the planet Earth. Sunlight may be stored as gravitational potential energy after it strikes the Earth, as (for example) water evaporates from oceans and is deposited upon mountains (where, after being released at a hydroelectric dam, it can be used to drive turbines or generators to produce electricity). Sunlight also drives many weather phenomena, save those generated by volcanic events. An example of a solar-mediated weather event is a hurricane, which occurs when large unstable areas of warm ocean, heated over months, give up some of their thermal energy suddenly to power a few days of violent air movement. In a slower process, radioactive decay of atoms in the core of the Earth releases heat. This thermal energy drives plate tectonics and may lift mountains, via orogenesis. This slow lifting represents a kind of gravitational potential energy storage of the thermal energy, which may be later released to active kinetic energy in landslides, after a triggering event. Earthquakes also release stored elastic potential energy in rocks, a store that has been produced ultimately from the same radioactive heat sources. Thus, according to present understanding, familiar events such as landslides and earthquakes release energy that has been stored as potential energy in the Earth's gravitational field or elastic strain (mechanical potential energy) in rocks. Prior to this, they represent release of energy that has been stored in heavy atoms since the collapse of long-destroyed supernova stars created these atoms.    Cosmology  In cosmology and astronomy the phenomena of stars, nova, supernova, quasars and gamma-ray bursts are the universe's highest-output energy transformations of matter. All stellar phenomena (including solar activity) are driven by various kinds of energy transformations. Energy in such transformations is either from gravitational collapse of matter (usually molecular hydrogen) into various classes of astronomical objects (stars, black holes, etc.), or from nuclear fusion (of lighter elements, primarily hydrogen). The nuclear fusion of hydrogen in the Sun also releases another store of potential energy which was created at the time of the Big Bang. At that time, according to theory, space expanded and the universe cooled too rapidly for hydrogen to completely fuse into heavier elements. This meant that hydrogen represents a store of potential energy that can be released by fusion. Such a fusion process is triggered by heat and pressure generated from gravitational collapse of hydrogen clouds when they produce stars, and some of the fusion energy is then transformed into sunlight.    Quantum mechanics   In quantum mechanics, energy is defined in terms of the energy operator as a time derivative of the wave function. The Schrödinger equation equates the energy operator to the full energy of a particle or a system. Its results can be considered as a definition of measurement of energy in quantum mechanics. The Schrödinger equation describes the space- and time-dependence of a slowly changing (non-relativistic) wave function of quantum systems. The solution of this equation for a bound system is discrete (a set of permitted states, each characterized by an energy level) which results in the concept of quanta. In the solution of the Schrödinger equation for any oscillator (vibrator) and for electromagnetic waves in a vacuum, the resulting energy states are related to the frequency by Planck's relation:  (where  is Planck's constant and  the frequency). In the case of an electromagnetic wave these energy states are called quanta of light or photons.    Relativity  When calculating kinetic energy (work to accelerate a mass from zero speed to some finite speed) relativistically – using Lorentz transformations instead of Newtonian mechanics – Einstein discovered an unexpected by-product of these calculations to be an energy term which does not vanish at zero speed. He called it rest mass energy: energy which every mass must possess even when being at rest. The amount of energy is directly proportional to the mass of body: , where m is the mass, c is the speed of light in vacuum, E is the rest mass energy. For example, consider electron–positron annihilation, in which the rest mass of individual particles is destroyed, but the inertia equivalent of the system of the two particles (its invariant mass) remains (since all energy is associated with mass), and this inertia and invariant mass is carried off by photons which individually are massless, but as a system retain their mass. This is a reversible process – the inverse process is called pair creation – in which the rest mass of particles is created from energy of two (or more) annihilating photons. In this system the matter (electrons and positrons) is destroyed and changed to non-matter energy (the photons). However, the total system mass and energy do not change during this interaction. In general relativity, the stress–energy tensor serves as the source term for the gravitational field, in rough analogy to the way mass serves as the source term in the non-relativistic Newtonian approximation. It is not uncommon to hear that energy is \"equivalent\" to mass. It would be more accurate to state that every energy has an inertia and gravity equivalent, and because mass is a form of energy, then mass too has inertia and gravity associated with it. In classical physics, energy is a scalar quantity, the canonical conjugate to time. In special relativity energy is also a scalar (although not a Lorentz scalar but a time component of the energy–momentum 4-vector). In other words, energy is invariant with respect to rotations of space, but not invariant with respect to rotations of space-time (= boosts).    Transformation   Energy may be transformed between different forms at various efficiencies. Items that transform between these forms are called transducers. Examples of transducers include a battery, from chemical energy to electric energy; a dam: gravitational potential energy to kinetic energy of moving water (and the blades of a turbine) and ultimately to electric energy through an electric generator. There are strict limits to how efficiently energy can be converted into other forms of energy via work, and heat as described by Carnot's theorem and the second law of thermodynamics. These limits are especially evident when an engine is used to perform work. However, some energy transformations can be quite efficient. The direction of transformations in energy (what kind of energy is transformed to what other kind) is often determined by entropy (equal energy spread among all available degrees of freedom) considerations. In practice all energy transformations are permitted on a small scale, but certain larger transformations are not permitted because it is statistically unlikely that energy or matter will randomly move into more concentrated forms or smaller spaces. Energy transformations in the universe over time are characterized by various kinds of potential energy that has been available since the Big Bang, later being \"released\" (transformed to more active types of energy such as kinetic or radiant energy), when a triggering mechanism is available. Familiar examples of such processes include nuclear decay, in which energy is released that was originally \"stored\" in heavy isotopes (such as uranium and thorium), by nucleosynthesis, a process ultimately using the gravitational potential energy released from the gravitational collapse of supernovae, to store energy in the creation of these heavy elements before they were incorporated into the solar system and the Earth. This energy is triggered and released in nuclear fission bombs or in civil nuclear power generation. Similarly, in the case of a chemical explosion, chemical potential energy is transformed to kinetic energy and thermal energy in a very short time. Yet another example is that of a pendulum. At its highest points the kinetic energy is zero and the gravitational potential energy is at maximum. At its lowest point the kinetic energy is at maximum and is equal to the decrease of potential energy. If one (unrealistically) assumes that there is no friction or other losses, the conversion of energy between these processes would be perfect, and the pendulum would continue swinging forever. Energy is also transferred from potential energy () to kinetic energy () and then back to potential energy constantly. This is referred to as conservation of energy. In this closed system, energy cannot be created or destroyed; therefore, the initial energy and the final energy will be equal to each other. This can be demonstrated by the following:  The equation can then be simplified further since  (mass times acceleration due to gravity times the height) and  (half mass times velocity squared). Then the total amount of energy can be found by adding .    Conservation of energy and mass in transformation  Energy gives rise to weight when it is trapped in a system with zero momentum, where it can be weighed. It is also equivalent to mass, and this mass is always associated with it. Mass is also equivalent to a certain amount of energy, and likewise always appears associated with it, as described in mass-energy equivalence. The formula E = mc², derived by Albert Einstein (1905) quantifies the relationship between rest-mass and rest-energy within the concept of special relativity. In different theoretical frameworks, similar formulas were derived by J. J. Thomson (1881), Henri Poincaré (1900), Friedrich Hasenöhrl (1904) and others (see Mass-energy equivalence#History for further information). Matter may be converted to energy (and vice versa), but mass cannot ever be destroyed; rather, mass/energy equivalence remains a constant for both the matter and the energy, during any process when they are converted into each other. However, since  is extremely large relative to ordinary human scales, the conversion of ordinary amount of matter (for example, 1 kg) to other forms of energy (such as heat, light, and other radiation) can liberate tremendous amounts of energy (~ joules = 21 megatons of TNT), as can be seen in nuclear reactors and nuclear weapons. Conversely, the mass equivalent of a unit of energy is minuscule, which is why a loss of energy (loss of mass) from most systems is difficult to measure by weight, unless the energy loss is very large. Examples of energy transformation into matter (i.e., kinetic energy into particles with rest mass) are found in high-energy nuclear physics.    Reversible and non-reversible transformations  Thermodynamics divides energy transformation into two kinds: reversible processes and irreversible processes. An irreversible process is one in which energy is dissipated (spread) into empty energy states available in a volume, from which it cannot be recovered into more concentrated forms (fewer quantum states), without degradation of even more energy. A reversible process is one in which this sort of dissipation does not happen. For example, conversion of energy from one type of potential field to another, is reversible, as in the pendulum system described above. In processes where heat is generated, quantum states of lower energy, present as possible excitations in fields between atoms, act as a reservoir for part of the energy, from which it cannot be recovered, in order to be converted with 100% efficiency into other forms of energy. In this case, the energy must partly stay as heat, and cannot be completely recovered as usable energy, except at the price of an increase in some other kind of heat-like increase in disorder in quantum states, in the universe (such as an expansion of matter, or a randomisation in a crystal). As the universe evolves in time, more and more of its energy becomes trapped in irreversible states (i.e., as heat or other kinds of increases in disorder). This has been referred to as the inevitable thermodynamic heat death of the universe. In this heat death the energy of the universe does not change, but the fraction of energy which is available to do work through a heat engine, or be transformed to other usable forms of energy (through the use of generators attached to heat engines), grows less and less.    Conservation of energy   According to conservation of energy, energy can neither be created (produced) nor destroyed by itself. It can only be transformed. The total inflow of energy into a system must equal the total outflow of energy from the system, plus the change in the energy contained within the system. Energy is subject to a strict global conservation law; that is, whenever one measures (or calculates) the total energy of a system of particles whose interactions do not depend explicitly on time, it is found that the total energy of the system always remains constant. Richard Feynman said during a 1961 lecture:  There is a fact, or if you wish, a law, governing all natural phenomena that are known to date. There is no known exception to this law—it is exact so far as we know. The law is called the conservation of energy. It states that there is a certain quantity, which we call energy, that does not change in manifold changes which nature undergoes. That is a most abstract idea, because it is a mathematical principle; it says that there is a numerical quantity which does not change when something happens. It is not a description of a mechanism, or anything concrete; it is just a strange fact that we can calculate some number and when we finish watching nature go through her tricks and calculate the number again, it is the same.  Most kinds of energy (with gravitational energy being a notable exception) are subject to strict local conservation laws as well. In this case, energy can only be exchanged between adjacent regions of space, and all observers agree as to the volumetric density of energy in any given space. There is also a global law of conservation of energy, stating that the total energy of the universe cannot change; this is a corollary of the local law, but not vice versa. This law is a fundamental principle of physics. As shown rigorously by Noether's theorem, the conservation of energy is a mathematical consequence of translational symmetry of time, a property of most phenomena below the cosmic scale that makes them independent of their locations on the time coordinate. Put differently, yesterday, today, and tomorrow are physically indistinguishable. This is because energy is the quantity which is canonical conjugate to time. This mathematical entanglement of energy and time also results in the uncertainty principle - it is impossible to define the exact amount of energy during any definite time interval. The uncertainty principle should not be confused with energy conservation - rather it provides mathematical limits to which energy can in principle be defined and measured. Each of the basic forces of nature is associated with a different type of potential energy, and all types of potential energy (like all other types of energy) appears as system mass, whenever present. For example, a compressed spring will be slightly more massive than before it was compressed. Likewise, whenever energy is transferred between systems by any mechanism, an associated mass is transferred with it. In quantum mechanics energy is expressed using the Hamiltonian operator. On any time scales, the uncertainty in the energy is by  which is similar in form to the Heisenberg Uncertainty Principle (but not really mathematically equivalent thereto, since H and t are not dynamically conjugate variables, neither in classical nor in quantum mechanics). In particle physics, this inequality permits a qualitative understanding of virtual particles which carry momentum, exchange by which and with real particles, is responsible for the creation of all known fundamental forces (more accurately known as fundamental interactions). Virtual photons (which are simply lowest quantum mechanical energy state of photons) are also responsible for electrostatic interaction between electric charges (which results in Coulomb law), for spontaneous radiative decay of exited atomic and nuclear states, for the Casimir force, for van der Waals bond forces and some other observable phenomena.    Transfer between systems     Closed systems  Energy transfer usually refers to movements of energy between systems which are closed to transfers of matter. The portion of the energy which is transferred by conservative forces over a distance is measured as the work the source system does on the receiving system. The portion of the energy which does not do work doing during the transfer is called heat. Energy can be transferred between systems in a variety of ways. Examples include the transmission of electromagnetic energy via photons, physical collisions which transfer kinetic energy, and the conductive transfer of thermal energy. Energy is strictly conserved and is also locally conserved wherever it can be defined. Mathematically, the process of energy transfer is described by the first law of thermodynamics:  where  is the amount of energy transferred,   represents the work done on the system, and  represents the heat flow into the system. As a simplification, the heat term, , is sometimes ignored, especially when the thermal efficiency of the transfer is high.  This simplified equation is the one used to define the joule, for example.    Open systems  There are other ways in which an open system can gain or lose energy. In chemical systems, energy can be added to a system by means of adding substances with different chemical potentials, which potentials are then extracted (both of these process are illustrated by fueling an auto, a system which gains in energy thereby, without addition of either work or heat). These terms may be added to the above equation, or they can generally be subsumed into a quantity called \"energy addition term \" which refers to any type of energy carried over the surface of a control volume or system volume. Examples may be seen above, and many others can be imagined (for example, the kinetic energy of a stream of particles entering a system, or energy from a laser beam adds to system energy, without either being either work-done or heat-added, in the classic senses).  Where  in this general equation represents other additional advected energy terms not covered by work done on a system, or heat added to it.    Thermodynamics     Internal energy  Internal energy is the sum of all microscopic forms of energy of a system. It is the energy needed to create the system. It is related to the potential energy, e.g., molecular structure, crystal structure, and other geometric aspects, as well as the motion of the particles, in form of kinetic energy. Thermodynamics is chiefly concerned with changes in internal energy and not its absolute value, which is impossible to determine with thermodynamics alone.    First law of thermodynamics  The first law of thermodynamics asserts that energy (but not necessarily thermodynamic free energy) is always conserved and that heat flow is a form of energy transfer. For homogeneous systems, with a well-defined temperature and pressure, a commonly used corollary of the first law is that, for a system subject only to pressure forces and heat transfer (e.g., a cylinder-full of gas), the differential change in the internal energy of the system (with a gain in energy signified by a positive quantity) is given as , where the first term on the right is the heat transferred into the system, expressed in terms of temperature T and entropy S (in which entropy increases and the change dS is positive when the system is heated), and the last term on the right hand side is identified as work done on the system, where pressure is P and volume V (the negative sign results since compression of the system requires work to be done on it and so the volume change, dV, is negative when work is done on the system). This equation is highly specific, ignoring all chemical, electrical, nuclear, and gravitational forces, effects such as advection of any form of energy other than heat and pV-work. The general formulation of the first law (i.e., conservation of energy) is valid even in situations in which the system is not homogeneous. For these cases the change in internal energy of a closed system is expressed in a general form by  where  is the heat supplied to the system and  is the work applied to the system.    Equipartition of energy  The energy of a mechanical harmonic oscillator (a mass on a spring) is alternatively kinetic and potential. At two points in the oscillation cycle it is entirely kinetic, and alternatively at two other points it is entirely potential. Over the whole cycle, or over many cycles, net energy is thus equally split between kinetic and potential. This is called equipartition principle; total energy of a system with many degrees of freedom is equally split among all available degrees of freedom. This principle is vitally important to understanding the behaviour of a quantity closely related to energy, called entropy. Entropy is a measure of evenness of a distribution of energy between parts of a system. When an isolated system is given more degrees of freedom (i.e., given new available energy states that are the same as existing states), then total energy spreads over all available degrees equally without distinction between \"new\" and \"old\" degrees. This mathematical result is called the second law of thermodynamics.    See also   Combustion Index of energy articles Index of wave articles Orders of magnitude (energy)    Notes     References     Further reading     External links  Energy at DMOZ","label":"foo"},{"text":"In physics, a force is any interaction that, when unopposed, will change the motion of an object. In other words, a force can cause an object with mass to change its velocity (which includes to begin moving from a state of rest), i.e., to accelerate. Force can also be described by intuitive concepts such as a push or a pull. A force has both magnitude and direction, making it a vector quantity. It is measured in the SI unit of newtons and represented by the symbol F. The original form of Newton's second law states that the net force acting upon an object is equal to the rate at which its momentum changes with time. If the mass of the object is constant, this law implies that the acceleration of an object is directly proportional to the net force acting on the object, is in the direction of the net force, and is inversely proportional to the mass of the object Related concepts to force include: thrust, which increases the velocity of an object; drag, which decreases the velocity of an object; and torque, which produces changes in rotational speed of an object. In an extended body, each part usually applies forces on the adjacent parts; the distribution of such forces through the body is the so-called mechanical stress. Pressure is a simple type of stress. Stress usually causes deformation of solid materials, or flow in fluids.    Development of the concept  Philosophers in antiquity used the concept of force in the study of stationary and moving objects and simple machines, but thinkers such as Aristotle and Archimedes retained fundamental errors in understanding force. In part this was due to an incomplete understanding of the sometimes non-obvious force of friction, and a consequently inadequate view of the nature of natural motion. A fundamental error was the belief that a force is required to maintain motion, even at a constant velocity. Most of the previous misunderstandings about motion and force were eventually corrected by Sir Isaac Newton; with his mathematical insight, he formulated laws of motion that were not improved-on for nearly three hundred years. By the early 20th century, Einstein developed a theory of relativity that correctly predicted the action of forces on objects with increasing momenta near the speed of light, and also provided insight into the forces produced by gravitation and inertia. With modern insights into quantum mechanics and technology that can accelerate particles close to the speed of light, particle physics has devised a Standard Model to describe forces between particles smaller than atoms. The Standard Model predicts that exchanged particles called gauge bosons are the fundamental means by which forces are emitted and absorbed. Only four main interactions are known: in order of decreasing strength, they are: strong, electromagnetic, weak, and gravitational. High-energy particle physics observations made during the 1970s and 1980s confirmed that the weak and electromagnetic forces are expressions of a more fundamental electroweak interaction.    Pre-Newtonian concepts   Since antiquity the concept of force has been recognized as integral to the functioning of each of the simple machines. The mechanical advantage given by a simple machine allowed for less force to be used in exchange for that force acting over a greater distance for the same amount of work. Analysis of the characteristics of forces ultimately culminated in the work of Archimedes who was especially famous for formulating a treatment of buoyant forces inherent in fluids. Aristotle provided a philosophical discussion of the concept of a force as an integral part of Aristotelian cosmology. In Aristotle's view, the terrestrial sphere contained four elements that come to rest at different \"natural places\" therein. Aristotle believed that motionless objects on Earth, those composed mostly of the elements earth and water, to be in their natural place on the ground and that they will stay that way if left alone. He distinguished between the innate tendency of objects to find their \"natural place\" (e.g., for heavy bodies to fall), which led to \"natural motion\", and unnatural or forced motion, which required continued application of a force. This theory, based on the everyday experience of how objects move, such as the constant application of a force needed to keep a cart moving, had conceptual trouble accounting for the behavior of projectiles, such as the flight of arrows. The place where the archer moves the projectile was at the start of the flight, and while the projectile sailed through the air, no discernible efficient cause acts on it. Aristotle was aware of this problem and proposed that the air displaced through the projectile's path carries the projectile to its target. This explanation demands a continuum like air for change of place in general. Aristotelian physics began facing criticism in Medieval science, first by John Philoponus in the 6th century. The shortcomings of Aristotelian physics would not be fully corrected until the 17th century work of Galileo Galilei, who was influenced by the late Medieval idea that objects in forced motion carried an innate force of impetus. Galileo constructed an experiment in which stones and cannonballs were both rolled down an incline to disprove the Aristotelian theory of motion early in the 17th century. He showed that the bodies were accelerated by gravity to an extent that was independent of their mass and argued that objects retain their velocity unless acted on by a force, for example friction.    Newtonian mechanics   Sir Isaac Newton sought to describe the motion of all objects using the concepts of inertia and force, and in doing so he found that they obey certain conservation laws. In 1687, Newton went on to publish his thesis Philosophiæ Naturalis Principia Mathematica. In this work Newton set out three laws of motion that to this day are the way forces are described in physics.    First law   Newton's First Law of Motion states that objects continue to move in a state of constant velocity unless acted upon by an external net force or resultant force. This law is an extension of Galileo's insight that constant velocity was associated with a lack of net force (see a more detailed description of this below). Newton proposed that every object with mass has an innate inertia that functions as the fundamental equilibrium \"natural state\" in place of the Aristotelian idea of the \"natural state of rest\". That is, the first law contradicts the intuitive Aristotelian belief that a net force is required to keep an object moving with constant velocity. By making rest physically indistinguishable from non-zero constant velocity, Newton's First Law directly connects inertia with the concept of relative velocities. Specifically, in systems where objects are moving with different velocities, it is impossible to determine which object is \"in motion\" and which object is \"at rest\". In other words, to phrase matters more technically, the laws of physics are the same in every inertial frame of reference, that is, in all frames related by a Galilean transformation. For instance, while traveling in a moving vehicle at a constant velocity, the laws of physics do not change from being at rest. A person can throw a ball straight up in the air and catch it as it falls down without worrying about applying a force in the direction the vehicle is moving. This is true even though another person who is observing the moving vehicle pass by also observes the ball follow a curving parabolic path in the same direction as the motion of the vehicle. It is the inertia of the ball associated with its constant velocity in the direction of the vehicle's motion that ensures the ball continues to move forward even as it is thrown up and falls back down. From the perspective of the person in the car, the vehicle and everything inside of it is at rest: It is the outside world that is moving with a constant speed in the opposite direction. Since there is no experiment that can distinguish whether it is the vehicle that is at rest or the outside world that is at rest, the two situations are considered to be physically indistinguishable. Inertia therefore applies equally well to constant velocity motion as it does to rest. The concept of inertia can be further generalized to explain the tendency of objects to continue in many different forms of constant motion, even those that are not strictly constant velocity. The rotational inertia of planet Earth is what fixes the constancy of the length of a day and the length of a year. Albert Einstein extended the principle of inertia further when he explained that reference frames subject to constant acceleration, such as those free-falling toward a gravitating object, were physically equivalent to inertial reference frames. This is why, for example, astronauts experience weightlessness when in free-fall orbit around the Earth, and why Newton's Laws of Motion are more easily discernible in such environments. If an astronaut places an object with mass in mid-air next to himself, it will remain stationary with respect to the astronaut due to its inertia. This is the same thing that would occur if the astronaut and the object were in intergalactic space with no net force of gravity acting on their shared reference frame. This principle of equivalence was one of the foundational underpinnings for the development of the general theory of relativity.    Second law   A modern statement of Newton's Second Law is a vector equation:  where  is the momentum of the system, and  is the net (vector sum) force. In equilibrium, there is zero net force by definition, but (balanced) forces may be present nevertheless. In contrast, the second law states an unbalanced force acting on an object will result in the object's momentum changing over time. By the definition of momentum,  where m is the mass and  is the velocity. Newton's second law applies only to a system of constant mass, and hence m may be moved outside the derivative operator. The equation then becomes  By substituting the definition of acceleration, the algebraic version of Newton's Second Law is derived:  Newton never explicitly stated the formula in the reduced form above. Newton's Second Law asserts the direct proportionality of acceleration to force and the inverse proportionality of acceleration to mass. Accelerations can be defined through kinematic measurements. However, while kinematics are well-described through reference frame analysis in advanced physics, there are still deep questions that remain as to what is the proper definition of mass. General relativity offers an equivalence between space-time and mass, but lacking a coherent theory of quantum gravity, it is unclear as to how or whether this connection is relevant on microscales. With some justification, Newton's second law can be taken as a quantitative definition of mass by writing the law as an equality; the relative units of force and mass then are fixed. The use of Newton's Second Law as a definition of force has been disparaged in some of the more rigorous textbooks, because it is essentially a mathematical truism. Notable physicists, philosophers and mathematicians who have sought a more explicit definition of the concept of force include Ernst Mach, Clifford Truesdell and Walter Noll. Newton's Second Law can be used to measure the strength of forces. For instance, knowledge of the masses of planets along with the accelerations of their orbits allows scientists to calculate the gravitational forces on planets.    Third law   Newton's Third Law is a result of applying symmetry to situations where forces can be attributed to the presence of different objects. The third law means that all forces are interactions between different bodies, and thus that there is no such thing as a unidirectional force or a force that acts on only one body. Whenever a first body exerts a force F on a second body, the second body exerts a force −F on the first body. F and −F are equal in magnitude and opposite in direction. This law is sometimes referred to as the action-reaction law, with F called the \"action\" and −F the \"reaction\". The action and the reaction are simultaneous:  If object 1 and object 2 are considered to be in the same system, then the net force on the system due to the interactions between objects 1 and 2 is zero since  This means that in a closed system of particles, there are no internal forces that are unbalanced. That is, the action-reaction force shared between any two objects in a closed system will not cause the center of mass of the system to accelerate. The constituent objects only accelerate with respect to each other, the system itself remains unaccelerated. Alternatively, if an external force acts on the system, then the center of mass will experience an acceleration proportional to the magnitude of the external force divided by the mass of the system. Combining Newton's Second and Third Laws, it is possible to show that the linear momentum of a system is conserved. Using  and integrating with respect to time, the equation:  is obtained. For a system that includes objects 1 and 2, , which is the conservation of linear momentum. Using the similar arguments, it is possible to generalize this to a system of an arbitrary number of particles. This shows that exchanging momentum between constituent objects will not affect the net momentum of a system. In general, as long as all forces are due to the interaction of objects with mass, it is possible to define a system such that net momentum is never lost nor gained.    Special theory of relativity  In the special theory of relativity, mass and energy are equivalent (as can be seen by calculating the work required to accelerate an object). When an object's velocity increases, so does its energy and hence its mass equivalent (inertia). It thus requires more force to accelerate it the same amount than it did at a lower velocity. Newton's Second Law  remains valid because it is a mathematical definition. But in order to be conserved, relativistic momentum must be redefined as:  where  is the velocity and  is the speed of light  is the rest mass. The relativistic expression relating force and acceleration for a particle with constant non-zero rest mass  moving in the  direction is:  where the Lorentz factor  In the early history of relativity, the expressions  and  were called longitudinal and transverse mass. Relativistic force does not produce a constant acceleration, but an ever decreasing acceleration as the object approaches the speed of light. Note that  is undefined for an object with a non-zero rest mass at the speed of light, and the theory yields no prediction at that speed. If  is very small compared to , then  is very close to 1 and  is a close approximation. Even for use in relativity, however, one can restore the form of  through the use of four-vectors. This relation is correct in relativity when  is the four-force,  is the invariant mass, and  is the four-acceleration.    Descriptions   Since forces are perceived as pushes or pulls, this can provide an intuitive understanding for describing forces. As with other physical concepts (e.g. temperature), the intuitive understanding of forces is quantified using precise operational definitions that are consistent with direct observations and compared to a standard measurement scale. Through experimentation, it is determined that laboratory measurements of forces are fully consistent with the conceptual definition of force offered by Newtonian mechanics. Forces act in a particular direction and have sizes dependent upon how strong the push or pull is. Because of these characteristics, forces are classified as \"vector quantities\". This means that forces follow a different set of mathematical rules than physical quantities that do not have direction (denoted scalar quantities). For example, when determining what happens when two forces act on the same object, it is necessary to know both the magnitude and the direction of both forces to calculate the result. If both of these pieces of information are not known for each force, the situation is ambiguous. For example, if you know that two people are pulling on the same rope with known magnitudes of force but you do not know which direction either person is pulling, it is impossible to determine what the acceleration of the rope will be. The two people could be pulling against each other as in tug of war or the two people could be pulling in the same direction. In this simple one-dimensional example, without knowing the direction of the forces it is impossible to decide whether the net force is the result of adding the two force magnitudes or subtracting one from the other. Associating forces with vectors avoids such problems. Historically, forces were first quantitatively investigated in conditions of static equilibrium where several forces canceled each other out. Such experiments demonstrate the crucial properties that forces are additive vector quantities: they have magnitude and direction. When two forces act on a point particle, the resulting force, the resultant (also called the net force), can be determined by following the parallelogram rule of vector addition: the addition of two vectors represented by sides of a parallelogram, gives an equivalent resultant vector that is equal in magnitude and direction to the transversal of the parallelogram. The magnitude of the resultant varies from the difference of the magnitudes of the two forces to their sum, depending on the angle between their lines of action. However, if the forces are acting on an extended body, their respective lines of application must also be specified in order to account for their effects on the motion of the body. Free-body diagrams can be used as a convenient way to keep track of forces acting on a system. Ideally, these diagrams are drawn with the angles and relative magnitudes of the force vectors preserved so that graphical vector addition can be done to determine the net force. As well as being added, forces can also be resolved into independent components at right angles to each other. A horizontal force pointing northeast can therefore be split into two forces, one pointing north, and one pointing east. Summing these component forces using vector addition yields the original force. Resolving force vectors into components of a set of basis vectors is often a more mathematically clean way to describe forces than using magnitudes and directions. This is because, for orthogonal components, the components of the vector sum are uniquely determined by the scalar addition of the components of the individual vectors. Orthogonal components are independent of each other because forces acting at ninety degrees to each other have no effect on the magnitude or direction of the other. Choosing a set of orthogonal basis vectors is often done by considering what set of basis vectors will make the mathematics most convenient. Choosing a basis vector that is in the same direction as one of the forces is desirable, since that force would then have only one non-zero component. Orthogonal force vectors can be three-dimensional with the third component being at right-angles to the other two.    Equilibrium  Equilibrium occurs when the resultant force acting on a point particle is zero (that is, the vector sum of all forces is zero). When dealing with an extended body, it is also necessary that the net torque in it is 0. There are two kinds of equilibrium: static equilibrium and dynamic equilibrium.   = Static =  Static equilibrium was understood well before the invention of classical mechanics. Objects that are at rest have zero net force acting on them. The simplest case of static equilibrium occurs when two forces are equal in magnitude but opposite in direction. For example, an object on a level surface is pulled (attracted) downward toward the center of the Earth by the force of gravity. At the same time, surface forces resist the downward force with equal upward force (called the normal force). The situation is one of zero net force and no acceleration. Pushing against an object on a frictional surface can result in a situation where the object does not move because the applied force is opposed by static friction, generated between the object and the table surface. For a situation with no movement, the static friction force exactly balances the applied force resulting in no acceleration. The static friction increases or decreases in response to the applied force up to an upper limit determined by the characteristics of the contact between the surface and the object. A static equilibrium between two forces is the most usual way of measuring forces, using simple devices such as weighing scales and spring balances. For example, an object suspended on a vertical spring scale experiences the force of gravity acting on the object balanced by a force applied by the \"spring reaction force\", which equals the object's weight. Using such tools, some quantitative force laws were discovered: that the force of gravity is proportional to volume for objects of constant density (widely exploited for millennia to define standard weights); Archimedes' principle for buoyancy; Archimedes' analysis of the lever; Boyle's law for gas pressure; and Hooke's law for springs. These were all formulated and experimentally verified before Isaac Newton expounded his Three Laws of Motion.   = Dynamic =  Dynamic equilibrium was first described by Galileo who noticed that certain assumptions of Aristotelian physics were contradicted by observations and logic. Galileo realized that simple velocity addition demands that the concept of an \"absolute rest frame\" did not exist. Galileo concluded that motion in a constant velocity was completely equivalent to rest. This was contrary to Aristotle's notion of a \"natural state\" of rest that objects with mass naturally approached. Simple experiments showed that Galileo's understanding of the equivalence of constant velocity and rest were correct. For example, if a mariner dropped a cannonball from the crow's nest of a ship moving at a constant velocity, Aristotelian physics would have the cannonball fall straight down while the ship moved beneath it. Thus, in an Aristotelian universe, the falling cannonball would land behind the foot of the mast of a moving ship. However, when this experiment is actually conducted, the cannonball always falls at the foot of the mast, as if the cannonball knows to travel with the ship despite being separated from it. Since there is no forward horizontal force being applied on the cannonball as it falls, the only conclusion left is that the cannonball continues to move with the same velocity as the boat as it falls. Thus, no force is required to keep the cannonball moving at the constant forward velocity. Moreover, any object traveling at a constant velocity must be subject to zero net force (resultant force). This is the definition of dynamic equilibrium: when all the forces on an object balance but it still moves at a constant velocity. A simple case of dynamic equilibrium occurs in constant velocity motion across a surface with kinetic friction. In such a situation, a force is applied in the direction of motion while the kinetic friction force exactly opposes the applied force. This results in zero net force, but since the object started with a non-zero velocity, it continues to move with a non-zero velocity. Aristotle misinterpreted this motion as being caused by the applied force. However, when kinetic friction is taken into consideration it is clear that there is no net force causing constant velocity motion.    Forces in Quantum Mechanics   The notion \"force\" keeps its meaning in quantum mechanics, though one is now dealing with operators instead of classical variables and though the physics is now described by the Schrödinger equation instead of Newtonian equations. This has the consequence that the results of a measurement are now sometimes \"quantized\", i.e. they appear in discrete portions. This is, of course, difficult to imagine in the context of \"forces\". However, the potentials V(x,y,z) or fields, from which the forces generally can be derived, are treated similar to classical position variables, i.e., . This becomes different only in the framework of quantum field theory, where these fields are also quantized. However, already in quantum mechanics there is one \"caveat\", namely the particles acting onto each other do not only possess the spatial variable, but also a discrete intrinsic angular momentum-like variable called the \"spin\", and there is the Pauli principle relating the space and the spin variables. Depending on the value of the spin, identical particles split into two different classes, fermions and bosons. If two identical fermions (e.g. electrons) have a symmetric spin function (e.g. parallel spins) the spatial variables must be antisymmetric (i.e. the force must be repulsive), and vice versa, i.e. for antiparallel spins the position variables must be symmetric (i.e. the force must be attractive). Thus in the case of two fermions there is a strictly negative correlation between spatial and spin variables, whereas for two bosons (e.g. quanta of electromagnetic waves, photons) the correlation is strictly positive. Thus the notion \"force\" loses already part of its meaning.    Feynman diagrams   In modern particle physics, forces and the acceleration of particles are explained as a mathematical by-product of exchange of momentum-carrying gauge bosons. With the development of quantum field theory and general relativity, it was realized that force is a redundant concept arising from conservation of momentum (4-momentum in relativity and momentum of virtual particles in quantum electrodynamics). The conservation of momentum can be directly derived from the homogeneity or symmetry of space and so is usually considered more fundamental than the concept of a force. Thus the currently known fundamental forces are considered more accurately to be \"fundamental interactions\". When particle A emits (creates) or absorbs (annihilates) virtual particle B, a momentum conservation results in recoil of particle A making impression of repulsion or attraction between particles A A' exchanging by B. This description applies to all forces arising from fundamental interactions. While sophisticated mathematical descriptions are needed to predict, in full detail, the accurate result of such interactions, there is a conceptually simple way to describe such interactions through the use of Feynman diagrams. In a Feynman diagram, each matter particle is represented as a straight line (see world line) traveling through time, which normally increases up or to the right in the diagram. Matter and anti-matter particles are identical except for their direction of propagation through the Feynman diagram. World lines of particles intersect at interaction vertices, and the Feynman diagram represents any force arising from an interaction as occurring at the vertex with an associated instantaneous change in the direction of the particle world lines. Gauge bosons are emitted away from the vertex as wavy lines and, in the case of virtual particle exchange, are absorbed at an adjacent vertex. The utility of Feynman diagrams is that other types of physical phenomena that are part of the general picture of fundamental interactions but are conceptually separate from forces can also be described using the same rules. For example, a Feynman diagram can describe in succinct detail how a neutron decays into an electron, proton, and neutrino, an interaction mediated by the same gauge boson that is responsible for the weak nuclear force.    Fundamental forces   All of the forces in the universe are based on four fundamental interactions. The strong and weak forces are nuclear forces that act only at very short distances, and are responsible for the interactions between subatomic particles, including nucleons and compound nuclei. The electromagnetic force acts between electric charges, and the gravitational force acts between masses. All other forces in nature derive from these four fundamental interactions. For example, friction is a manifestation of the electromagnetic force acting between the atoms of two surfaces, and the Pauli exclusion principle, which does not permit atoms to pass through each other. Similarly, the forces in springs, modeled by Hooke's law, are the result of electromagnetic forces and the Exclusion Principle acting together to return an object to its equilibrium position. Centrifugal forces are acceleration forces that arise simply from the acceleration of rotating frames of reference. The development of fundamental theories for forces proceeded along the lines of unification of disparate ideas. For example, Isaac Newton unified the force responsible for objects falling at the surface of the Earth with the force responsible for the orbits of celestial mechanics in his universal theory of gravitation. Michael Faraday and James Clerk Maxwell demonstrated that electric and magnetic forces were unified through one consistent theory of electromagnetism. In the 20th century, the development of quantum mechanics led to a modern understanding that the first three fundamental forces (all except gravity) are manifestations of matter (fermions) interacting by exchanging virtual particles called gauge bosons. This standard model of particle physics posits a similarity between the forces and led scientists to predict the unification of the weak and electromagnetic forces in electroweak theory subsequently confirmed by observation. The complete formulation of the standard model predicts an as yet unobserved Higgs mechanism, but observations such as neutrino oscillations indicate that the standard model is incomplete. A Grand Unified Theory allowing for the combination of the electroweak interaction with the strong force is held out as a possibility with candidate theories such as supersymmetry proposed to accommodate some of the outstanding unsolved problems in physics. Physicists are still attempting to develop self-consistent unification models that would combine all four fundamental interactions into a theory of everything. Einstein tried and failed at this endeavor, but currently the most popular approach to answering this question is string theory.    Gravitational   What we now call gravity was not identified as a universal force until the work of Isaac Newton. Before Newton, the tendency for objects to fall towards the Earth was not understood to be related to the motions of celestial objects. Galileo was instrumental in describing the characteristics of falling objects by determining that the acceleration of every object in free-fall was constant and independent of the mass of the object. Today, this acceleration due to gravity towards the surface of the Earth is usually designated as  and has a magnitude of about 9.81 meters per second squared (this measurement is taken from sea level and may vary depending on location), and points toward the center of the Earth. This observation means that the force of gravity on an object at the Earth's surface is directly proportional to the object's mass. Thus an object that has a mass of  will experience a force:  In free-fall, this force is unopposed and therefore the net force on the object is its weight. For objects not in free-fall, the force of gravity is opposed by the reactions of their supports. For example, a person standing on the ground experiences zero net force, since his weight is balanced by a normal force exerted by the ground. Newton's contribution to gravitational theory was to unify the motions of heavenly bodies, which Aristotle had assumed were in a natural state of constant motion, with falling motion observed on the Earth. He proposed a law of gravity that could account for the celestial motions that had been described earlier using Kepler's laws of planetary motion. Newton came to realize that the effects of gravity might be observed in different ways at larger distances. In particular, Newton determined that the acceleration of the Moon around the Earth could be ascribed to the same force of gravity if the acceleration due to gravity decreased as an inverse square law. Further, Newton realized that the acceleration due to gravity is proportional to the mass of the attracting body. Combining these ideas gives a formula that relates the mass () and the radius () of the Earth to the gravitational acceleration:  where the vector direction is given by , the unit vector directed outward from the center of the Earth. In this equation, a dimensional constant  is used to describe the relative strength of gravity. This constant has come to be known as Newton's Universal Gravitation Constant, though its value was unknown in Newton's lifetime. Not until 1798 was Henry Cavendish able to make the first measurement of  using a torsion balance; this was widely reported in the press as a measurement of the mass of the Earth since knowing  could allow one to solve for the Earth's mass given the above equation. Newton, however, realized that since all celestial bodies followed the same laws of motion, his law of gravity had to be universal. Succinctly stated, Newton's Law of Gravitation states that the force on a spherical object of mass  due to the gravitational pull of mass  is  where  is the distance between the two objects' centers of mass and  is the unit vector pointed in the direction away from the center of the first object toward the center of the second object. This formula was powerful enough to stand as the basis for all subsequent descriptions of motion within the solar system until the 20th century. During that time, sophisticated methods of perturbation analysis were invented to calculate the deviations of orbits due to the influence of multiple bodies on a planet, moon, comet, or asteroid. The formalism was exact enough to allow mathematicians to predict the existence of the planet Neptune before it was observed.  It was only the orbit of the planet Mercury that Newton's Law of Gravitation seemed not to fully explain. Some astrophysicists predicted the existence of another planet (Vulcan) that would explain the discrepancies; however, despite some early indications, no such planet could be found. When Albert Einstein formulated his theory of general relativity (GR) he turned his attention to the problem of Mercury's orbit and found that his theory added a correction, which could account for the discrepancy. This was the first time that Newton's Theory of Gravity had been shown to be less correct than an alternative. Since then, and so far, general relativity has been acknowledged as the theory that best explains gravity. In GR, gravitation is not viewed as a force, but rather, objects moving freely in gravitational fields travel under their own inertia in straight lines through curved space-time – defined as the shortest space-time path between two space-time events. From the perspective of the object, all motion occurs as if there were no gravitation whatsoever. It is only when observing the motion in a global sense that the curvature of space-time can be observed and the force is inferred from the object's curved path. Thus, the straight line path in space-time is seen as a curved line in space, and it is called the ballistic trajectory of the object. For example, a basketball thrown from the ground moves in a parabola, as it is in a uniform gravitational field. Its space-time trajectory (when the extra ct dimension is added) is almost a straight line, slightly curved (with the radius of curvature of the order of few light-years). The time derivative of the changing momentum of the object is what we label as \"gravitational force\".    Electromagnetic   The electrostatic force was first described in 1784 by Coulomb as a force that existed intrinsically between two charges. The properties of the electrostatic force were that it varied as an inverse square law directed in the radial direction, was both attractive and repulsive (there was intrinsic polarity), was independent of the mass of the charged objects, and followed the superposition principle. Coulomb's law unifies all these observations into one succinct statement. Subsequent mathematicians and physicists found the construct of the electric field to be useful for determining the electrostatic force on an electric charge at any point in space. The electric field was based on using a hypothetical \"test charge\" anywhere in space and then using Coulomb's Law to determine the electrostatic force. Thus the electric field anywhere in space is defined as  where  is the magnitude of the hypothetical test charge. Meanwhile, the Lorentz force of magnetism was discovered to exist between two electric currents. It has the same mathematical character as Coulomb's Law with the proviso that like currents attract and unlike currents repel. Similar to the electric field, the magnetic field can be used to determine the magnetic force on an electric current at any point in space. In this case, the magnitude of the magnetic field was determined to be  where  is the magnitude of the hypothetical test current and  is the length of hypothetical wire through which the test current flows. The magnetic field exerts a force on all magnets including, for example, those used in compasses. The fact that the Earth's magnetic field is aligned closely with the orientation of the Earth's axis causes compass magnets to become oriented because of the magnetic force pulling on the needle. Through combining the definition of electric current as the time rate of change of electric charge, a rule of vector multiplication called Lorentz's Law describes the force on a charge moving in a magnetic field. The connection between electricity and magnetism allows for the description of a unified electromagnetic force that acts on a charge. This force can be written as a sum of the electrostatic force (due to the electric field) and the magnetic force (due to the magnetic field). Fully stated, this is the law:  where  is the electromagnetic force,  is the magnitude of the charge of the particle,  is the electric field,  is the velocity of the particle that is crossed with the magnetic field (). The origin of electric and magnetic fields would not be fully explained until 1864 when James Clerk Maxwell unified a number of earlier theories into a set of 20 scalar equations, which were later reformulated into 4 vector equations by Oliver Heaviside and Josiah Willard Gibbs. These \"Maxwell Equations\" fully described the sources of the fields as being stationary and moving charges, and the interactions of the fields themselves. This led Maxwell to discover that electric and magnetic fields could be \"self-generating\" through a wave that traveled at a speed that he calculated to be the speed of light. This insight united the nascent fields of electromagnetic theory with optics and led directly to a complete description of the electromagnetic spectrum. However, attempting to reconcile electromagnetic theory with two observations, the photoelectric effect, and the nonexistence of the ultraviolet catastrophe, proved troublesome. Through the work of leading theoretical physicists, a new theory of electromagnetism was developed using quantum mechanics. This final modification to electromagnetic theory ultimately led to quantum electrodynamics (or QED), which fully describes all electromagnetic phenomena as being mediated by wave–particles known as photons. In QED, photons are the fundamental exchange particle, which described all interactions relating to electromagnetism including the electromagnetic force. It is a common misconception to ascribe the stiffness and rigidity of solid matter to the repulsion of like charges under the influence of the electromagnetic force. However, these characteristics actually result from the Pauli exclusion principle. Since electrons are fermions, they cannot occupy the same quantum mechanical state as other electrons. When the electrons in a material are densely packed together, there are not enough lower energy quantum mechanical states for them all, so some of them must be in higher energy states. This means that it takes energy to pack them together. While this effect is manifested macroscopically as a structural force, it is technically only the result of the existence of a finite set of electron states.    Nuclear   There are two \"nuclear forces\", which today are usually described as interactions that take place in quantum theories of particle physics. The strong nuclear force is the force responsible for the structural integrity of atomic nuclei while the weak nuclear force is responsible for the decay of certain nucleons into leptons and other types of hadrons. The strong force is today understood to represent the interactions between quarks and gluons as detailed by the theory of quantum chromodynamics (QCD). The strong force is the fundamental force mediated by gluons, acting upon quarks, antiquarks, and the gluons themselves. The (aptly named) strong interaction is the \"strongest\" of the four fundamental forces. The strong force only acts directly upon elementary particles. However, a residual of the force is observed between hadrons (the best known example being the force that acts between nucleons in atomic nuclei) as the nuclear force. Here the strong force acts indirectly, transmitted as gluons, which form part of the virtual pi and rho mesons, which classically transmit the nuclear force (see this topic for more). The failure of many searches for free quarks has shown that the elementary particles affected are not directly observable. This phenomenon is called color confinement. The weak force is due to the exchange of the heavy W and Z bosons. Its most familiar effect is beta decay (of neutrons in atomic nuclei) and the associated radioactivity. The word \"weak\" derives from the fact that the field strength is some 1013 times less than that of the strong force. Still, it is stronger than gravity over short distances. A consistent electroweak theory has also been developed, which shows that electromagnetic forces and the weak force are indistinguishable at a temperatures in excess of approximately 1015 kelvins. Such temperatures have been probed in modern particle accelerators and show the conditions of the universe in the early moments of the Big Bang.    Non-fundamental forces  Some forces are consequences of the fundamental ones. In such situations, idealized models can be utilized to gain physical insight.    Normal force   The normal force is due to repulsive forces of interaction between atoms at close contact. When their electron clouds overlap, Pauli repulsion (due to fermionic nature of electrons) follows resulting in the force that acts in a direction normal to the surface interface between two objects. The normal force, for example, is responsible for the structural integrity of tables and floors as well as being the force that responds whenever an external force pushes on a solid object. An example of the normal force in action is the impact force on an object crashing into an immobile surface.    Friction   Friction is a surface force that opposes relative motion. The frictional force is directly related to the normal force that acts to keep two solid objects separated at the point of contact. There are two broad classifications of frictional forces: static friction and kinetic friction. The static friction force () will exactly oppose forces applied to an object parallel to a surface contact up to the limit specified by the coefficient of static friction () multiplied by the normal force (). In other words, the magnitude of the static friction force satisfies the inequality:  The kinetic friction force () is independent of both the forces applied and the movement of the object. Thus, the magnitude of the force equals:  where  is the coefficient of kinetic friction. For most surface interfaces, the coefficient of kinetic friction is less than the coefficient of static friction.    Tension   Tension forces can be modeled using ideal strings that are massless, frictionless, unbreakable, and unstretchable. They can be combined with ideal pulleys, which allow ideal strings to switch physical direction. Ideal strings transmit tension forces instantaneously in action-reaction pairs so that if two objects are connected by an ideal string, any force directed along the string by the first object is accompanied by a force directed along the string in the opposite direction by the second object. By connecting the same string multiple times to the same object through the use of a set-up that uses movable pulleys, the tension force on a load can be multiplied. For every string that acts on a load, another factor of the tension force in the string acts on the load. However, even though such machines allow for an increase in force, there is a corresponding increase in the length of string that must be displaced in order to move the load. These tandem effects result ultimately in the conservation of mechanical energy since the work done on the load is the same no matter how complicated the machine.    Elastic force   An elastic force acts to return a spring to its natural length. An ideal spring is taken to be massless, frictionless, unbreakable, and infinitely stretchable. Such springs exert forces that push when contracted, or pull when extended, in proportion to the displacement of the spring from its equilibrium position. This linear relationship was described by Robert Hooke in 1676, for whom Hooke's law is named. If  is the displacement, the force exerted by an ideal spring equals:  where  is the spring constant (or force constant), which is particular to the spring. The minus sign accounts for the tendency of the force to act in opposition to the applied load.    Continuum mechanics   Newton's laws and Newtonian mechanics in general were first developed to describe how forces affect idealized point particles rather than three-dimensional objects. However, in real life, matter has extended structure and forces that act on one part of an object might affect other parts of an object. For situations where lattice holding together the atoms in an object is able to flow, contract, expand, or otherwise change shape, the theories of continuum mechanics describe the way forces affect the material. For example, in extended fluids, differences in pressure result in forces being directed along the pressure gradients as follows:  where  is the volume of the object in the fluid and  is the scalar function that describes the pressure at all locations in space. Pressure gradients and differentials result in the buoyant force for fluids suspended in gravitational fields, winds in atmospheric science, and the lift associated with aerodynamics and flight. A specific instance of such a force that is associated with dynamic pressure is fluid resistance: a body force that resists the motion of an object through a fluid due to viscosity. For so-called \"Stokes' drag\" the force is approximately proportional to the velocity, but opposite in direction:  where:  is a constant that depends on the properties of the fluid and the dimensions of the object (usually the cross-sectional area), and  is the velocity of the object. More formally, forces in continuum mechanics are fully described by a stress–tensor with terms that are roughly defined as  where  is the relevant cross-sectional area for the volume for which the stress-tensor is being calculated. This formalism includes pressure terms associated with forces that act normal to the cross-sectional area (the matrix diagonals of the tensor) as well as shear terms associated with forces that act parallel to the cross-sectional area (the off-diagonal elements). The stress tensor accounts for forces that cause all strains (deformations) including also tensile stresses and compressions.    Fictitious forces   There are forces that are frame dependent, meaning that they appear due to the adoption of non-Newtonian (that is, non-inertial) reference frames. Such forces include the centrifugal force and the Coriolis force. These forces are considered fictitious because they do not exist in frames of reference that are not accelerating. Because these forces are not genuine they are also referred to as \"pseudo forces\". In general relativity, gravity becomes a fictitious force that arises in situations where spacetime deviates from a flat geometry. As an extension, Kaluza–Klein theory and string theory ascribe electromagnetism and the other fundamental forces respectively to the curvature of differently scaled dimensions, which would ultimately imply that all forces are fictitious.    Rotations and torque   Forces that cause extended objects to rotate are associated with torques. Mathematically, the torque of a force  is defined relative to an arbitrary reference point as the cross-product:  where  is the position vector of the force application point relative to the reference point. Torque is the rotation equivalent of force in the same way that angle is the rotational equivalent for position, angular velocity for velocity, and angular momentum for momentum. As a consequence of Newton's First Law of Motion, there exists rotational inertia that ensures that all bodies maintain their angular momentum unless acted upon by an unbalanced torque. Likewise, Newton's Second Law of Motion can be used to derive an analogous equation for the instantaneous angular acceleration of the rigid body:  where  is the moment of inertia of the body  is the angular acceleration of the body. This provides a definition for the moment of inertia, which is the rotational equivalent for mass. In more advanced treatments of mechanics, where the rotation over a time interval is described, the moment of inertia must be substituted by the tensor that, when properly analyzed, fully determines the characteristics of rotations including precession and nutation. Equivalently, the differential form of Newton's Second Law provides an alternative definition of torque:  where  is the angular momentum of the particle. Newton's Third Law of Motion requires that all objects exerting torques themselves experience equal and opposite torques, and therefore also directly implies the conservation of angular momentum for closed systems that experience rotations and revolutions through the action of internal torques.    Centripetal force   For an object accelerating in circular motion, the unbalanced force acting on the object equals:  where  is the mass of the object,  is the velocity of the object and  is the distance to the center of the circular path and  is the unit vector pointing in the radial direction outwards from the center. This means that the unbalanced centripetal force felt by any object is always directed toward the center of the curving path. Such forces act perpendicular to the velocity vector associated with the motion of an object, and therefore do not change the speed of the object (magnitude of the velocity), but only the direction of the velocity vector. The unbalanced force that accelerates an object can be resolved into a component that is perpendicular to the path, and one that is tangential to the path. This yields both the tangential force, which accelerates the object by either slowing it down or speeding it up, and the radial (centripetal) force, which changes its direction.    Kinematic integrals   Forces can be used to define a number of physical concepts by integrating with respect to kinematic variables. For example, integrating with respect to time gives the definition of impulse:  which by Newton's Second Law must be equivalent to the change in momentum (yielding the Impulse momentum theorem). Similarly, integrating with respect to position gives a definition for the work done by a force:  which is equivalent to changes in kinetic energy (yielding the work energy theorem). Power P is the rate of change dW/dt of the work W, as the trajectory is extended by a position change  in a time interval dt:  with  the velocity.    Potential energy   Instead of a force, often the mathematically related concept of a potential energy field can be used for convenience. For instance, the gravitational force acting upon an object can be seen as the action of the gravitational field that is present at the object's location. Restating mathematically the definition of energy (via the definition of work), a potential scalar field  is defined as that field whose gradient is equal and opposite to the force produced at every point:  Forces can be classified as conservative or nonconservative. Conservative forces are equivalent to the gradient of a potential while nonconservative forces are not.    Conservative forces   A conservative force that acts on a closed system has an associated mechanical work that allows energy to convert only between kinetic or potential forms. This means that for a closed system, the net mechanical energy is conserved whenever a conservative force acts on the system. The force, therefore, is related directly to the difference in potential energy between two different locations in space, and can be considered to be an artifact of the potential field in the same way that the direction and amount of a flow of water can be considered to be an artifact of the contour map of the elevation of an area. Conservative forces include gravity, the electromagnetic force, and the spring force. Each of these forces has models that are dependent on a position often given as a radial vector  emanating from spherically symmetric potentials. Examples of this follow: For gravity:  where  is the gravitational constant, and  is the mass of object n. For electrostatic forces:  where  is electric permittivity of free space, and  is the electric charge of object n. For spring forces:  where  is the spring constant.    Nonconservative forces  For certain physical scenarios, it is impossible to model forces as being due to gradient of potentials. This is often due to macrophysical considerations that yield forces as arising from a macroscopic statistical average of microstates. For example, friction is caused by the gradients of numerous electrostatic potentials between the atoms, but manifests as a force model that is independent of any macroscale position vector. Nonconservative forces other than friction include other contact forces, tension, compression, and drag. However, for any sufficiently detailed description, all these forces are the results of conservative ones since each of these macroscopic forces are the net results of the gradients of microscopic potentials. The connection between macroscopic nonconservative forces and microscopic conservative forces is described by detailed treatment with statistical mechanics. In macroscopic closed systems, nonconservative forces act to change the internal energies of the system, and are often associated with the transfer of heat. According to the Second law of thermodynamics, nonconservative forces necessarily result in energy transformations within closed systems from ordered to more random conditions as entropy increases.    Units of measurement  The SI unit of force is the newton (symbol N), which is the force required to accelerate a one kilogram mass at a rate of one meter per second squared, or kg·m·s−2. The corresponding CGS unit is the dyne, the force required to accelerate a one gram mass by one centimeter per second squared, or g·cm·s−2. A newton is thus equal to 100,000 dynes. The gravitational foot-pound-second English unit of force is the pound-force (lbf), defined as the force exerted by gravity on a pound-mass in the standard gravitational field of 9.80665 m·s−2. The pound-force provides an alternative unit of mass: one slug is the mass that will accelerate by one foot per second squared when acted on by one pound-force. An alternative unit of force in a different foot-pound-second system, the absolute fps system, is the poundal, defined as the force required to accelerate a one-pound mass at a rate of one foot per second squared. The units of slug and poundal are designed to avoid a constant of proportionality in Newton's Second Law. The pound-force has a metric counterpart, less commonly used than the newton: the kilogram-force (kgf) (sometimes kilopond), is the force exerted by standard gravity on one kilogram of mass. The kilogram-force leads to an alternate, but rarely used unit of mass: the metric slug (sometimes mug or hyl) is that mass that accelerates at 1 m·s−2 when subjected to a force of 1 kgf. The kilogram-force is not a part of the modern SI system, and is generally deprecated; however it still sees use for some purposes as expressing aircraft weight, jet thrust, bicycle spoke tension, torque wrench settings and engine output torque. Other arcane units of force include the sthène, which is equivalent to 1000 N, and the kip, which is equivalent to 1000 lbf. See also Ton-force.    Force measurement  See force gauge, spring scale, load cell    See also  Orders of magnitude (force)    Notes     References     Further reading     External links  Video lecture on Newton's three laws by Walter Lewin from MIT OpenCourseWare A Java simulation on vector addition of forces Force demonstrated as any influence on an object that changes the object's shape or motion (video)","label":"foo"},{"text":"","label":"foo"},{"text":"Some articles on Wikipedia may contain significant factual inaccuracies, i.e. information that is verifiably wrong. Articles for which much of the factual accuracy is actively disputed should have a {{Disputed}} warning place at the top, and they are listed at Category:Accuracy disputes. Also see a recent list of disputed articles, and the current list of articles that link here.    Handling content that may be inaccurateEdit  If you see an article that may be inaccurate, please do the following: Correct it yourself if you can. Add citations to reliable sources to verify the information. If the neutrality of the content is in question, please see at Wikipedia:NPOV dispute. If only a few statements seem inaccurate, see Disputed statement. If there are several dubious statements, or if a dispute arises: Begin a \"Disputed\" section on the talk page to describe the problem, to alert other editors and gain more opinions on whether the content is inaccurate and how to handle it. Paste {{Disputed}} at the beginning of the article to add a general warning. If only the accuracy of a section is disputed, add {{Disputed-section}} at the beginning of the section.    Handling factual inaccuracy warningsEdit  If you see an article with a factual accuracy warning, please do the following: Don't remove the warning simply because the material appears reasonable: please ensure that content is verifiable using reliable sources, that it is unbiased, and that it contains no original research before removing the notice. Visit the talk page to see what the issues are. Correct it yourself if you can. Add citations to reliable sources to verify the information. If you are sure that a statement is factually inaccurate then remove it, or move it to the talk page for further discussion. If you are familiar with subject matter contained in the article, but are not sure about the accuracy of a statement, then add \"{{Citation needed}}\" at the end of the statement. If you are not familiar with subject matter contained in the article, but wish to ask about the accuracy of a statement, then raise the issue on the article's talk page. Please see WP:BURDEN and WP:HANDLE for a more detailed explanation of how to handle potentially false or misleading information.    Resolving disputesEdit  Check dispute resolution for ways to resolve it. There are several noticeboards at which accuracy disputes may be listed to gain the views of other editors, particularly the content, reliable sources, no original research, neutral point-of-view, and biographies of living persons noticeboards (see {{Noticeboard links}} for a fuller list). Once you have found the correct information, edit the page to correct it, remove the warnings, and put something like the following in your edit summary:  Verified article – removed accuracy dispute    Disputed statementEdit  If a Wikipedia article links to this page, it is because someone is concerned that the article contains inaccurate statement(s). Such statements are followed by the following warnings: \"[dubious - discuss]\". The accuracy of a statement may be a cause for concern if: It contains unlikely information, without providing suitable references. It contains information which is particularly difficult to verify. It contains information of a highly specific nature which changes too frequently to be assumed accurate at any given time. It has been written (or edited) by a user who is known to write inaccurately on the topic. It contains information which is ambiguous and open to interpretation, either due to grammar, opinionated wording, so on. It contains secondary reliably sourced information that would be demonstrably false if tertiary sources were brought to bear. If you come across a statement with an accuracy warning, please do the following: Don't remove the warning simply because the material looks reasonable: please take the time to verify it properly. Visit the talk page to see what the issues are. Correct it right away if you can. Please take the time to verify it properly. Please also add to the article any sources you used to verify the information in it: see cite your sources. If you come across a statement which seems or is inaccurate, please do the following: Correct it right away if you can. Please take the time to verify it properly. Please also add to the article citations for any sources you used to verify the information in it: see cite your sources. If the neutrality of the statement is in question, please look at Wikipedia:NPOV dispute. If you cannot correct it right away: First, insert a \"Dubious\" section in the talk page to describe the problem. (Please don't mark up the article text without first describing the problem on the talk page.) Insert {{Dubious}} after the relevant sentence or paragraph (add the correct month and year to the template). (Or insert {{Dubious}} replacing 'talkpage section' with the appropriate section on the talk page, if one already exists.) If there are more than 5 dubious statements, or if a dispute arises: First, insert a \"Disputed\" section in the talk page to describe the problem. This will help focus contributions from others. Insert {{Disputed}} in the beginning of the article to add a general warning. Check dispute resolution for ways to resolve it. Once you've found the correct information, edit the page to correct it, remove the warnings, and put something like the following in your edit summary:  Verified article -- removed accuracy dispute  When you add an accuracy warning, you are invited to also help resolve other accuracy disputes or fact-check other articles listed in: Category:Accuracy disputes Category:Wikipedia articles needing factual verification    See alsoEdit  Wikipedia:Template_messages/Disputes Wikipedia:Template messages/Cleanup Wikipedia:Template messages/General Wikipedia:NPOV dispute Wikipedia:WikiProject Fact and Reference Check Category:Wikipedia articles needing factual verification Note: This page was previously a noticeboard for accuracy disputes. See the archive for previous disputes, and also the talk page archives for further disputes.","label":"foo"},{"text":"Edit on history  The following statement has been deleted: \"experimental physics had its debuts with experimentation concerning statics by medieval Muslim physicists like al-Biruni and Alhazen.\"\" Reason: 1) I have obtained and read through both these sources and neither of them make the claim of the above statement either explicitly or implicitly. All these sources do is claim that al-Biruni performed experiments in mechanics. In order to for one to claim that experimental physics had its debut with Muslim physicists like al-Biruni and Alhazen the source would have to explicitly make the claim, which none of these sources do.  2) Furthermore the claim in itself is ridiculous. Experimentation has existed in physics and all of science since man began building tools and other complex objects, how else could the pyramids, Colosseum, etc have been built without experimentation. Also Archimedes discovered the principle of the buoyancy and the lever through experimentation as provided by a translation of Archimedes work \"On Floating Bodies\", one can follow this link to get access: http://www.archive.org/stream/worksofarchimede00arch#page/260/mode/2up Are we to assume all science and physics before Muslim physicists like al-Biruni and Alhazen was just pure guess work, and that Archimedes guessed the principle of buoyancy and lever (not to mention he provided a qualitative and quantitative description of these laws) 3) The sources listed here don't even mention alhazen on the pages listed, hence that in itself is already a misrepresentation of the sources 4)This claim is a throw back remnant of jagged_85 work which has since been banned for misrepresenting sources and flat out lying in order to pursue an agenda, follow this link: http://en.wikipedia.org/wiki/Wikipedia:Requests_for_comment/Jagged_85 If one can find scholarly sources that make the above claim then these claims can be reintroduced but not until sources that explicitly make the claim are found. Belief Action    Prerequisites  The section on prerequisites needs to be changed as follows (I suggest two options, both of which could be ignored). I am willing to do the changes (including replacing the diagram) if we meet consensus, but I'm worried that my views constitute OR. Remove Ontology and any mention of analytic/synthetic. I suspect that most physicists do not need to know about these philosophical positions, i.e., they are not prerequisites, but they can inform and be used by some in physics. Retain ontology but Still remove the Analytic–synthetic distinction terminology. If no consensus to remove, then add the hyperlink.  Move the ontology box within physics (or have an additional arrow from physics to ontology). Math does not create ontology. Earth, wind, rain, and fire did not arise from math. Mass, force and quantum numbers did not arise from Math. It is physics that determines the appropriate observables - the ontological atoms of physics - and how to measure them. Note that under physics all observables (and theories) are useful (predictive) fictions, but there has not yet been a single universally applicable theory of physics that could be said to be true. Certainly, physics might be said to approach ultimate reality (ontology) but it has never fully achieved it. Indeed, one would also assume that formal philosophical ontology should have physics as its prerequisite, as demonstrated by Interpretations of quantum mechanics and of time.  Enclose they entire diagram within a box called Epistemology. This includes the scientific method and even the organization of all elements in the box. Note that formal (philosophical) epistemology is not a prerequisite field of study by physicists, but they nevertheless engage in doing it.  Dpleibovitz (talk) 15:35, 17 April 2014 (UTC) If the 'ontology' mention is retained (I'm personally inclined to think that ontology actually undergirds mathematics), then this sentence - \"Ontology is a prerequisite for physics, but not for mathematics.\" - should be changed. There are many arguments over the ontological status of mathematical objects. I propose this replacement: \"Ontology is a prerequisite for physics, but arguably not for mathematics.\" 70.138.217.107 (talk) 19:38, 24 April 2014 (UTC) I second changing the picture about ontology, which mathematics exceeds not. Duxwing (talk) 03:21, 26 December 2014 (UTC)    Get your Fundaments right here!  Text of current lede a) fails to characterize physics properly and b) expresses the counterfactual that physics is an ill-defined and amorphous activity. \"Fundamental\" does occur but in an improper and tangential way. Lycurgus (talk) 03:19, 26 March 2015 (UTC) I see only concrete statements. Can you be more specific? For example in the lede 'matter and motion' is directly from Maxwell's 1878 textbook. so ... ? --Ancheta Wis   (talk | contribs) 03:32, 26 March 2015 (UTC) A lede is about generality, summarizing the essence of the thing exposited in the article. There are no specifics to specify but rather that lack of cogent and correct generalization is the issue as is the counterfactual implying an amorphous and ill defined thing. Responding under assumption no changes since I opened this thread, also I'm not going to put time into this article. Lycurgus (talk) 22:35, 26 March 2015 (UTC)    Philosophy section -- Mathematics as analytical  The philosophy section advocates an understanding of physics as synthetic and mathematics as analytical. The status of mathematics as analytical vs. synthetic is a core point of divergence in various philosophies of mathematics. Mathematics as synthetic is argued by numerous proponents, the most prolific being Immanuel Kant, as articulated in the introduction of the Critique of Pure Reason. See: http://en.wikipedia.org/wiki/Critique_of_Pure_Reason#Synthetic_a_priori_judgments    It's not all of Science  The lede states that \"More broadly, it [Physics] is the general analysis of nature, conducted in order to understand how the universe behaves.\" This definition is so broad that it'd conflate with that of science as a whole. It claims to be sourced based on Young & Freedman 2014, p.1, whose most relevant quote says that \"You will come to see physics as a towering achievement of the human intellect in its quest to understand our world and ourselves.\" In their statement, \"its\" refers to the human intellect broadly, not physics specifically. The other source is \"Physics for Dummies\", and I challenge its reliability as a source for precise definitions. Fgnievinski (talk) 00:53, 13 July 2015 (UTC) But see \"Physics is the most fundamental and all-inclusive of the sciences, and has had a profound effect on all scientific development. In fact, physics is the present-day equivalent of what used to be called natural philosophy, from which most of our modern sciences arose.\" --Richard Feynman Feynman Lectures on Physics Volume I p.3-1. --Ancheta Wis   (talk | contribs) 02:27, 13 July 2015 (UTC) That may serve as a historical definition; modernly, physics is not all of natural science either. Fgnievinski (talk) 04:17, 13 July 2015 (UTC) If you have access to JSTOR, there is a wealth of material on the fundamental character of physics; it's not just historical, it's up-to-date and points to future, and also to speculative directions in science: Reviewed Work: How Modern Science Came into the World: Four Civilizations, One Seventeenth-Century Breakthrough by H. Floris Cohen, via JSTOR. If you can get it, there is H. Floris Cohen's 2010 How Modern Science Came Into the World. HFC (2010) p.266 finds 3 distinct transformations from Greek science, what he denotes \"realist-mathematical science, the natural philosophy of kinetic corpuscularianism, and fact-finding experimentalism\" which all fall under the realm of physics. Transformation 1 can be traced from Hellenic Alexandria (i.e. Archimedes), Transformation 2 from theories of matter, most famously Newton's world picture, Transformation 3 from Alhacen and Galileo. --Ancheta Wis   (talk | contribs) 17:49, 13 July 2015 (UTC) That still doesn't mean \"it is the general analysis of nature,\" including human nature and all. Fgnievinski (talk) 16:14, 14 July 2015 (UTC) English wikipedia uses the nature article, for the English/American view that 'nature' is natural, not artificial, not man-made. That is, the nature article stems from the English/American cultural view that no artifice can explain natural phenomena, as nature arose separately without human fiat. The DNA story, solved by collaboration of a former physicist and a naturalist has proven to be pivotal for the understanding of life, are rooted in scientific scholarship and the philosophical conviction that the gene had a physical basis. The explanations for the rise of societies and cooperative behavior, the operation of the brain, and consciousness are still works in process. These studies do not preclude physics or natural science. Indeed, to be comprehensible, these studies need to be rooted in physics (a paraphrase of Einstein), without artifice or fiat, in a chain of explanation, ultimately like that in a mathematical proof. --Ancheta Wis   (talk | contribs) 05:05, 15 July 2015 (UTC) So by extension, I guess you'd argue that in the sentence \"... conducted in order to understand how the universe behaves\", the \"universe\" also excludes humans? Without footnotes making explicit these caveats, the current version (\"More broadly, it [physics] is the general analysis of nature, conducted in order to understand how the universe behaves\") suggests that physics seeks to explain everything there is to know -- including psychology, sociology, etc., which seems well outside the scope of physics. Fgnievinski (talk) 05:55, 15 July 2015 (UTC)  ┌────────────────────────────────────────────────────────────────────────────────────────────────────┘ No, that's not the argument. Understanding must be grounded in physical reasoning. Feynman said, for example \"The American Civil War will pale into insignificance compared to Maxwell's equations.\". The universe existed before human fiat, and will exist long after human fiat. It's in the articles. --Ancheta Wis   (talk | contribs) 08:09, 15 July 2015 (UTC) And, I think, that if you are a strict materialist, which I am not but I understand Science to be (it can't really be anything else), then Physics *is* the most fundamental and all-encompassing science. Without a metaphysical component, ultimately any final-cause explanation of biology, psychology, sociology, even humanities, would have a physical root. And, as Laplace might say \"All we need do, is add up the summations\" to have a physical theory have predictive utility in these non-physics disciplines. Being a non-materialist, I don't buy totally into the concept, so I would not say that Physics (nor even \"Science\") is the Be-all and End-all in philosophy, but I think that Physics may well be the bottom of all other sciences. 70.109.187.202 (talk) 03:49, 18 July 2015 (UTC)    Physics doesn't study everything that exists  Let's get back to the sources. The current ones, Young & Freedman and Physics for Dummies, don't support the assertion of an all-encompassing physics. Then there's Feynman's, which is fine, but only mentions physics vis-à-vis sciences, thus excluding all the arts and humanities -- which of course are still concerned with phenomena contained in this natural universe, just are not the subject of physics or natural sciences or sciences as fields of study. Unless it can be properly sourced, I plan to remove from the lede the statement unsupported by the current sources (\"More broadly, it [physics] is the general analysis of nature, conducted in order to understand how the universe behaves\"). Fgnievinski (talk) 22:00, 18 July 2015 (UTC) But as 70.109.187.202 (a non-materialist) points out, a materialist POV will entail the fundamental position of physics as the base science. I recall the editor who selected Young & Freedman, etc as the citation, years ago; he wanted to use readily available resources. At the time, Feynman was not available online. What if we were to replace Young & Freedman, etc with R. P. Feynman, The Feynman Lectures on Physics, Vol.1, Chaps.1,2,&3.? That would support 70.109.187.202's statement about those who are materialist, as well. --Ancheta Wis   (talk | contribs) 00:38, 19 July 2015 (UTC) Note: I think at least we 3 recognize that currently all the arts and humanities are not founded on the same basis as the sciences. That does not mean that the this condition will be true in the future. Polymaths in particular are multidisciplinary. Jacob Bronowski Science and Human Values abstract writes that Science and art spring from the same root. Each of the National Laboratories has an artist in residence, to illustrate their awareness of this commonality. --Ancheta Wis   (talk | contribs) 02:08, 19 July 2015 (UTC) have been re-reading Feynman v.I p3-1, and point out that although the scope of physics is all space-time, by no means do all physical theories explain everything. There would have to be more work in physics and in all of science to do, for that to be true. --Ancheta Wis   (talk | contribs) 09:35, 19 July 2015 (UTC) Thanks for bringing up Feynman, always an enjoyable reading. He says: \"There is another kind of problem in the sister sciences which does not exist in physics...\" Alas, there you have it: a scope around physics. He goes on to single out History, and there's also the Arts, and the rest of Humanities and Social sciences. This article must stop stating that physics is concerned with all of reality. Feynman concludes saying: \"If our small minds, for some convenience, divide this glass of wine, this universe, into parts — physics, biology, geology, astronomy, psychology, and so on — remember that nature does not know it!\"; we should also remember the present article is not about the universe or nature, but about one particular field of study. Fgnievinski (talk) 19:42, 19 July 2015 (UTC) You're welcome. In addition Feynman states a condition for using the methods of physics to solve the problem: \"In order for physics to be useful to other sciences in a theoretical way, other than in the invention of instruments, the science in question must supply to the physicist a description of the object in a physicist’s language.\" This translation requirement has been a mechanism for cross-fertilization. It has occurred over and over again in the history of physics, propagating though the sciences. A history ranging from application to theory, from insight & terminology to instrument, back & forth, repeatedly. But the history that Feynman was referring to was 'how did this science (such as geology) get that way?'. Thus for geology, the problem is an application of thermodynamics to the specific configuration of the materials which comprise Earth. Same for biology, astronomy, etc. --Ancheta Wis   (talk | contribs) 08:20, 20 July 2015 (UTC)","label":"foo"},{"text":"Nature, in the broadest sense, is the natural, physical, or material world or universe. \"Nature\" can refer to the phenomena of the physical world, and also to life in general. The study of nature is a large part of science. Although humans are part of nature, human activity is often understood as a separate category from other natural phenomena. The word nature is derived from the Latin word natura, or \"essential qualities, innate disposition\", and in ancient times, literally meant \"birth\". Natura is a Latin translation of the Greek word physis (φύσις), which originally related to the intrinsic characteristics that plants, animals, and other features of the world develop of their own accord. The concept of nature as a whole, the physical universe, is one of several expansions of the original notion; it began with certain core applications of the word φύσις by pre-Socratic philosophers, and has steadily gained currency ever since. This usage continued during the advent of modern scientific method in the last several centuries. Within the various uses of the word today, \"nature\" often refers to geology and wildlife. Nature can refer to the general realm of living plants and animals, and in some cases to the processes associated with inanimate objects – the way that particular types of things exist and change of their own accord, such as the weather and geology of the Earth. It is often taken to mean the \"natural environment\" or wilderness–wild animals, rocks, forest, and in general those things that have not been substantially altered by human intervention, or which persist despite human intervention. For example, manufactured objects and human interaction generally are not considered part of nature, unless qualified as, for example, \"human nature\" or \"the whole of nature\". This more traditional concept of natural things which can still be found today implies a distinction between the natural and the artificial, with the artificial being understood as that which has been brought into being by a human consciousness or a human mind. Depending on the particular context, the term \"natural\" might also be distinguished from the unnatural or the supernatural.    Earth   Earth (or, \"the earth\") is the only planet known to support life, and its natural features are the subject of many fields of scientific research. Within the solar system, it is third closest to the sun; it is the largest terrestrial planet and the fifth largest overall. Its most prominent climatic features are its two large polar regions, two relatively narrow temperate zones, and a wide equatorial tropical to subtropical region. Precipitation varies widely with location, from several metres of water per year to less than a millimetre. 71 percent of the Earth's surface is covered by salt-water oceans. The remainder consists of continents and islands, with most of the inhabited land in the Northern Hemisphere. Earth has evolved through geological and biological processes that have left traces of the original conditions. The outer surface is divided into several gradually migrating tectonic plates. The interior remains active, with a thick layer of plastic mantle and an iron-filled core that generates a magnetic field. This iron core is composed of a solid inner phase, and a fluid outer phase. It is the rotation of the outer, fluid iron core that generates an electric current through dynamo action, which in turn generates a strong magnetic field. The atmospheric conditions have been significantly altered from the original conditions by the presence of life-forms, which create an ecological balance that stabilizes the surface conditions. Despite the wide regional variations in climate by latitude and other geographic factors, the long-term average global climate is quite stable during interglacial periods, and variations of a degree or two of average global temperature have historically had major effects on the ecological balance, and on the actual geography of the Earth.    Geology   Geology is the science and study of the solid and liquid matter that constitutes the Earth. The field of geology encompasses the study of the composition, structure, physical properties, dynamics, and history of Earth materials, and the processes by which they are formed, moved, and changed. The field is a major academic discipline, and is also important for mineral and hydrocarbon extraction, knowledge about and mitigation of natural hazards, some Geotechnical engineering fields, and understanding past climates and environments.   = Geological evolution = The geology of an area evolves through time as rock units are deposited and inserted and deformational processes change their shapes and locations. Rock units are first emplaced either by deposition onto the surface or intrude into the overlying rock. Deposition can occur when sediments settle onto the surface of the Earth and later lithify into sedimentary rock, or when as volcanic material such as volcanic ash or lava flows, blanket the surface. Igneous intrusions such as batholiths, laccoliths, dikes, and sills, push upwards into the overlying rock, and crystallize as they intrude. After the initial sequence of rocks has been deposited, the rock units can be deformed and/or metamorphosed. Deformation typically occurs as a result of horizontal shortening, horizontal extension, or side-to-side (strike-slip) motion. These structural regimes broadly relate to convergent boundaries, divergent boundaries, and transform boundaries, respectively, between tectonic plates.    Historical perspective   Earth is estimated to have formed 4.54 billion years ago from the solar nebula, along with the Sun and other planets. The moon formed roughly 20 million years later. Initially molten, the outer layer of the Earth cooled, resulting in the solid crust. Outgassing and volcanic activity produced the primordial atmosphere. Condensing water vapor, most or all of which came from ice delivered by comets, produced the oceans and other water sources. The highly energetic chemistry is believed to have produced a self-replicating molecule around 4 billion years ago. Continents formed, then broke up and reformed as the surface of Earth reshaped over hundreds of millions of years, occasionally combining to make a supercontinent. Roughly 750 million years ago, the earliest known supercontinent Rodinia, began to break apart. The continents later recombined to form Pannotia which broke apart about 540 million years ago, then finally Pangaea, which broke apart about 180 million years ago. During the Neoproterozoic era covered much of the Earth in glaciers and ice sheets. This hypothesis has been termed the \"Snowball Earth\", and it is of particular interest as it precedes the Cambrian explosion in which multicellular life forms began to proliferate about 530–540 million years ago. Since the Cambrian explosion there have been five distinctly identifiable mass extinctions. The last mass extinction occurred some 66 million years ago, when a meteorite collision probably triggered the extinction of the non-avian dinosaurs and other large reptiles, but spared small animals such as mammals. Over the past 66 million years, mammalian life diversified. Several million years ago, a species of small African ape gained the ability to stand upright. The subsequent advent of human life, and the development of agriculture and further civilization allowed humans to affect the Earth more rapidly than any previous life form, affecting both the nature and quantity of other organisms as well as global climate. By comparison, the Great Oxygenation Event, produced by the proliferation of algae during the Siderian period, required about 300 million years to culminate. The present era is classified as part of a mass extinction event, the Holocene extinction event, the fastest ever to have occurred. Some, such as E. O. Wilson of Harvard University, predict that human destruction of the biosphere could cause the extinction of one-half of all species in the next 100 years. The extent of the current extinction event is still being researched, debated and calculated by biologists.    Atmosphere, climate, and weather   The Earth's atmosphere is a key factor in sustaining the ecosystem. The thin layer of gases that envelops the Earth is held in place by gravity. Air is mostly nitrogen, oxygen, water vapor, with much smaller amounts of carbon dioxide, argon, etc. The atmospheric pressure declines steadily with altitude. The ozone layer plays an important role in depleting the amount of ultraviolet (UV) radiation that reaches the surface. As DNA is readily damaged by UV light, this serves to protect life at the surface. The atmosphere also retains heat during the night, thereby reducing the daily temperature extremes. Terrestrial weather occurs almost exclusively in the lower part of the atmosphere, and serves as a convective system for redistributing heat. Ocean currents are another important factor in determining climate, particularly the major underwater thermohaline circulation which distributes heat energy from the equatorial oceans to the polar regions. These currents help to moderate the differences in temperature between winter and summer in the temperate zones. Also, without the redistributions of heat energy by the ocean currents and atmosphere, the tropics would be much hotter, and the polar regions much colder. Weather can have both beneficial and harmful effects. Extremes in weather, such as tornadoes or hurricanes and cyclones, can expend large amounts of energy along their paths, and produce devastation. Surface vegetation has evolved a dependence on the seasonal variation of the weather, and sudden changes lasting only a few years can have a dramatic effect, both on the vegetation and on the animals which depend on its growth for their food. Climate is a measure of the long-term trends in the weather. Various factors are known to influence the climate, including ocean currents, surface albedo, greenhouse gases, variations in the solar luminosity, and changes to the Earth's orbit. Based on historical records, the Earth is known to have undergone drastic climate changes in the past, including ice ages. The climate of a region depends on a number of factors, especially latitude. A latitudinal band of the surface with similar climatic attributes forms a climate region. There are a number of such regions, ranging from the tropical climate at the equator to the polar climate in the northern and southern extremes. Weather is also influenced by the seasons, which result from the Earth's axis being tilted relative to its orbital plane. Thus, at any given time during the summer or winter, one part of the Earth is more directly exposed to the rays of the sun. This exposure alternates as the Earth revolves in its orbit. At any given time, regardless of season, the northern and southern hemispheres experience opposite seasons. Weather is a chaotic system that is readily modified by small changes to the environment, so accurate weather forecasting is limited to only a few days. Overall, two things are happening worldwide: (1) temperature is increasing on the average; and (2) regional climates have been undergoing noticeable changes.    Water on Earth   Water is a chemical substance that is composed of hydrogen and oxygen and is vital for all known forms of life. In typical usage, water refers only to its liquid form or state, but the substance also has a solid state, ice, and a gaseous state, water vapor or steam. Water covers 71% of the Earth's surface. On Earth, it is found mostly in oceans and other large water bodies, with 1.6% of water below ground in aquifers and 0.001% in the air as vapor, clouds, and precipitation. Oceans hold 97% of surface water, glaciers and polar ice caps 2.4%, and other land surface water such as rivers, lakes and ponds 0.6%. Additionally, a minute amount of the Earth's water is contained within biological bodies and manufactured products.    Oceans   An ocean is a major body of saline water, and a principal component of the hydrosphere. Approximately 71% of the Earth's surface (an area of some 361 million square kilometers) is covered by ocean, a continuous body of water that is customarily divided into several principal oceans and smaller seas. More than half of this area is over 3,000 meters (9,800 feet) deep. Average oceanic salinity is around 35 parts per thousand (ppt) (3.5%), and nearly all seawater has a salinity in the range of 30 to 38 ppt. Though generally recognized as several 'separate' oceans, these waters comprise one global, interconnected body of salt water often referred to as the World Ocean or global ocean. This concept of a global ocean as a continuous body of water with relatively free interchange among its parts is of fundamental importance to oceanography. The major oceanic divisions are defined in part by the continents, various archipelagos, and other criteria: these divisions are (in descending order of size) the Pacific Ocean, the Atlantic Ocean, the Indian Ocean, the Southern Ocean and the Arctic Ocean. Smaller regions of the oceans are called seas, gulfs, bays and other names. There are also salt lakes, which are smaller bodies of landlocked saltwater that are not interconnected with the World Ocean. Two notable examples of salt lakes are the Aral Sea and the Great Salt Lake.    Lakes   A lake (from Latin lacus) is a terrain feature (or physical feature), a body of liquid on the surface of a world that is localized to the bottom of basin (another type of landform or terrain feature; that is, it is not global) and moves slowly if it moves at all. On Earth, a body of water is considered a lake when it is inland, not part of the ocean, is larger and deeper than a pond, and is fed by a river. The only world other than Earth known to harbor lakes is Titan, Saturn's largest moon, which has lakes of ethane, most likely mixed with methane. It is not known if Titan's lakes are fed by rivers, though Titan's surface is carved by numerous river beds. Natural lakes on Earth are generally found in mountainous areas, rift zones, and areas with ongoing or recent glaciation. Other lakes are found in endorheic basins or along the courses of mature rivers. In some parts of the world, there are many lakes because of chaotic drainage patterns left over from the last Ice Age. All lakes are temporary over geologic time scales, as they will slowly fill in with sediments or spill out of the basin containing them.   = Ponds =  A pond is a body of standing water, either natural or man-made, that is usually smaller than a lake. A wide variety of man-made bodies of water are classified as ponds, including water gardens designed for aesthetic ornamentation, fish ponds designed for commercial fish breeding, and solar ponds designed to store thermal energy. Ponds and lakes are distinguished from streams via current speed. While currents in streams are easily observed, ponds and lakes possess thermally driven microcurrents and moderate wind driven currents. These features distinguish a pond from many other aquatic terrain features, such as stream pools and tide pools.    Rivers   A river is a natural watercourse, usually freshwater, flowing toward an ocean, a lake, a sea or another river. In a few cases, a river simply flows into the ground or dries up completely before reaching another body of water. Small rivers may also be called by several other names, including stream, creek, brook, rivulet, and rill; there is no general rule that defines what can be called a river. Many names for small rivers are specific to geographic location; one example is Burn in Scotland and North-east England. Sometimes a river is said to be larger than a creek, but this is not always the case, due to vagueness in the language. A river is part of the hydrological cycle. Water within a river is generally collected from precipitation through surface runoff, groundwater recharge, springs, and the release of stored water in natural ice and snowpacks (i.e., from glaciers).    Streams   A stream is a flowing body of water with a current, confined within a bed and stream banks. In the United States a stream is classified as a watercourse less than 60 feet (18 metres) wide. Streams are important as conduits in the water cycle, instruments in groundwater recharge, and they serve as corridors for fish and wildlife migration. The biological habitat in the immediate vicinity of a stream is called a riparian zone. Given the status of the ongoing Holocene extinction, streams play an important corridor role in connecting fragmented habitats and thus in conserving biodiversity. The study of streams and waterways in general involves many branches of inter-disciplinary natural science and engineering, including hydrology, fluvial geomorphology, aquatic ecology, fish biology, riparian ecology and others.    Ecosystems   Ecosystems are composed of a variety of abiotic and biotic components that function in an interrelated way. The structure and composition is determined by various environmental factors that are interrelated. Variations of these factors will initiate dynamic modifications to the ecosystem. Some of the more important components are: soil, atmosphere, radiation from the sun, water, and living organisms.  Central to the ecosystem concept is the idea that living organisms interact with every other element in their local environment. Eugene Odum, a founder of ecology, stated: \"Any unit that includes all of the organisms (ie: the \"community\") in a given area interacting with the physical environment so that a flow of energy leads to clearly defined trophic structure, biotic diversity, and material cycles (i.e.: exchange of materials between living and nonliving parts) within the system is an ecosystem.\" Within the ecosystem, species are connected and dependent upon one another in the food chain, and exchange energy and matter between themselves as well as with their environment. The human ecosystem concept is grounded in the deconstruction of the human/nature dichotomy and the premise that all species are ecologically integrated with each other, as well as with the abiotic constituents of their biotope. A smaller unit of size is called a microecosystem. For example, a microsystem can be a stone and all the life under it. A macroecosystem might involve a whole ecoregion, with its drainage basin.    Wilderness   Wilderness is generally defined as areas that have not been significantly modified by human activity. Wilderness areas can be found in preserves, estates, farms, conservation preserves, ranches, national forests, national parks and even in urban areas along rivers, gulches or otherwise undeveloped areas. Wilderness areas and protected parks are considered important for the survival of certain species, ecological studies, conservation, solitude, and recreation. Some nature writers believe wilderness areas are vital for the human spirit and creativity, and some Ecologists consider wilderness areas to be an integral part of the Earth's self-sustaining natural ecosystem (the biosphere). They may also preserve historic genetic traits and that they provide habitat for wild flora and fauna that may be difficult to recreate in zoos, arboretums or laboratories.    Life   Although there is no universal agreement on the definition of life, scientists generally accept that the biological manifestation of life is characterized by organization, metabolism, growth, adaptation, response to stimuli and reproduction. Life may also be said to be simply the characteristic state of organisms. Properties common to terrestrial organisms (plants, animals, fungi, protists, archaea and bacteria) are that they are cellular, carbon-and-water-based with complex organization, having a metabolism, a capacity to grow, respond to stimuli, and reproduce. An entity with these properties is generally considered life. However, not every definition of life considers all of these properties to be essential. Human-made analogs of life may also be considered to be life. The biosphere is the part of Earth's outer shell – including land, surface rocks, water, air and the atmosphere – within which life occurs, and which biotic processes in turn alter or transform. From the broadest geophysiological point of view, the biosphere is the global ecological system integrating all living beings and their relationships, including their interaction with the elements of the lithosphere (rocks), hydrosphere (water), and atmosphere (air). The entire Earth contains over 75 billion tons (150 trillion pounds or about 6.8×1013 kilograms) of biomass (life), which lives within various environments within the biosphere. Over nine-tenths of the total biomass on Earth is plant life, on which animal life depends very heavily for its existence. More than 2 million species of plant and animal life have been identified to date, and estimates of the actual number of existing species range from several million to well over 50 million. The number of individual species of life is constantly in some degree of flux, with new species appearing and others ceasing to exist on a continual basis. The total number of species is in rapid decline.    Evolution   The origin of life on Earth is not well understood, but it is known to have occurred at least 3.5 billion years ago, during the hadean or archean eons on a primordial Earth that had a substantially different environment than is found at present. These life forms possessed the basic traits of self-replication and inheritable traits. Once life had appeared, the process of evolution by natural selection resulted in the development of ever-more diverse life forms. Species that were unable to adapt to the changing environment and competition from other life forms became extinct. However, the fossil record retains evidence of many of these older species. Current fossil and DNA evidence shows that all existing species can trace a continual ancestry back to the first primitive life forms. The advent of photosynthesis in very basic forms of plant life worldwide allowed the sun's energy to be harvested to create conditions allowing for more complex life. The resultant oxygen accumulated in the atmosphere and gave rise to the ozone layer. The incorporation of smaller cells within larger ones resulted in the development of yet more complex cells called eukaryotes. Cells within colonies became increasingly specialized, resulting in true multicellular organisms. With the ozone layer absorbing harmful ultraviolet radiation, life colonized the surface of Earth.    Microbes   The first form of life to develop on the Earth were microbes, and they remained the only form of life until about a billion years ago when multi-cellular organisms began to appear. Microorganisms are single-celled organisms that are generally microscopic, and smaller than the human eye can see. They include Bacteria, Fungi, Archaea and Protista. These life forms are found in almost every location on the Earth where there is liquid water, including in the Earth's interior. Their reproduction is both rapid and profuse. The combination of a high mutation rate and a horizontal gene transfer ability makes them highly adaptable, and able to survive in new environments, including outer space. They form an essential part of the planetary ecosystem. However, some microorganisms are pathogenic and can post health risk to other organisms.    Plants and Animals   Originally Aristotle divided all living things between plants, which generally do not move fast enough for humans to notice, and animals. In Linnaeus' system, these became the kingdoms Vegetabilia (later Plantae) and Animalia. Since then, it has become clear that the Plantae as originally defined included several unrelated groups, and the fungi and several groups of algae were removed to new kingdoms. However, these are still often considered plants in many contexts. Bacterial life is sometimes included in flora, and some classifications use the term bacterial flora separately from plant flora. Among the many ways of classifying plants are by regional floras, which, depending on the purpose of study, can also include fossil flora, remnants of plant life from a previous era. People in many regions and countries take great pride in their individual arrays of characteristic flora, which can vary widely across the globe due to differences in climate and terrain. Regional floras commonly are divided into categories such as native flora and agricultural and garden flora, the lastly mentioned of which are intentionally grown and cultivated. Some types of \"native flora\" actually have been introduced centuries ago by people migrating from one region or continent to another, and become an integral part of the native, or natural flora of the place to which they were introduced. This is an example of how human interaction with nature can blur the boundary of what is considered nature. Another category of plant has historically been carved out for weeds. Though the term has fallen into disfavor among botanists as a formal way to categorize \"useless\" plants, the informal use of the word \"weeds\" to describe those plants that are deemed worthy of elimination is illustrative of the general tendency of people and societies to seek to alter or shape the course of nature. Similarly, animals are often categorized in ways such as domestic, farm animals, wild animals, pests, etc. according to their relationship to human life. Animals as a category have several characteristics that generally set them apart from other living things. Animals are eukaryotic and usually multicellular (although see Myxozoa), which separates them from bacteria, archaea and most protists. They are heterotrophic, generally digesting food in an internal chamber, which separates them from plants and algae. They are also distinguished from plants, algae, and fungi by lacking cell walls. With a few exceptions, most notably the sponges (Phylum Porifera), animals have bodies differentiated into separate tissues. These include muscles, which are able to contract and control locomotion, and a nervous system, which sends and processes signals. There is also typically an internal digestive chamber. The eukaryotic cells possessed by all animals are surrounded by a characteristic extracellular matrix composed of collagen and elastic glycoproteins. This may be calcified to form structures like shells, bones, and spicules, a framework upon which cells can move about and be reorganized during development and maturation, and which supports the complex anatomy required for mobility.    Human interrelationship   Although humans comprise only a minuscule proportion of the total living biomass on Earth, the human effect on nature is disproportionately large. Because of the extent of human influence, the boundaries between what humans regard as nature and \"made environments\" is not clear cut except at the extremes. Even at the extremes, the amount of natural environment that is free of discernible human influence is diminishing at an increasingly rapid pace. The development of technology by the human race has allowed the greater exploitation of natural resources and has helped to alleviate some of the risk from natural hazards. In spite of this progress, however, the fate of human civilization remains closely linked to changes in the environment. There exists a highly complex feedback loop between the use of advanced technology and changes to the environment that are only slowly becoming understood. Man-made threats to the Earth's natural environment include pollution, deforestation, and disasters such as oil spills. Humans have contributed to the extinction of many plants and animals. Humans employ nature for both leisure and economic activities. The acquisition of natural resources for industrial use remains the primary component of the world's economic system. Some activities, such as hunting and fishing, are used for both sustenance and leisure, often by different people. Agriculture was first adopted around the 9th millennium BCE. Ranging from food production to energy, nature influences economic wealth. Although early humans gathered uncultivated plant materials for food and employed the medicinal properties of vegetation for healing, most modern human use of plants is through agriculture. The clearance of large tracts of land for crop growth has led to a significant reduction in the amount available of forestation and wetlands, resulting in the loss of habitat for many plant and animal species as well as increased erosion.    Aesthetics and beauty   Beauty in nature has historically been a prevalent theme in art and books, filling large sections of libraries and bookstores. That nature has been depicted and celebrated by so much art, photography, poetry and other literature shows the strength with which many people associate nature and beauty. Reasons why this association exists, and what the association consists of, are studied by the branch of philosophy called aesthetics. Beyond certain basic characteristics that many philosophers agree about to explain what is seen as beautiful, the opinions are virtually endless. Nature and wildness have been important subjects in various eras of world history. An early tradition of landscape art began in China during the Tang Dynasty (618–907). The tradition of representing nature as it is became one of the aims of Chinese painting and was a significant influence in Asian art. Although natural wonders are celebrated in the Psalms and the Book of Job, wilderness portrayals in art became more prevalent in the 1800s, especially in the works of the Romantic movement. British artists John Constable and J. M. W. Turner turned their attention to capturing the beauty of the natural world in their paintings. Before that, paintings had been primarily of religious scenes or of human beings. William Wordsworth's poetry described the wonder of the natural world, which had formerly been viewed as a threatening place. Increasingly the valuing of nature became an aspect of Western culture. This artistic movement also coincided with the Transcendentalist movement in the Western world. A common classical idea of beautiful art involves the word mimesis, the imitation of nature. Also in the realm of ideas about beauty in nature is that the perfect is implied through perfect mathematical forms and more generally by patterns in nature. As David Rothenburg writes, \"The beautiful is the root of science and the goal of art, the highest possibility that humanity can ever hope to see\".    Value of Nature  In her 1988 book If Women Counted, Marilyn Waring points to how the value of Nature is not measured in human economic activity, leading to destruction of Nature.    Matter and energy   Some fields of science see nature as matter in motion, obeying certain laws of nature which science seeks to understand. For this reason the most fundamental science is generally understood to be \"physics\" – the name for which is still recognizable as meaning that it is the study of nature. Matter is commonly defined as the substance of which physical objects are composed. It constitutes the observable universe. The visible components of the universe are now believed to compose only 4.9 percent of the total mass. The remainder is believed to consist of 26.8 percent cold dark matter and 68.3 percent dark energy. The exact nature of these components is still unknown and is under intensive investigation by physicists. The behavior of matter and energy throughout the observable universe appears to follow well-defined physical laws. These laws have been employed to produce cosmological models that successfully explain the structure and the evolution of the universe we can observe. The mathematical expressions of the laws of physics employ a set of twenty physical constants that appear to be static across the observable universe. The values of these constants have been carefully measured, but the reason for their specific values remains a mystery.    Beyond Earth   Outer space, also simply called space, refers to the relatively empty regions of the universe outside the atmospheres of celestial bodies. Outer space is used to distinguish it from airspace (and terrestrial locations). There is no discrete boundary between the Earth's atmosphere and space, as the atmosphere gradually attenuates with increasing altitude. Outer space within the Solar System is called interplanetary space, which passes over into interstellar space at what is known as the heliopause. Outer space is sparsely filled with several dozen types of organic molecules discovered to date by microwave spectroscopy, blackbody radiation left over from the big bang and the origin of the universe, and cosmic rays, which include ionized atomic nuclei and various subatomic particles. There is also some gas, plasma and dust, and small meteors. Additionally, there are signs of human life in outer space today, such as material left over from previous manned and unmanned launches which are a potential hazard to spacecraft. Some of this debris re-enters the atmosphere periodically. Although the Earth is the only body within the solar system known to support life, evidence suggests that in the distant past the planet Mars possessed bodies of liquid water on the surface. For a brief period in Mars' history, it may have also been capable of forming life. At present though, most of the water remaining on Mars is frozen. If life exists at all on Mars, it is most likely to be located underground where liquid water can still exist. Conditions on the other terrestrial planets, Mercury and Venus, appear to be too harsh to support life as we know it. But it has been conjectured that Europa, the fourth-largest moon of Jupiter, may possess a sub-surface ocean of liquid water and could potentially host life. Astronomers have started to discover extrasolar Earth analogs – planets that lie in the habitable zone of space surrounding a star, and therefore could possibly host life as we know it.    See also   Media: Natural History, by Pliny the Elder Nature, by Ralph Waldo Emerson Nature, a prominent scientific journal National Wildlife (magazine), publication of the National Wildlife Federation Nature (TV series) Natural World (TV series) Organizations: The Nature Conservancy Nature Detectives Science: Natural history Natural landscape Philosophy: Mother Nature Nature (philosophy) Naturalism (philosophy): any of several philosophical stances, typically those descended from Materialism and Pragmatism that do not distinguish the supernatural from nature. This includes the methodological naturalism of natural science, which makes the methodological assumption that observable events in nature are explained only by natural causes, without assuming either the existence or non-existence of the supernatural. Balance of nature (biological fallacy): A discredited concept of natural equilibrium in predator: prey dynamics.    Notes and references     External links  The IUCN Red List of Threatened Species (iucnredlist.org) The Wild Foundation – The heart of the global wilderness conservation movement (wild.org)* Fauna & Flora International is taking decisive action to help save the world’s wild species and spaces (fauna-flora.org) European Wildlife is a Pan-European non-profit organization dedicated to nature preservation and environmental protection (eurowildlife.org) Nature Journal (nature.com) The National Geographic Society (nationalgeographic.com) Record of life on Earth (arkive.org) BBC – Science and Nature (bbc.co.uk) PBS – Science and Nature (pbs.org) Science Daily (sciencedaily.com) European Commission – Nature and Biodiversity (ec.europa.eu) Natural History Museum (.nhm.ac.uk) Encyclopedia of Life (eol.org). Science.gov – Environment & Environmental Quality.","label":"foo"},{"text":"The Universe is all of time and space and its contents. The Universe includes planets, stars, galaxies, the contents of intergalactic space, the smallest subatomic particles, and all matter and energy. The observable universe is about 28 billion parsecs (91 billion light-years) in diameter at the present time. The size of the whole Universe is not known and may be infinite. Observations and the development of physical theories have led to inferences about the composition and evolution of the Universe. Throughout recorded history, cosmologies and cosmogonies, including scientific models, have been proposed to explain observations of the Universe. The earliest quantitative geocentric models were developed by ancient Greek philosophers and Indian philosophers. Over the centuries, more precise astronomical observations led to Nicolaus Copernicus's heliocentric model of the Solar System and Johannes Kepler's improvement on that model with elliptical orbits, which was eventually explained by Isaac Newton's theory of gravity. Further observational improvements led to the realization that the Solar System is located in a galaxy composed of billions of stars, the Milky Way. It was subsequently discovered that our galaxy is one of many. On the largest scales, it is assumed that the distribution of galaxies is uniform and the same in all directions, meaning that the Universe has neither an edge nor a center. Observations of the distribution of these galaxies and their spectral lines have led to many of the theories of modern physical cosmology. The discovery in the early 20th century that galaxies are systematically redshifted suggested that the Universe is expanding, and the discovery of the cosmic microwave background radiation suggested that the Universe had a beginning. Finally, observations in the late 1990s indicated the rate of the expansion of the Universe is increasing indicating that the majority of energy is most likely in an unknown form called dark energy. The majority of mass in the universe also appears to exist in an unknown form, called dark matter. The Big Bang theory is the prevailing cosmological model describing the development of the Universe. Space and time were created in the Big Bang, and these were imbued with a fixed amount of energy and matter; as space expands, the density of that matter and energy decreases. After the initial expansion, the Universe cooled sufficiently to allow the formation first of subatomic particles and later of simple atoms. Giant clouds of these primordial elements later coalesced through gravity to form stars. Assuming that the prevailing model is correct, the age of the Universe is measured to be 7001137990000000000♠13.799±0.021 billion years. There are many competing hypotheses about the ultimate fate of the Universe. Physicists and philosophers remain unsure about what, if anything, preceded the Big Bang. Many refuse to speculate, doubting that any information from any such prior state could ever be accessible. There are various multiverse hypotheses, in which some physicists have suggested that the Universe might be one among many universes that likewise exist.    DefinitionEdit  The Universe is customarily defined as everything that exists, everything that has existed, and everything that will exist. According to our current understanding, the Universe consists of three constituents: spacetime, forms of energy (including electromagnetic radiation and matter), and the physical laws that relate them. The Universe also encompasses all of life, all of history, and some philosophers and scientists even suggest that it encompasses ideas such as mathematics.    EtymologyEdit  The word universe derives from the Old French word univers, which in turn derives from the Latin word universum. The Latin word was used by Cicero and later Latin authors in many of the same senses as the modern English word is used.    SynonymsEdit  A term for \"universe\" among the ancient Greek philosophers from Pythagoras onwards was τὸ πᾶν tò pân (\"the all\"), defined as all matter and all space, and τὸ ὅλον tò hólon (\"all things\"), which did not necessarily include the void. Another synonym was ὁ κόσμος ho kósmos (meaning the world, the cosmos). Synonyms are also found in Latin authors (totum, mundus, natura) and survive in modern languages, e.g., the German words Das All, Weltall, and Natur for Universe. The same synonyms are found in English, such as everything (as in the theory of everything), the cosmos (as in cosmology), the world (as in the many-worlds interpretation), and nature (as in natural laws or natural philosophy).    Chronology and the Big BangEdit   The prevailing model for the evolution of the Universe is the Big Bang theory. The Big Bang model states that the earliest state of the Universe was extremely hot and dense and that it subsequently expanded. The model is based on general relativity and on simplifying assumptions such as homogeneity and isotropy of space. A version of the model with a cosmological constant (Lambda) and cold dark matter, known as the Lambda-CDM model, is the simplest model that provides a reasonably good account of various observations about the Universe. The Big Bang model accounts for observations such as the correlation of distance and redshift of galaxies, the ratio of the number of hydrogen to helium atoms, and the microwave radiation background. The initial hot, dense state is called the Planck epoch, a brief period extending from time zero to one Planck time unit of approximately 10−43 seconds. During the Planck epoch, all types of matter and all types of energy were concentrated into a dense state, where gravitation is believed to have been as strong as the other fundamental forces, and all the forces may have been unified. Since the Planck epoch, the Universe has been expanding to its present form, possibly with a very brief period of cosmic inflation which caused the Universe to reach a much larger size in less than 10−32 seconds. After the Planck epoch and inflation came the quark, hadron, and lepton epochs. Together, these epochs encompassed less than 10 seconds of time following the Big Bang. The observed abundance of the elements can be explained by combining the overall expansion of space with nuclear and atomic physics. As the Universe expands, the energy density of electromagnetic radiation decreases more quickly than does that of matter because the energy of a photon decreases with its wavelength. As the Universe expanded and cooled, elementary particles associated stably into ever larger combinations. Thus, in the early part of the matter-dominated era, stable protons and neutrons formed, which then formed atomic nuclei through nuclear reactions. This process, known as Big Bang nucleosynthesis, led to the present abundances of lighter nuclei, particularly hydrogen, deuterium, and helium. Big Bang nucleosynthesis ended about 20 minutes after the Big Bang, when the Universe had cooled enough so that nuclear fusion could no longer occur. At this stage, matter in the Universe was mainly a hot, dense plasma of negatively charged electrons, neutral neutrinos and positive nuclei. This era, called the photon epoch, lasted about 380 thousand years. Eventually, at a time known as recombination, electrons and nuclei formed stable atoms, which are transparent to most wavelengths of radiation. With photons decoupled from matter, the Universe entered the matter-dominated era. Light from this era could now travel freely, and it can still be seen in the Universe as the cosmic microwave background (CMB). After around 100 million years, the first stars formed; these were likely very massive, luminous, and responsible for the reionization of the Universe. Having no elements heavier than lithium, these stars also produced the first heavy elements through stellar nucleosynthesis. The Universe also contains a mysterious energy called dark energy; the energy density of dark energy does not change over time. After about 9.8 billion years, the Universe had expanded sufficiently so that the density of matter was less than the density of dark energy, marking the beginning of the present dark-energy-dominated era. In this era, the expansion of the Universe is accelerating due to dark energy.    PropertiesEdit   The spacetime of the Universe is usually interpreted from a Euclidean perspective, with space as consisting of three dimensions, and time as consisting of one dimension, the \"fourth dimension\". By combining space and time into a single manifold called Minkowski space, physicists have simplified a large number of physical theories, as well as described in a more uniform way the workings of the Universe at both the supergalactic and subatomic levels. Spacetime events are not absolutely defined spatially and temporally but rather are known relative to the motion of an observer. Minkowski space approximates the Universe without gravity; the pseudo-Riemannian manifolds of general relativity describe spacetime with matter and gravity. String theory postulates the existence of additional dimensions. Of the four fundamental interactions, gravitation is dominant at cosmological length scales, including galaxies and larger-scale structures. Gravity's effects are cumulative; by contrast, the effects of positive and negative charges tend to cancel one another, making electromagnetism relatively insignificant on cosmological length scales. The remaining two interactions, the weak and strong nuclear forces, decline very rapidly with distance; their effects are confined mainly to sub-atomic length scales. The Universe appears to have much more matter than antimatter, an asymmetry possibly related to the observations of CP violation. The Universe also appears to have neither net momentum nor angular momentum. The absence of net charge and momentum would follow from accepted physical laws (Gauss's law and the non-divergence of the stress-energy-momentum pseudotensor, respectively) if the Universe were finite.    ShapeEdit   General relativity describes how spacetime is curved and bent by mass and energy. The topology or geometry of the Universe includes both local geometry in the observable universe and global geometry. Cosmologists often work with a given space-like slice of spacetime called the comoving coordinates. The section of spacetime which can be observed is the backward light cone, which delimits the cosmological horizon. The cosmological horizon (also called the particle horizon or the light horizon) is the maximum distance from which particles can have traveled to the observer in the age of the Universe. This horizon represents the boundary between the observable and the unobservable regions of the Universe. The existence, properties, and significance of a cosmological horizon depend on the particular cosmological model. An important parameter determining the future evolution of the Universe theory is the density parameter, Omega (Ω), defined as the average matter density of the universe divided by a critical value of that density. This selects one of three possible geometries depending on whether Ω is equal to, less than, or greater than 1. These are called, respectively, the flat, open and closed universes. Observations, including the Cosmic Background Explorer (COBE), Wilkinson Microwave Anisotropy Probe (WMAP), and Planck maps of the CMB, suggest that the Universe is infinite in extent with a finite age, as described by the Friedmann–Lemaître–Robertson–Walker (FLRW) models. These FLRW models thus support inflationary models and the standard model of cosmology, describing a flat, homogeneous universe presently dominated by dark matter and dark energy.    Size and regionsEdit   The size of the Universe is somewhat difficult to define. According to a restrictive definition, the Universe is everything within our connected spacetime that could have a chance to interact with us and vice versa. According to the general theory of relativity, some regions of space may never interact with ours even in the lifetime of the Universe due to the finite speed of light and the ongoing expansion of space. For example, radio messages sent from Earth may never reach some regions of space, even if the Universe were to exist forever: space may expand faster than light can traverse it. Distant regions of space are assumed to exist and be part of reality as much as we are even though we can never interact with them. The spatial region that we can affect and be affected by is the observable universe. The observable universe depends on the location of the observer. By traveling, an observer can come into contact with a greater region of spacetime than an observer who remains still. Nevertheless, even the most rapid traveler will not be able to interact with all of space. Typically, the observable universe is taken to mean the portion of the Universe that is observable from our vantage point in the Milky Way. The proper distance—the distance as would be measured at a specific time, including the present—between Earth and the edge of the observable universe is 46 billion light-years (14 billion parsecs), making the diameter of the observable universe about 91 billion light-years (28×10^9 pc). The distance the light from the edge of the observable universe has travelled is very close to the age of the Universe times the speed of light, 13.8 billion light-years (4.2×10^9 pc), but this does not represent the distance at any given time because the edge of the Universe and the Earth have moved since further apart. For comparison, the diameter of a typical galaxy is 30,000 light-years, and the typical distance between two neighboring galaxies is 3 million light-years. As an example, the Milky Way is roughly 100,000 light years in diameter, and the nearest sister galaxy to the Milky Way, the Andromeda Galaxy, is located roughly 2.5 million light years away. Because we cannot observe space beyond the edge of the observable universe, it is unknown whether the size of the Universe is finite or infinite.    Age and expansionEdit   Astronomers calculate the age of the Universe by assuming that the Lambda-CDM model accurately describes the evolution of the Universe from a very uniform, hot, dense primordial state to its present state and measuring the cosmological parameters which constitute the model. This model is well understood theoretically and supported by recent high-precision astronomical observations such as WMAP and Planck. Commonly, the set of observations fitted includes the cosmic microwave background anisotropy, the brightness/redshift relation for Type Ia supernovae, and large-scale galaxy clustering including the baryon acoustic oscillation feature. Other observations, such as the Hubble constant, the abundance of galaxy clusters, weak gravitational lensing and globular cluster ages, are generally consistent with these, providing a check of the model, but are less accurately measured at present. With the prior that the Lambda-CDM model is correct, the measurements of the parameters using a variety of techniques by numerous experiments yield a best value of the age of the Universe as of 2015 of 13.799 ± 0.021 billion years. Over time, the Universe and its contents have evolved; for example, the relative population of quasars and galaxies has changed and space itself has expanded. Due to this expansion, scientists on Earth can observe the light from a galaxy 30 billion light years away even though that light has traveled for only 13 billion years; the very space between them has expanded. This expansion is consistent with the observation that the light from distant galaxies has been redshifted; the photons emitted have been stretched to longer wavelengths and lower frequency during their journey. Analyses of Type Ia supernovae indicate that the spatial expansion is accelerating. The more matter there is in the Universe, the stronger the mutual gravitational pull of the matter. If the Universe were too dense then it would re-collapse into a gravitational singularity. However, if the Universe contained too little matter then the expansion would accelerate too rapidly for planets and planetary systems to form. Since the Big Bang, the universe has expanded monotonically. Surprisingly, our universe has just the right mass density of about 5 protons per cubic meter which has allowed it to expand for the last 13.8 billion years, giving time to form the universe as observed today. There are dynamical forces acting on the particles in the Universe which affect the expansion rate. Before 1998, it was expected that the rate of increase of the Hubble Constant would be decreasing as time went on due to the influence of gravitational interactions in the Universe, and thus there is an additional observable quantity in the Universe called the deceleration parameter which cosmologists expected to be directly related to the matter density of the Universe. In 1998, the deceleration parameter was measured by two different groups to be consistent with −1 but not zero, which implied that the present-day rate of increase of the Hubble Constant is increasing over time.    SpacetimeEdit   Spacetimes are the arenas in which all physical events take place—an event is a point in spacetime specified by its time and place. The basic elements of spacetime are events. In any given spacetime, an event is a unique position at a unique time. Because events are spacetime points, an example of an event in classical relativistic physics is , the location of an elementary (point-like) particle at a particular time. A spacetime is the union of all events in the same way that a line is the union of all of its points, formally organized into a manifold. The Universe appears to be a smooth spacetime continuum consisting of three spatial dimensions and one temporal (time) dimension. On the average, space is observed to be very nearly flat (close to zero curvature), meaning that Euclidean geometry is empirically true with high accuracy throughout most of the Universe. Spacetime also appears to have a simply connected topology, in analogy with a sphere, at least on the length-scale of the observable Universe. However, present observations cannot exclude the possibilities that the Universe has more dimensions and that its spacetime may have a multiply connected global topology, in analogy with the cylindrical or toroidal topologies of two-dimensional spaces.    ContentsEdit   The Universe is composed almost completely of dark energy, dark matter, and ordinary matter. Other contents are electromagnetic radiation (estimated to be from 0.005% to close to 0.01%) and antimatter. The total amount of electromagnetic radiation generated within the universe has decreased by 1/2 in the past 2 billion years. Ordinary matter, which includes atoms, stars, galaxies, and life, accounts for only 4.9% of the contents of the Universe. The present overall density of this type of matter is very low, roughly 4.5 × 10−31 grams per cubic centimetre, corresponding to a density of the order of only one proton for every four cubic meters of volume. The nature of both dark energy and dark matter is unknown. Dark matter, a mysterious form of matter that has not yet been identified, accounts for 26.8% of the contents. Dark energy, which is the energy of empty space and that is causing the expansion of the Universe to accelerate, accounts for the remaining 68.3% of the contents.  Matter, dark matter, and dark energy are distributed homogeneously throughout the Universe over length scales longer than 300 million light-years or so. However, over shorter length-scales, matter tends to clump hierarchically; many atoms are condensed into stars, most stars into galaxies, most galaxies into clusters, superclusters and, finally, large-scale galactic filaments. The observable Universe contains approximately 300 sextillion (3×1023) stars and more than 100 billion (1011) galaxies. Typical galaxies range from dwarfs with as few as ten million (107) stars up to giants with one trillion (1012) stars. Between the structures are voids, which are typically 10–150 Mpc (33 million–490 million ly) in diameter. The Milky Way is in the Local Group of galaxies, which in turn is in the Laniakea Supercluster. This supercluster spans over 500 million light years, while the Local Group spans over 10 million light years. The Universe also has vast regions of relative emptiness; the largest known void measures 1.8 billion ly (550 Mpc) across.  The observable Universe is isotropic on scales significantly larger than superclusters, meaning that the statistical properties of the Universe are the same in all directions as observed from Earth. The Universe is bathed in highly isotropic microwave radiation that corresponds to a thermal equilibrium blackbody spectrum of roughly 2.72548 kelvin. The hypothesis that the large-scale Universe is homogeneous and isotropic is known as the cosmological principle. A Universe that is both homogeneous and isotropic looks the same from all vantage points and has no center.    Dark energyEdit   An explanation for why the expansion of the Universe is accelerating remains elusive. It is often attributed to \"dark energy\", an unknown form of energy that is hypothesized to permeate space. On a mass–energy equivalence basis, the density of dark energy (6.91 × 10−27 kg/m3) is much less than the density of ordinary matter or dark matter within galaxies. However, in the present dark-energy era, it dominates the mass–energy of the universe because it is uniform across space. Two proposed forms for dark energy are the cosmological constant, a constant energy density filling space homogeneously, and scalar fields such as quintessence or moduli, dynamic quantities whose energy density can vary in time and space. Contributions from scalar fields that are constant in space are usually also included in the cosmological constant. The cosmological constant can be formulated to be equivalent to vacuum energy. Scalar fields having only a slight amont of spatial inhomogeneity would be difficult to distinguish from a cosmological constant.    Dark matterEdit   Dark matter is a hypothetical kind of matter that cannot be seen with telescopes, but which accounts for most of the matter in the Universe. The existence and properties of dark matter are inferred from its gravitational effects on visible matter, radiation, and the large-scale structure of the Universe. Other than neutrinos, a form of hot dark matter, dark matter has not been detected directly, making it one of the greatest mysteries in modern astrophysics. Dark matter neither emits nor absorbs light or any other electromagnetic radiation at any significant level. Dark matter is estimated to constitute 26.8% of the total mass–energy and 84.5% of the total matter in the Universe.    Ordinary MatterEdit   The remaining 4.9% of the mass–energy of the Universe is ordinary matter, that is, atoms, ions, electrons and the objects they form. This matter includes stars, which produce nearly all of the light we see from galaxies, as well as interstellar gas in the interstellar and intergalactic media, planets, and all the objects from everyday life that we can bump into, touch or squeeze. Ordinary matter commonly exists in four states (or phases): solid, liquid, gas, and plasma. However, advances in experimental techniques have revealed other previously theoretical phases, such as Bose–Einstein condensates and fermionic condensates. Ordinary matter is composed of two types of elementary particles: quarks and leptons. For example, the proton is formed of two up quarks and one down quark; the neutron is formed of two down quarks and one up quark; and the electron is a kind of lepton. An atom consists of an atomic nucleus, made up of protons and neutrons, and electrons that orbit the nucleus. Because most of the mass of an atom is concentrated in its nucleus, which is made up of baryons, astronomers often use the term baryonic matter to describe ordinary matter, although a small fraction of this \"baryonic matter\" is electrons. Soon after the Big Bang, primordial protons and neutrons formed from the quark–gluon plasma of the early Universe as it cooled below two trillion degrees. A few minutes later, in a process known as Big Bang nucleosynthesis, nuclei formed from the primordial protons and neutrons. This nucleosynthesis formed lighter elements, those with small atomic numbers up to lithium and beryllium, but the abundance of heavier elements dropped off sharply with increasing atomic number. Some boron may have been formed at this time, but the next heavier element, carbon, was not be formed in significant amounts. Big Bang nucleosynthesis shut down after about 20 minutes due to the rapid drop in temperature and density of the expanding Universe. Subsequent formation of heavier elements resulted from stellar nucleosynthesis and supernova nucleosynthesis.    ParticlesEdit   Ordinary matter and the forces that act on matter can be described in terms of elementary particles. These particles are sometimes described as being fundamental, since they have an unknown substructure, and it is unknown whether or not they are composed of smaller and even more fundamental particles. Of central importance is the Standard Model, a theory that is concerned with electromagnetic interactions and the weak and strong nuclear interactions. The Standard Model is supported by the experimental confirmation of the existence of particles that compose matter: quarks and leptons, and their corresponding \"antimatter\" duals, as well as the force particles that mediate interactions: the photon, the W and Z bosons, and the gluon. The Standard Model predicted the existence of the recently discovered Higgs boson, a particle that is a manifestation of a field within the Universe that can endow particles with mass. Because of its success in explaining a wide variety of experimental results, the Standard Model is sometimes regarded as a \"theory of almost everything\". The Standard Model does not, however, accommodate gravity. A true force-particle \"theory of everything\" has not been attained.   = HadronsEdit =  A hadron is a composite particle made of quarks held together by the strong force. Hadrons are categorized into two families: baryons (such as protons and neutrons) made of three quarks, and mesons (such as pions) made of one quark and one antiquark. Of the hadrons, protons are stable, and neutrons bound within atomic nuclei are stable. Other hadrons are unstable under ordinary conditions and are thus insignificant constituents of the modern Universe. From approximately 10−6 seconds after the Big Bang, during a period is known as the hadron epoch, the temperature of the universe had fallen sufficiently to allow quarks to bind together into hadrons, and the mass of the Universe was dominated by hadrons. Initially the temperature was high enough to allow the formation of hadron/anti-hadron pairs, which kept matter and antimatter in thermal equilibrium. However, as the temperature of the Universe continued to fall, hadron/anti-hadron pairs were no longer produced. Most of the hadrons and anti-hadrons were then eliminated in particle-antiparticle annihilation reactions, leaving a small residual of hadrons by the time the Universe was about one second old.   = LeptonsEdit =  A lepton is an elementary, half-integer spin particle that does not undergo strong interactions but is subject to the Pauli exclusion principle; no two leptons of the same species can be in exactly the same state at the same time. Two main classes of leptons exist: charged leptons (also known as the electron-like leptons), and neutral leptons (better known as neutrinos). Electrons are stable and the most common charged lepton in the Universe, whereas muons and taus are unstable particle that quickly decay after being produced in high energy collisions, such as those involving cosmic rays or carried out in particle accelerators. Charged leptons can combine with other particles to form various composite particles such as atoms and positronium. The electron governs nearly all of chemistry, as it is found in atoms and is directly tied to all chemical properties. Neutrinos rarely interact with anything, and are consequently rarely observed. Neutrinos stream throughout the Universe but rarely interact with normal matter. The lepton epoch was the period in the evolution of the early Universe in which the leptons dominated the mass of the Universe. It started roughly 1 second after the Big Bang, after the majority of hadrons and anti-hadrons annihilated each other at the end of the hadron epoch. During the lepton epoch the temperature of the Universe was still high enough to create lepton/anti-lepton pairs, so leptons and anti-leptons were in thermal equilibrium. Approximately 10 seconds after the Big Bang, the temperature of the Universe had fallen to the point where lepton/anti-lepton pairs were no longer created. Most leptons and anti-leptons were then eliminated in annihilation reactions, leaving a small residue of leptons. The mass of the Universe was then dominated by photons as it entered the following photon epoch.   = PhotonsEdit =  A photon is the quantum of light and all other forms of electromagnetic radiation. It is the force carrier for the electromagnetic force, even when static via virtual photons. The effects of this force are easily observable at the microscopic and at the macroscopic level because the photon has zero rest mass; this allows long distance interactions. Like all elementary particles, photons are currently best explained by quantum mechanics and exhibit wave–particle duality, exhibiting properties of waves and of particles. The photon epoch started after most leptons and anti-leptons were annihilated at the end of the lepton epoch, about 10 seconds after the Big Bang. Atomic nuclei were created in the process of nucleosynthesis which occurred during the first few minutes of the photon epoch. For the remainder of the photon epoch the Universe contained a hot dense plasma of nuclei, electrons and photons. About 380,000 years after the Big Bang, the temperature of the Universe fell to the point where nuclei could combine with electrons to create neutral atoms. As a result, photons no longer interacted frequently with matter and the Universe became transparent. The highly redshifted photons from this period form the cosmic microwave background. Tiny variations in temperature and density detectable in the CMB were the early \"seeds\" from which all subsequent structure formation took place.    Cosmological modelsEdit     Model of the Universe based on general relativityEdit   General relativity is the geometric theory of gravitation published by Albert Einstein in 1915 and the current description of gravitation in modern physics. It is the basis of current cosmological models of the Universe. General relativity generalizes special relativity and Newton's law of universal gravitation, providing a unified description of gravity as a geometric property of space and time, or spacetime. In particular, the curvature of spacetime is directly related to the energy and momentum of whatever matter and radiation are present. The relation is specified by the Einstein field equations, a system of partial differential equations. In general relativity, the distribution of matter and energy determines the geometry of spacetime, which in turn describes the acceleration of matter. Therefore, solutions of the Einstein field equations describe the evolution of the Universe. Combined with measurements of the amount, type, and distribution of matter in the Universe, the equations of general relativity describe the evolution of the Universe over time. With the assumption of the cosmological principle that the Universe is homogeneous and isotropic everywhere, a specific solution of the field equations that describes the Universe is the metric tensor called the Friedmann–Lemaître–Robertson–Walker metric,  where (r, θ, φ) correspond to a spherical coordinate system. This metric has only two undetermined parameters. An overall dimensionless length scale factor R describes the size scale of the Universe as a function of time; an increase in R is the expansion of the Universe. A curvature index k describes the geometry. The index k is defined so that it can be only 0, corresponding to flat Euclidean geometry, 1, corresponding to a space of positive curvature, or −1, a space of positive or negative curvature. The value of R as a function of time t depends upon k and the cosmological constant Λ. The cosmological constant represents the energy density of the vacuum of space and could be related to dark energy. The equation describing how R varies with time is known as the Friedmann equation after its inventor, Alexander Friedmann. The solutions for R(t) depend on k and Λ, but some qualitative features of such solutions are general. First and most importantly, the length scale R of the Universe can remain constant only if the Universe is perfectly isotropic with positive curvature (k=1) and has one precise value of density everywhere, as first noted by Albert Einstein. However, this equilibrium is unstable: because the Universe is known to be inhomogeneous on smaller scales, R must change over time. When R changes, all the spatial distances in the Universe change in tandem; there is an overall expansion or contraction of space itself. This accounts for the observation that galaxies appear to be flying apart; the space between them is stretching. The stretching of space also accounts for the apparent paradox that two galaxies can be 40 billion light years apart, although they started from the same point 13.8 billion years ago and never moved faster than the speed of light. Second, all solutions suggest that there was a gravitational singularity in the past, when R went to zero and matter and energy were infinitely dense. It may seem that this conclusion is uncertain because it is based on the questionable assumptions of perfect homogeneity and isotropy (the cosmological principle) and that only the gravitational interaction is significant. However, the Penrose–Hawking singularity theorems show that a singularity should exist for very general conditions. Hence, according to Einstein's field equations, R grew rapidly from an unimaginably hot, dense state that existed immediately following this singularity (when R had a small, finite value); this is the essence of the Big Bang model of the Universe. Understanding the singularity of the Big Bang likely requires a quantum theory of gravity, which does not yet exist. Third, the curvature index k determines the sign of the mean spatial curvature of spacetime averaged over sufficiently large length scales (greater than about a billion light years). If k=1, the curvature is positive and the Universe has a finite volume. Such universes are often visualized as a three-dimensional sphere embedded in a four-dimensional space. Conversely, if k is zero or negative, the Universe has infinite volume. It may seem counter-intuitive that an infinite and yet infinitely dense Universe could be created in a single instant at the Big Bang when R=0, but exactly that is predicted mathematically when k does not equal 1. By analogy, an infinite plane has zero curvature but infinite area, whereas an infinite cylinder is finite in one direction and a torus is finite in both. A toroidal Universe could behave like a normal Universe with periodic boundary conditions. The ultimate fate of the Universe is still unknown, because it depends critically on the curvature index k and the cosmological constant Λ. If the Universe were sufficiently dense, k would equal +1, meaning that its average curvature throughout is positive and the Universe will eventually recollapse in a Big Crunch, possibly starting a new Universe in a Big Bounce. Conversely, if the Universe were insufficiently dense, k would equal 0 or −1 and the Universe would expand forever, cooling off and eventually reaching the Big Freeze and the heat death of the Universe. Modern data suggests that the expansion speed of the Universe is not decreasing as originally expected, but increasing; if this continues indefinitely, the Universe may eventually reach a Big Rip. Observationally, the Universe appears to be flat (k = 0), with an overall density that is very close to the critical value between recollapse and eternal expansion.    Multiverse hypothesisEdit   Some speculative theories have proposed that our Universe is but one of a set of disconnected universes, collectively denoted as the multiverse, challenging or enhancing more limited definitions of the Universe. Scientific multiverse models are distinct from concepts such as alternate planes of consciousness and simulated reality. Max Tegmark developed a four-part classification scheme for the different types of multiverses that scientists have suggested in various problem domains. An example of such a model is the chaotic inflation model of the early universe. Another is the many-worlds interpretation of quantum mechanics. Parallel worlds are generated in a manner similar to quantum superposition and decoherence, with all states of the wave function being realized in separate worlds. Effectively, the multiverse evolves as a universal wavefunction. If the Big Bang that created our multiverse created an ensemble of multiverses, the wave function of the ensemble would be entangled in this sense. The least controversial category of multiverse in Tegmark's scheme is Level I, which describes distant spacetime events \"in our own universe\", but suggests that statistical analysis exploiting the anthropic principle provides an opportunity to test multiverse theories in some cases. If space is infinite, or sufficiently large and uniform, identical instances of the history of Earth's entire Hubble volume occur every so often, simply by chance. Tegmark calculated our nearest so-called doppelgänger, is 1010115 meters away from us (a double exponential function larger than a googolplex). In principle, it would be impossible to scientifically verify an identical Hubble volume. However, it does follow as a fairly straightforward consequence from otherwise unrelated scientific observations and theories. It is possible to conceive of disconnected spacetimes, each existing but unable to interact with one another. An easily visualized metaphor is a group of separate soap bubbles, in which observers living on one soap bubble cannot interact with those on other soap bubbles, even in principle. According to one common terminology, each \"soap bubble\" of spacetime is denoted as a universe, whereas our particular spacetime is denoted as the Universe, just as we call our moon the Moon. The entire collection of these separate spacetimes is denoted as the multiverse. With this terminology, different Universes are not causally connected to each other. In principle, the other unconnected Universes may have different dimensionalities and topologies of spacetime, different forms of matter and energy, and different physical laws and physical constants, although such possibilities are purely speculative. Others consider each of several bubbles created as part of chaotic inflation to be separate Universes, though in this model these universes all share a causal origin.    Fine-tuned UniverseEdit   The fine-tuned Universe is the proposition that the conditions that allow life in the Universe can only occur when certain universal fundamental physical constants lie within a very narrow range, so that if any of several fundamental constants were only slightly different, the Universe would be unlikely to be conducive to the establishment and development of matter, astronomical structures, elemental diversity, or life as it is understood. The proposition is discussed among philosophers, scientists, theologians, and proponents and detractors of creationism.    Historical developmentEdit   Historically, there have been many ideas of the cosmos (cosmologies) and its origin (cosmogonies). Theories of an impersonal Universe governed by physical laws were first proposed by the Greeks and Indians. Ancient Chinese philosophy encompassed the notion of the Universe including both all of space and all of time. Over the centuries, improvements in astronomical observations and theories of motion and gravitation led to ever more accurate descriptions of the Universe. The modern era of cosmology began with Albert Einstein's 1915 general theory of relativity, which made it possible to quantitatively predict the origin, evolution, and conclusion of the Universe as a whole. Most modern, accepted theories of cosmology are based on general relativity and, more specifically, the predicted Big Bang.    MythologiesEdit   Many cultures have stories describing the origin of the world and universe. Cultures generally regard these stories as having some truth. There are however many differing beliefs in how these stories apply amongst those believing in a supernatural origin, ranging from a god directly creating the Universe as it is now to a god just setting the \"wheels in motion\" (for example via mechanisms such as the big bang and evolution). Ethnologists and anthropologists who study myths have developed various classification schemes for the various themes that appear in creation stories. For example, in one type of story, the world is born from a world egg; such stories include the Finnish epic poem Kalevala, the Chinese story of Pangu or the Indian Brahmanda Purana. In related stories, the Universe is created by a single entity emanating or producing something by him- or herself, as in the Tibetan Buddhism concept of Adi-Buddha, the ancient Greek story of Gaia (Mother Earth), the Aztec goddess Coatlicue myth, the ancient Egyptian god Atum story, and the Judeo-Christian Genesis creation narrative in which the Abrahamic God created the Universe. In another type of story, the Universe is created from the union of male and female deities, as in the Maori story of Rangi and Papa. In other stories, the Universe is created by crafting it from pre-existing materials, such as the corpse of a dead god — as from Tiamat in the Babylonian epic Enuma Elish or from the giant Ymir in Norse mythology – or from chaotic materials, as in Izanagi and Izanami in Japanese mythology. In other stories, the Universe emanates from fundamental principles, such as Brahman and Prakrti, the creation myth of the Serers, or the yin and yang of the Tao.    Philosophical modelsEdit   The pre-Socratic Greek philosophers and Indian philosophers developed some of the earliest philosophical concepts of the Universe. The earliest Greek philosophers noted that appearances can be deceiving, and sought to understand the underlying reality behind the appearances. In particular, they noted the ability of matter to change forms (e.g., ice to water to steam) and several philosophers proposed that all the physical materials in the world are different forms of a single primordial material, or arche. The first to do so was Thales, who proposed this material to be water. Thales' student, Anaximander, proposed that everything came from the limitless apeiron. Anaximenes proposed the primordial material to be air on account of its perceived attractive and repulsive qualities that cause the arche to condense or dissociate into different forms. Anaxagoras proposed the principle of Nous (Mind), while Heraclitus proposed fire (and spoke of logos). Empedocles proposed the elements to be earth, water, air and fire. His four-element model became very popular. Like Pythagoras, Plato believed that all things were composed of number, with Empedocles' elements taking the form of the Platonic solids. Democritus, and later philosophers—most notably Leucippus—proposed that the Universe is composed of indivisible atoms moving through void (vacuum), although Aristotle did not believe that to be feasible because air, like water, offers resistance to motion. Air will immediately rush in to fill a void, and moreover, without resistance, it would do so indefinitely fast. Although Heraclitus argued for eternal change, his contemporary Parmenides made the radical suggestion that all change is an illusion, that the true underlying reality is eternally unchanging and of a single nature. Parmenides denoted this reality as τὸ ἐν (The One). Parmenides' idea seemed implausible to many Greeks, but his student Zeno of Elea challenged them with several famous paradoxes. Aristotle responded to these paradoxes by developing the notion of a potential countable infinity, as well as the infinitely divisible continuum. Unlike the eternal and unchanging cycles of time, he believed that the world is bounded by the celestial spheres and that cumulative stellar magnitude is only finitely multiplicative. The Indian philosopher Kanada, founder of the Vaisheshika school, developed a notion of atomism and proposed that light and heat were varieties of the same substance. In the 5th century AD, the Buddhist atomist philosopher Dignāga proposed atoms to be point-sized, durationless, and made of energy. They denied the existence of substantial matter and proposed that movement consisted of momentary flashes of a stream of energy. The notion of temporal finitism was inspired by the doctrine of creation shared by the three Abrahamic religions: Judaism, Christianity and Islam. The Christian philosopher, John Philoponus, presented the philosophical arguments against the ancient Greek notion of an infinite past and future. Philoponus' arguments against an infinite past were used by the early Muslim philosopher, Al-Kindi (Alkindus); the Jewish philosopher, Saadia Gaon (Saadia ben Joseph); and the Muslim theologian, Al-Ghazali (Algazel).    Astronomical conceptsEdit   Astronomical models of the Universe were proposed soon after astronomy began with the Babylonian astronomers, who viewed the Universe as a flat disk floating in the ocean, and this forms the premise for early Greek maps like those of Anaximander and Hecataeus of Miletus. Later Greek philosophers, observing the motions of the heavenly bodies, were concerned with developing models of the Universe-based more profoundly on empirical evidence. The first coherent model was proposed by Eudoxus of Cnidos. According to Aristotle's physical interpretation of the model, celestial spheres eternally rotate with uniform motion around a stationary Earth. Normal matter is entirely contained within the terrestrial sphere. De Mundo (composed before 250 BC or between 350 and 200 BC), stated, Five elements, situated in spheres in five regions, the less being in each case surrounded by the greater — namely, earth surrounded by water, water by air, air by fire, and fire by ether — make up the whole Universe. This model was also refined by Callippus and after concentric spheres were abandoned, it was brought into nearly perfect agreement with astronomical observations by Ptolemy. The success of such a model is largely due to the mathematical fact that any function (such as the position of a planet) can be decomposed into a set of circular functions (the Fourier modes). Other Greek scientists, such as the Pythagorean philosopher Philolaus, postulated (according to Stobaeus account) that at the center of the Universe was a \"central fire\" around which the Earth, Sun, Moon and Planets revolved in uniform circular motion. The Greek astronomer Aristarchus of Samos was the first known individual to propose a heliocentric model of the Universe. Though the original text has been lost, a reference in Archimedes' book The Sand Reckoner describes Aristarchus's heliocentric model. Archimedes wrote: (translated into English):  \"You, King Gelon, are aware the Universe is the name given by most astronomers to the sphere the center of which is the center of the Earth, while its radius is equal to the straight line between the center of the Sun and the center of the Earth. This is the common account as you have heard from astronomers. But Aristarchus has brought out a book consisting of certain hypotheses, wherein it appears, as a consequence of the assumptions made, that the Universe is many times greater than the Universe just mentioned. His hypotheses are that the fixed stars and the Sun remain unmoved, that the Earth revolves about the Sun on the circumference of a circle, the Sun lying in the middle of the orbit, and that the sphere of fixed stars, situated about the same center as the Sun, is so great that the circle in which he supposes the Earth to revolve bears such a proportion to the distance of the fixed stars as the center of the sphere bears to its surface\"  Aristarchus thus believed the stars to be very far away, and saw this as the reason why stellar parallax had not been observed, that is, the stars had not been observed to move relative each other as the Earth moved around the Sun. The stars are in fact much farther away than the distance that was generally assumed in ancient times, which is why stellar parallax is only detectable with precision instruments. The geocentric model, consistent with planetary parallax, was assumed to be an explanation for the unobservability of the parallel phenomenon, stellar parallax. The rejection of the heliocentric view was apparently quite strong, as the following passage from Plutarch suggests (On the Apparent Face in the Orb of the Moon):  \"Cleanthes [a contemporary of Aristarchus and head of the Stoics ] thought it was the duty of the Greeks to indict Aristarchus of Samos on the charge of impiety for putting in motion the Hearth of the Universe [i.e. the Earth], . . . supposing the heaven to remain at rest and the Earth to revolve in an oblique circle, while it rotates, at the same time, about its own axis\"  The only other astronomer from antiquity known by name who supported Aristarchus's heliocentric model was Seleucus of Seleucia, a Hellenistic astronomer who lived a century after Aristarchus. According to Plutarch, Seleucus was the first to prove the heliocentric system through reasoning, but it is not known what arguments he used. Seleucus' arguments for a heliocentric cosmology were probably related to the phenomenon of tides. According to Strabo (1.1.9), Seleucus was the first to state that the tides are due to the attraction of the Moon, and that the height of the tides depends on the Moon's position relative to the Sun. Alternatively, he may have proved heliocentricity by determining the constants of a geometric model for it, and by developing methods to compute planetary positions using this model, like what Nicolaus Copernicus later did in the 16th century. During the Middle Ages, heliocentric models were also proposed by the Indian astronomer Aryabhata, and by the Persian astronomers Albumasar and Al-Sijzi.  The Aristotelian model was accepted in the Western world for roughly two millennia, until Copernicus revived Aristarchus's perspective that the astronomical data could be explained more plausibly if the earth rotated on its axis and if the sun were placed at the center of the Universe.  In the center rests the Sun. For who would place this lamp of a very beautiful temple in another or better place than this wherefrom it can illuminate everything at the same time?  As noted by Copernicus himself, the notion that the Earth rotates is very old, dating at least to Philolaus (c. 450 BC), Heraclides Ponticus (c. 350 BC) and Ecphantus the Pythagorean. Roughly a century before Copernicus, the Christian scholar Nicholas of Cusa also proposed that the Earth rotates on its axis in his book, On Learned Ignorance (1440). Aryabhata (476–550 AD/CE) and Al-Sijzi also proposed that the Earth rotates on its axis. Empirical evidence for the Earth's rotation on its axis, using the phenomenon of comets, was given by Tusi (1201–1274) and Ali Qushji (1403–1474). This cosmology was accepted by Isaac Newton, Christiaan Huygens and later scientists. Edmund Halley (1720) and Jean-Philippe de Chéseaux (1744) noted independently that the assumption of an infinite space filled uniformly with stars would lead to the prediction that the nighttime sky would be as bright as the Sun itself; this became known as Olbers' paradox in the 19th century. Newton believed that an infinite space uniformly filled with matter would cause infinite forces and instabilities causing the matter to be crushed inwards under its own gravity. This instability was clarified in 1902 by the Jeans instability criterion. One solution to these paradoxes is the Charlier Universe, in which the matter is arranged hierarchically (systems of orbiting bodies that are themselves orbiting in a larger system, ad infinitum) in a fractal way such that the Universe has a negligibly small overall density; such a cosmological model had also been proposed earlier in 1761 by Johann Heinrich Lambert. A significant astronomical advance of the 18th century was the realization by Thomas Wright, Immanuel Kant and others of nebulae. The modern era of physical cosmology began in 1917, when Albert Einstein first applied his general theory of relativity to model the structure and dynamics of the Universe.    See alsoEdit     ReferencesEdit   Sources Bartel, Leendert van der Waerden (1987). \"The Heliocentric System in Greek, Persian and Hindu Astronomy\". Annals of the New York Academy of Sciences 500 (1): 525–545. Bibcode:1987NYASA.500..525V. doi:10.1111/j.1749-6632.1987.tb37224.x.  Landau L, Lifshitz EM (1975). The Classical Theory of Fields (Course of Theoretical Physics) 2 (revised 4th English ed.). New York: Pergamon Press. pp. 358–397. ISBN 978-0-08-018176-9.  Liddell, H. G. & Scott, R. (1968). A Greek-English Lexicon. Oxford University Press. ISBN 0-19-864214-8.  Misner, C.W., Thorne, Kip, Wheeler, J.A. (1973). Gravitation. San Francisco: W. H. Freeman. pp. 703–816. ISBN 978-0-7167-0344-0.  Raine, D. J.; Thomas, E. G. (2001). An Introduction to the Science of Cosmology. Institute of Physics Publishing.  Rindler, W. (1977). Essential Relativity: Special, General, and Cosmological. New York: Springer Verlag. pp. 193–244. ISBN 0-387-10090-3.","label":"foo"},{"text":"","label":"foo"},{"text":"","label":"foo"},{"text":"","label":"foo"},{"text":"","label":"foo"},{"text":"An academic discipline is a branch of knowledge. It incorporates expertise, people, projects, communities, challenges, studies, inquiry, and research areas that are strongly associated with a given academic discipline. For example, the branches of science are commonly referred to as the scientific disciplines, e.g. physics, mathematics, computer science. Individuals associated with academic disciplines are commonly referred to as experts or specialists. Others, who may have studied liberal arts or systems theory rather than concentrating in a specific academic discipline are classified as generalists. While academic disciplines in and of themselves are more or less focused practices, scholarly approaches such as multidisciplinarity, interdisciplinarity, transdisciplinarity, and crossdisciplinarity, integrate aspects from multiple academic disciplines, therefore addressing any problems that may arise from narrow concentration within specialized fields of study. For example, professionals may encounter trouble communicating across academic disciplines because of differences in language and/or specified concepts. Some researchers believe that academic disciplines may be replaced by what is known as Mode 2 or \"post academic science\", which involves the acquisition of cross-disciplinary knowledge through collaboration of specialists from various academic disciplines.    History of the conceptEdit  The University of Paris in 1231 consisted of four faculties: Theology, Medicine, Canon Law and Arts. Educational institutions originally used the term \"discipline\" to catalog and archive the new and expanding body of information produced by the scientific community. Disciplinary designations originated in German Universities during the beginning of the nineteenth century. Most academic disciplines have their roots in the mid-to-late-19th century secularization of universities, when the traditional curricula were supplemented with non-classical languages and literatures, social sciences such as political science, economics, sociology and public administration, and natural science and technology disciplines such as physics, chemistry, biology, and engineering. In the early 20th century, new academic disciplines such as education and psychology were added. In the 1970s and 1980s, there was an explosion of new academic disciplines focusing on specific themes, such as media studies, women's studies, and black studies. Many academic disciplines designed as preparation for careers and professions, such as nursing, hospitality management, and corrections, also emerged in the universities. Finally, interdisciplinary scientific fields of study such as biochemistry and geophysics gained prominence as their contribution to knowledge became widely recognized. As the twentieth century approached, these designations were gradually adopted by other countries and became the accepted conventional subjects. However, these designations differed between various countries. In the twentieth century, the science disciplines included: physics, chemistry, biology, geology, and astronomy. The social science disciplines included: economics, politics, sociology, and psychology. Prior to the twentieth century, categories were broad and general, which was expected due to the lack of interest in science at the time. With rare exceptions, practitioners of science tended to be amateurs and were referred to as \"natural historians\" and \"natural philosophers\"—labels that date back to Aristotle—instead of \"scientists\". Natural history referred to what we now call life sciences and natural philosophy referred to the current physical sciences. Few opportunities existed for science as an occupation outside of the educational system. Higher education provided the institutional structure for scientific investigation, as well as economic support. Soon, the volume of scientific information rapidly increased and people realized the importance of concentrating on smaller fields of scientific activity. Because of this, scientific specializations emerged. As these specializations developed, modern scientific disciplines in universities also improved. Eventually, academia's identified disciplines became the foundations for people of specific specialized interests and expertise.    Functions and criticismEdit  A very influential critique of the concept of academic disciplines came from Michel Foucault in his 1975 book, Discipline and Punish. Foucault asserts that academic disciplines originate from the same social movements and mechanisms of control that established the modern prison and penal system in 18th century France, and that this fact reveals essential aspects they continue to have in common: \"The disciplines characterize, classify, specialize; they distribute along a scale, around a norm, hierarchize individuals in relation to one another and, if necessary, disqualify and invalidate.\" (Foucault, 1975/1979, p 223.)    Communities of academic disciplinesEdit  Communities of academic disciplines can be found outside of academia within corporations, government agencies, and independent organizations, where they take the form of associations of professionals with common interests and specific knowledge. Such communities include corporate think tanks, NASA, and IUPAC. Communities such as these exist to benefit the organizations affiliated with them by providing specialized new ideas, research, and findings. Nations at various developmental stages will find need for different academic disciplines during different times of growth. A newly developing nation will likely prioritize government and political matters over those of the arts and sciences. On the other hand, a well-developed nation may be capable of investing more into the arts and sciences. Communities of academic disciplines would contribute at varying levels of importance during different stages of development.    VariationsEdit  These categories explain how the different academic disciplines interact with one another.    MultidisciplinaryEdit   Multidisciplinary' knowledge is associated with more than one existing academic discipline or profession. A multidisciplinary community or project is made up of people from different academic disciplines and professions. These people are engaged in working together as equal stakeholders in addressing a common challenge. A multidisciplinary person is one with degrees from two or more academic disciplines. This one person can take the place of two or more people in a multidisciplinary community. Over time, multidisciplinary work does not typically lead to an increase or a decrease in the number of academic disciplines. One key question is how well the challenge can be decomposed into subparts, and then addressed via the distributed knowledge in the community. The lack of shared vocabulary between people and communication overhead can sometimes be an issue in these communities and projects. If challenges of a particular type need to be repeatedly addressed so that each one can be properly decomposed, a multidisciplinary community can be exceptionally efficient and effective. There are many examples of a particular idea appearing in different academic disciplines, all of which came about around the same time. One example of this scenario is the shift from the approach of focusing on sensory awareness of the whole, \"an attention to the 'total field'\", a \"sense of the whole pattern, of form and function as a unity\", an \"integral idea of structure and configuration\". This has happened in art (in the form of cubism), physics, poetry, communication and educational theory. According to Marshall McLuhan, this paradigm shift was due to the passage from the era of mechanization, which brought sequentiality, to the era of the instant speed of electricity, which brought simultaneity. Multidisciplinary approaches also encourage people to help shape the innovation of the future. The political dimensions of forming new multidisciplinary partnerships to solve the so-called societal Grand Challenges were presented in the Innovation Union and in the European Framework Programme, the Horizon 2020 operational overlay. Innovation across academic disciplines is considered the pivotal foresight of the creation of new products, systems, and processes for the benefit of all societies' growth and wellbeing. Regional examples such as Biopeople and industry-academia initiatives in translational medicine such as SHARE.ku.dk in Denmark provides the evidence of the successful endavour of multidisciplinary innovation and facilitation of the paradigm shift.    InterdisciplinaryEdit   Interdisciplinary knowledge is the extent of knowledge that exists between or beyond existing academic disciplines or professions. The new knowledge may be claimed by members of none, one, both, or an emerging new academic discipline or profession. An interdisciplinary community, or project, is made up of people from multiple disciplines and professions who are engaged in creating and applying new knowledge as they work together as equal stakeholders in addressing a common challenge. The key question is, what new knowledge (of an academic discipline nature), which is outside the existing disciplines, is required to address the challenge? Aspects of the challenge cannot be addressed easily with existing distributed knowledge, and acquiring new knowledge becomes a primary subgoal of addressing the common challenge. The nature of the challenge, either its scale or complexity, requires that many people have interactional expertise to improve their efficiency working across multiple disciplines as well as within the new interdisciplinary area. An interdisciplinarian is a person with degrees and additional interactional expertise from/in one or more academic disciplines, and new knowledge that is claimed by more than one discipline. Over time, interdisciplinary work can lead to an increase or a decrease in the number of academic disciplines.    TransdisciplinaryEdit   In practice, transdisciplinary can be thought of as the union of all interdisciplinary efforts. While interdisciplinary teams may be creating new knowledge that lies between several existing disciplines, a transdisciplinary team is more holistic and seeks to relate all disciplines into a coherent whole.    Cross-disciplinaryEdit  Cross-disciplinary knowledge is that which explains aspects of one discipline in terms of another. Common examples of cross-disciplinary approaches are studies of the physics of music or the politics of literature.    Bibliometric studies of disciplinesEdit  Bibliometrics can be used to map several issues in relation to disciplines, for example the flow of ideas within and among disciplines (Lindholm-Romantschuk, 1998) or the existence of specific national traditions within disciplines.    See alsoEdit   List of academic disciplines and sub-disciplines Omniscience Scientific community    ReferencesEdit     Further readingEdit  R. Fagin, J. Y. Halpern, Y. Moses, and M. Y. Vardi. Reasoning about Knowledge, The MIT Press, 1995. ISBN 0-262-56200-6 A. Abbott. The System of Professions: An Essay on the Division of Expert Labor, University of Chicago Press, 1988. ISBN 978-0-226-00069-5 Augsburg, Tanya. (2005), Becoming Interdisciplinary: An Introduction to Interdisciplinary Studies. Dogan, Mattei & Pahre, R. (1990). \"The fate of formal disciplines: from coherence to dispersion.\" In Creative Marginality: Innovation at the Intersections of Social Sciences. Boulder, CO: Westview. pp. 85–113. Dullemeijer, P. (1980). \"Dividing biology into disciplines: Chaos or multiformity?\" Journal Acta Biotheoretica, 29(2), 87–93. Gibbons, M.; Limoges, C.; Nowotny, H.; Schwartzman,S.; Scott, P. & Trow, M. (1994). The New Production of Knowledge: The Dynamics of Science and Research in Contemporary Societies. London: Sage. Golinski, Jan (1998/2005). Making Natural Knowledge: Constructivis, and the History of Science. New York: Cambridge University Press. Chapter 2: \"Identity and discipline.\" Part II: The Disciplinary Mold (pp. 66–78). Hicks, Diana (2004). \"The Four Literatures of Social Science\". IN: Handbook of Quantitative Science and Technology Research: The Use of Publication and Patent Statistics in Studies of S&T Systems. Ed. Henk Moed. Dordrecht: Kluwer Academic. Hyland, Ken (2004). Disciplinary Discourses: Social Interactions in Academic Writing. New edition. University of Michigan Press/ESL. Klein, J. T. (1990). Interdisciplinarity: History, Theory, and Practice. Detroit: Wayne State University Press. Leydesdorff, Loet & Rafols, Ismael. (2008). A global map of science based on the ISI subject categories. Journal of the American Society for Information Science and Technology. The decomposition of scientific literature into disciplinary and subdisciplinary structures is one of the core goals of scientometrics. How can we achieve a good decomposition? The ISI subject categories classify journals included in the Science Citation Index (SCI). The aggregated journal-journal citation matrix contained in the Journal Citation Reports can be aggregated on the basis of these categories. This leads to an asymmetrical matrix (citing versus cited) that is much more densely populated than the underlying matrix at the journal level. Exploratory factor analysis of the matrix of subject categories suggests a 14-factor solution. This solution could be interpreted as the disciplinary structure of science. Lindholm-Romantschuk, Y. (1998). Scholarly Book Reviewing in the Social Sciences and Humanities: The Flow of Ideas within and among Disciplines. Westport, Connecticut: Greenwood Press. Martin, B. (1998). Information Liberation: Challenging the Corruptions of Information Power. London: Freedom Press. Book freely available. Morillo, F.; Bordons, M. & Gomez, I. (2003). \"Interdisciplinarity in science: A tentative typology of disciplines and research areas\". Journal of the American Society for Information Science and Technology, 54(13), 1237–1249. Morillo, F.; Bordons, M; & Gomez, I. (2001). \"An approach to interdisciplinarity bibliometric indicators.\" Scientometrics, 51(1), 203–222. Newell, A. (1983). \"Reflections on the structure of an interdiscipline.\" In Machlup, F. & U. Mansfield (Eds.), The Study of Information: Interdisciplinary Messages (pp. 99–110). NY: John Wiley & Sons. Pierce, S. J. (1991). \"Subject areas, disciplines and the concept of authority\". Library and Information Science Research, 13, 21–35. Porter, A. L.; Roessner, J. D.; Cohen, A. S. & Perreault, M. (2006). \"Interdisciplinary research: meaning, metrics and nurture.\" Research Evaluation, 15(3), 187–195. Prior, Paul (1998). Writing/Disciplinarity: A Sociohistoric Account of Literate Activity in the Academy. Lawrence Erlbaum. (Rhetoric, Knowledge and Society Series) Qin, J.; Lancaster, F. W. & Allen, B. (1997). \"Types and levels of collaboration in interdisciplinary research in the sciences.\" Journal of the American Society for Information Science, 48(10), 893–916. Rinia, E. J.; van Leeuwen, T. N.; Bruins, E. E. W.; van Vuren, H. G. & van Raan, A. F. J. (2002). \"Measuring knowledge transfer between fields of science.\" Scientometrics, 54(3), 347–362. Sanz-Menendez, L.; Bordons, M. & Zulueta, M. A. (2001). \"Interdisciplinarity as a multidimensional concept: its measure in three different research areas.\" Research Evaluation, 10(1), 47–58. Stichweh, R. (2001). \"Scientific Disciplines, History of\". IN: Smelser, N. J. & Baltes, P. B. (eds.). International Encyclopedia of the Social and Behavioral Sciences. Oxford: Elsevier Science (pp. 13727–13731). Szostak, R. (2000). Superdisciplinarity: A Simple Definition of Interdisciplinarity With Profound Implications. Association for Integrative Studies, Portland, Oregon, October 2000. (Meeting presentation) Tengström, E. (1993). Biblioteks- och informationsvetenskapen – ett fler- eller tvärvetenskapligt område? Svensk Biblioteksforskning(1), 9–20. Tomov, D. T.& Mutafov, H. G. (1996). \"Comparative indicators of interdisciplinarity in modern science.\" Scientometrics, 37(2), 267–278. van Leeuwen, T. N. & Tijssen, R. J. W. (1993). \"Assessing multidisciplinary areas of science and technology – A synthetic bibliometric study of Dutch nuclear-energy research.\" Scientometrics, 26(1), 115–133. van Leeuwen, T. N. & Tijssen, R. J. W. (2000). \"Interdisciplinary dynamics of modern science: analysis of cross-disciplinary citation flows.\" Research Evaluation, 9(3), 183–187. Weisgerber, D. W. (1993). \"Interdisciplinary searching – problems and suggested remedies – A Report from the ICSTI Group on Interdisciplinary Searching.\" Journal of Documentation, 49(3), 231–254. Wittrock, B. (2001). \"Disciplines, History of, in the Social Sciences.\" IN: International Encyclopedia of the Social & Behavioral Sciences, (pp. 3721–3728). Ed. By N. J. Smeltser & P. B. Baltes. Amsterdam: Elsevier.    External linksEdit  Association for Integrative Studies","label":"foo"},{"text":"Astronomy is a natural science which is the study of celestial objects (such as stars, galaxies, planets, moons, asteroids, comets and nebulae), the physics, chemistry, and evolution of such objects, and phenomena that originate outside the atmosphere of Earth, including supernovae explosions, gamma ray bursts, and cosmic microwave background radiation. A related but distinct subject, physical cosmology, is concerned with studying the universe as a whole. Astronomy is one of the oldest sciences. The early civilizations in recorded history, such as the Babylonians, Greeks, Indians, Egyptians, Nubians, Iranians, Chinese, and Maya performed methodical observations of the night sky. However, the invention of the telescope was required before astronomy was able to develop into a modern science. Historically, astronomy has included disciplines as diverse as astrometry, celestial navigation, observational astronomy and the making of calendars, but professional astronomy is nowadays often considered to be synonymous with astrophysics. During the 20th century, the field of professional astronomy split into observational and theoretical branches. Observational astronomy is focused on acquiring data from observations of astronomical objects, which is then analyzed using basic principles of physics. Theoretical astronomy is oriented toward the development of computer or analytical models to describe astronomical objects and phenomena. The two fields complement each other, with theoretical astronomy seeking to explain the observational results and observations being used to confirm theoretical results. Astronomy is one of the few sciences where amateurs can still play an active role, especially in the discovery and observation of transient phenomena. Amateur astronomers have made and contributed to many important astronomical discoveries.    Etymology   Astronomy (from the Greek ἀστρονομία from ἄστρον astron, \"star\" and -νομία -nomia from νόμος nomos, \"law\" or \"culture\") means \"law of the stars\" (or \"culture of the stars\" depending on the translation). Astronomy should not be confused with astrology, the belief system which claims that human affairs are correlated with the positions of celestial objects. Although the two fields share a common origin they are now entirely distinct.    Use of terms \"astronomy\" and \"astrophysics\"  Generally, either the term \"astronomy\" or \"astrophysics\" may be used to refer to this subject. Based on strict dictionary definitions, \"astronomy\" refers to \"the study of objects and matter outside the Earth's atmosphere and of their physical and chemical properties\" and \"astrophysics\" refers to the branch of astronomy dealing with \"the behavior, physical properties, and dynamic processes of celestial objects and phenomena\". In some cases, as in the introduction of the introductory textbook The Physical Universe by Frank Shu, \"astronomy\" may be used to describe the qualitative study of the subject, whereas \"astrophysics\" is used to describe the physics-oriented version of the subject. However, since most modern astronomical research deals with subjects related to physics, modern astronomy could actually be called astrophysics. Few fields, such as astrometry, are purely astronomy rather than also astrophysics. Various departments in which scientists carry out research on this subject may use \"astronomy\" and \"astrophysics,\" partly depending on whether the department is historically affiliated with a physics department, and many professional astronomers have physics rather than astronomy degrees. One of the leading scientific journals in the field is the European journal named Astronomy and Astrophysics. The leading American journals are The Astrophysical Journal and The Astronomical Journal.    History   In early times, astronomy only comprised the observation and predictions of the motions of objects visible to the naked eye. In some locations, early cultures assembled massive artifacts that possibly had some astronomical purpose. In addition to their ceremonial uses, these observatories could be employed to determine the seasons, an important factor in knowing when to plant crops, as well as in understanding the length of the year. Before tools such as the telescope were invented, early study of the stars was conducted using the naked eye. As civilizations developed, most notably in Mesopotamia, Greece, India, China, Egypt, and Central America, astronomical observatories were assembled, and ideas on the nature of the universe began to be explored. Most of early astronomy actually consisted of mapping the positions of the stars and planets, a science now referred to as astrometry. From these observations, early ideas about the motions of the planets were formed, and the nature of the Sun, Moon and the Earth in the universe were explored philosophically. The Earth was believed to be the center of the universe with the Sun, the Moon and the stars rotating around it. This is known as the geocentric model of the universe, or the Ptolemaic system, named after Ptolemy. A particularly important early development was the beginning of mathematical and scientific astronomy, which began among the Babylonians, who laid the foundations for the later astronomical traditions that developed in many other civilizations. The Babylonians discovered that lunar eclipses recurred in a repeating cycle known as a saros.  Following the Babylonians, significant advances in astronomy were made in ancient Greece and the Hellenistic world. Greek astronomy is characterized from the start by seeking a rational, physical explanation for celestial phenomena. In the 3rd century BC, Aristarchus of Samos estimated the size and distance of the Moon and Sun, and was the first to propose a heliocentric model of the solar system. In the 2nd century BC, Hipparchus discovered precession, calculated the size and distance of the Moon and invented the earliest known astronomical devices such as the astrolabe. Hipparchus also created a comprehensive catalog of 1020 stars, and most of the constellations of the northern hemisphere derive from Greek astronomy. The Antikythera mechanism (c. 150–80 BC) was an early analog computer designed to calculate the location of the Sun, Moon, and planets for a given date. Technological artifacts of similar complexity did not reappear until the 14th century, when mechanical astronomical clocks appeared in Europe. During the Middle Ages, astronomy was mostly stagnant in medieval Europe, at least until the 13th century. However, astronomy flourished in the Islamic world and other parts of the world. This led to the emergence of the first astronomical observatories in the Muslim world by the early 9th century. In 964, the Andromeda Galaxy, the largest galaxy in the Local Group, was discovered by the Persian astronomer Azophi and first described in his Book of Fixed Stars. The SN 1006 supernova, the brightest apparent magnitude stellar event in recorded history, was observed by the Egyptian Arabic astronomer Ali ibn Ridwan and the Chinese astronomers in 1006. Some of the prominent Islamic (mostly Persian and Arab) astronomers who made significant contributions to the science include Al-Battani, Thebit, Azophi, Albumasar, Biruni, Arzachel, Al-Birjandi, and the astronomers of the Maragheh and Samarkand observatories. Astronomers during that time introduced many Arabic names now used for individual stars. It is also believed that the ruins at Great Zimbabwe and Timbuktu may have housed an astronomical observatory. Europeans had previously believed that there had been no astronomical observation in pre-colonial Middle Ages sub-Saharan Africa but modern discoveries show otherwise. The Roman Catholic Church gave more financial and social support to the study of astronomy for over six centuries, from the recovery of ancient learning during the late Middle Ages into the Enlightenment, than any other, and, probably, all other, institutions. Among the Church's motives was finding the date for Easter.    Scientific revolution   During the Renaissance, Nicolaus Copernicus proposed a heliocentric model of the solar system. His work was defended, expanded upon, and corrected by Galileo Galilei and Johannes Kepler. Galileo used telescopes to enhance his observations. Kepler was the first to devise a system that described correctly the details of the motion of the planets with the Sun at the center. However, Kepler did not succeed in formulating a theory behind the laws he wrote down. It was left to Newton's invention of celestial dynamics and his law of gravitation to finally explain the motions of the planets. Newton also developed the reflecting telescope. Further discoveries paralleled the improvements in the size and quality of the telescope. More extensive star catalogues were produced by Lacaille. The astronomer William Herschel made a detailed catalog of nebulosity and clusters, and in 1781 discovered the planet Uranus, the first new planet found. The distance to a star was first announced in 1838 when the parallax of 61 Cygni was measured by Friedrich Bessel. During the 18–19th centuries, attention to the three body problem by Euler, Clairaut, and D'Alembert led to more accurate predictions about the motions of the Moon and planets. This work was further refined by Lagrange and Laplace, allowing the masses of the planets and moons to be estimated from their perturbations. Significant advances in astronomy came about with the introduction of new technology, including the spectroscope and photography. Fraunhofer discovered about 600 bands in the spectrum of the Sun in 1814–15, which, in 1859, Kirchhoff ascribed to the presence of different elements. Stars were proven to be similar to the Earth's own Sun, but with a wide range of temperatures, masses, and sizes. The existence of the Earth's galaxy, the Milky Way, as a separate group of stars, was only proved in the 20th century, along with the existence of \"external\" galaxies, and soon after, the expansion of the Universe, seen in the recession of most galaxies from us. Modern astronomy has also discovered many exotic objects such as quasars, pulsars, blazars, and radio galaxies, and has used these observations to develop physical theories which describe some of these objects in terms of equally exotic objects such as black holes and neutron stars. Physical cosmology made huge advances during the 20th century, with the model of the Big Bang heavily supported by the evidence provided by astronomy and physics, such as the cosmic microwave background radiation, Hubble's law, and cosmological abundances of elements. Space telescopes have enabled measurements in parts of the electromagnetic spectrum normally blocked or blurred by the atmosphere.    Observational astronomy   In astronomy, the main source of information about celestial bodies and other objects is visible light or more generally electromagnetic radiation. Observational astronomy may be divided according to the observed region of the electromagnetic spectrum. Some parts of the spectrum can be observed from the Earth's surface, while other parts are only observable from either high altitudes or outside the Earth's atmosphere. Specific information on these subfields is given below.    Radio astronomy   Radio astronomy studies radiation with wavelengths greater than approximately one millimeter. Radio astronomy is different from most other forms of observational astronomy in that the observed radio waves can be treated as waves rather than as discrete photons. Hence, it is relatively easier to measure both the amplitude and phase of radio waves, whereas this is not as easily done at shorter wavelengths. Although some radio waves are produced by astronomical objects in the form of thermal emission, most of the radio emission that is observed from Earth is the result of synchrotron radiation, which is produced when electrons orbit magnetic fields. Additionally, a number of spectral lines produced by interstellar gas, notably the hydrogen spectral line at 21 cm, are observable at radio wavelengths. A wide variety of objects are observable at radio wavelengths, including supernovae, interstellar gas, pulsars, and active galactic nuclei.    Infrared astronomy   Infrared astronomy is founded on the detection and analysis of infrared radiation (wavelengths longer than red light). The infrared spectrum is useful for studying objects that are too cold to radiate visible light, such as planets, circumstellar disks or nebulae whose light is blocked by dust. Longer infrared wavelengths can penetrate clouds of dust that block visible light, allowing the observation of young stars in molecular clouds and the cores of galaxies. Observations from the Wide-field Infrared Survey Explorer (WISE) have been particularly effective at unveiling numerous Galactic protostars and their host star clusters. With the exception of wavelengths close to visible light, infrared radiation is heavily absorbed by the atmosphere, or masked, as the atmosphere itself produces significant infrared emission. Consequently, infrared observatories have to be located in high, dry places or in space. Some molecules radiate strongly in the infrared. This allows the study the chemistry of space; more specifically it can detect water in comets.    Optical astronomy   Historically, optical astronomy, also called visible light astronomy, is the oldest form of astronomy. Optical images of observations were originally drawn by hand. In the late 19th century and most of the 20th century, images were made using photographic equipment. Modern images are made using digital detectors, particularly detectors using charge-coupled devices (CCDs) and recorded on modern medium. Although visible light itself extends from approximately 4000 Å to 7000 Å (400 nm to 700 nm), that same equipment can be used to observe some near-ultraviolet and near-infrared radiation.    Ultraviolet astronomy   Ultraviolet astronomy refers to observations at ultraviolet wavelengths between approximately 100 and 3200 Å (10 to 320 nm). Light at these wavelengths is absorbed by the Earth's atmosphere, so observations at these wavelengths must be performed from the upper atmosphere or from space. Ultraviolet astronomy is best suited to the study of thermal radiation and spectral emission lines from hot blue stars (OB stars) that are very bright in this wave band. This includes the blue stars in other galaxies, which have been the targets of several ultraviolet surveys. Other objects commonly observed in ultraviolet light include planetary nebulae, supernova remnants, and active galactic nuclei. However, as ultraviolet light is easily absorbed by interstellar dust, an appropriate adjustment of ultraviolet measurements is necessary.    X-ray astronomy   X-ray astronomy is the study of astronomical objects at X-ray wavelengths. Typically, X-ray radiation is produced by synchrotron emission (the result of electrons orbiting magnetic field lines), thermal emission from thin gases above 107 (10 million) kelvins, and thermal emission from thick gases above 107 Kelvin. Since X-rays are absorbed by the Earth's atmosphere, all X-ray observations must be performed from high-altitude balloons, rockets, or spacecraft. Notable X-ray sources include X-ray binaries, pulsars, supernova remnants, elliptical galaxies, clusters of galaxies, and active galactic nuclei. X-rays were first observed and documented in 1895 by Wilhelm Conrad Röntgen, a German scientist who found them when experimenting with vacuum tubes. Through a series of experiments, Röntgen was able to discover the beginning elements of radiation. The \"X\", in fact, holds its own significance, as it represents Röntgen's inability to identify exactly the type of radiation.    Gamma-ray astronomy   Gamma ray astronomy is the study of astronomical objects at the shortest wavelengths of the electromagnetic spectrum. Gamma rays may be observed directly by satellites such as the Compton Gamma Ray Observatory or by specialized telescopes called atmospheric Cherenkov telescopes. The Cherenkov telescopes do not actually detect the gamma rays directly but instead detect the flashes of visible light produced when gamma rays are absorbed by the Earth's atmosphere. Most gamma-ray emitting sources are actually gamma-ray bursts, objects which only produce gamma radiation for a few milliseconds to thousands of seconds before fading away. Only 10% of gamma-ray sources are non-transient sources. These steady gamma-ray emitters include pulsars, neutron stars, and black hole candidates such as active galactic nuclei.    Fields not based on the electromagnetic spectrum  In addition to electromagnetic radiation, a few other events originating from great distances may be observed from the Earth. In neutrino astronomy, astronomers use heavily shielded underground facilities such as SAGE, GALLEX, and Kamioka II/III for the detection of neutrinos. The vast majority of the neutrinos streaming through the Earth originate from the Sun, but 24 neutrinos were also detected from supernova 1987A. Cosmic rays, which consist of very high energy particles that can decay or be absorbed when they enter the Earth's atmosphere, result in a cascade of particles which can be detected by current observatories. Additionally, some future neutrino detectors may also be sensitive to the particles produced when cosmic rays hit the Earth's atmosphere. Gravitational wave astronomy is an emerging new field of astronomy which aims to use gravitational wave detectors to collect observational data about compact objects. A few observatories have been constructed, such as the Laser Interferometer Gravitational Observatory LIGO, but gravitational waves are extremely difficult to detect. Combining observations made using electromagnetic radiation, neutrinos or gravitational waves with those made using a different means, which shall give complementary information, is known as multi-messenger astronomy.    Astrometry and celestial mechanics   One of the oldest fields in astronomy, and in all of science, is the measurement of the positions of celestial objects. Historically, accurate knowledge of the positions of the Sun, Moon, planets and stars has been essential in celestial navigation (the use of celestial objects to guide navigation) and in the making of calendars. Careful measurement of the positions of the planets has led to a solid understanding of gravitational perturbations, and an ability to determine past and future positions of the planets with great accuracy, a field known as celestial mechanics. More recently the tracking of near-Earth objects will allow for predictions of close encounters, and potential collisions, with the Earth. The measurement of stellar parallax of nearby stars provides a fundamental baseline in the cosmic distance ladder that is used to measure the scale of the universe. Parallax measurements of nearby stars provide an absolute baseline for the properties of more distant stars, as their properties can be compared. Measurements of radial velocity and proper motion plot the movement of these systems through the Milky Way galaxy. Astrometric results are the basis used to calculate the distribution of dark matter in the galaxy. During the 1990s, the measurement of the stellar wobble of nearby stars was used to detect large extrasolar planets orbiting nearby stars.    Theoretical astronomy   Theoretical astronomers use several tools including analytical models (for example, polytropes to approximate the behaviors of a star) and computational numerical simulations. Each has some advantages. Analytical models of a process are generally better for giving insight into the heart of what is going on. Numerical models reveal the existence of phenomena and effects otherwise unobserved. Theorists in astronomy endeavor to create theoretical models and from the results predict observational consequences of those models. The observation of a phenomenon predicted by a model allows astronomers to select between several alternate or conflicting models. Theorists also try to generate or modify models to take into account new data. In the case of an inconsistency, the general tendency is to try to make minimal modifications to the model so that it produces results that fit the data. In some cases, a large amount of inconsistent data over time may lead to total abandonment of a model. Topics studied by theoretical astronomers include: stellar dynamics and evolution; galaxy formation; large-scale structure of matter in the Universe; origin of cosmic rays; general relativity and physical cosmology, including string cosmology and astroparticle physics. Astrophysical relativity serves as a tool to gauge the properties of large scale structures for which gravitation plays a significant role in physical phenomena investigated and as the basis for black hole (astro)physics and the study of gravitational waves. Some widely accepted and studied theories and models in astronomy, now included in the Lambda-CDM model are the Big Bang, Cosmic inflation, dark matter, and fundamental theories of physics. A few examples of this process: Dark matter and dark energy are the current leading topics in astronomy, as their discovery and controversy originated during the study of the galaxies.    Specific subfields     Solar astronomy   At a distance of about eight light-minutes, the most frequently studied star is the Sun, a typical main-sequence dwarf star of stellar class G2 V, and about 4.6 billion years (Gyr) old. The Sun is not considered a variable star, but it does undergo periodic changes in activity known as the sunspot cycle. This is an 11-year fluctuation in sunspot numbers. Sunspots are regions of lower-than- average temperatures that are associated with intense magnetic activity. The Sun has steadily increased in luminosity over the course of its life, increasing by 40% since it first became a main-sequence star. The Sun has also undergone periodic changes in luminosity that can have a significant impact on the Earth. The Maunder minimum, for example, is believed to have caused the Little Ice Age phenomenon during the Middle Ages. The visible outer surface of the Sun is called the photosphere. Above this layer is a thin region known as the chromosphere. This is surrounded by a transition region of rapidly increasing temperatures, and finally by the super-heated corona. At the center of the Sun is the core region, a volume of sufficient temperature and pressure for nuclear fusion to occur. Above the core is the radiation zone, where the plasma conveys the energy flux by means of radiation. Above that are the outer layers that form a convection zone where the gas material transports energy primarily through physical displacement of the gas. It is believed that this convection zone creates the magnetic activity that generates sunspots. A solar wind of plasma particles constantly streams outward from the Sun until, at the outermost limit of the Solar System, it reaches the heliopause. This solar wind interacts with the magnetosphere of the Earth to create the Van Allen radiation belts about the Earth, as well as the aurora where the lines of the Earth's magnetic field descend into the atmosphere.    Planetary science   Planetary science is the study of the assemblage of planets, moons, dwarf planets, comets, asteroids, and other bodies orbiting the Sun, as well as extrasolar planets. The Solar System has been relatively well-studied, initially through telescopes and then later by spacecraft. This has provided a good overall understanding of the formation and evolution of this planetary system, although many new discoveries are still being made.  The Solar System is subdivided into the inner planets, the asteroid belt, and the outer planets. The inner terrestrial planets consist of Mercury, Venus, Earth, and Mars. The outer gas giant planets are Jupiter, Saturn, Uranus, and Neptune. Beyond Neptune lies the Kuiper Belt, and finally the Oort Cloud, which may extend as far as a light-year. The planets were formed in the protoplanetary disk that surrounded the early Sun. Through a process that included gravitational attraction, collision, and accretion, the disk formed clumps of matter that, with time, became protoplanets. The radiation pressure of the solar wind then expelled most of the unaccreted matter, and only those planets with sufficient mass retained their gaseous atmosphere. The planets continued to sweep up, or eject, the remaining matter during a period of intense bombardment, evidenced by the many impact craters on the Moon. During this period, some of the protoplanets may have collided, the leading hypothesis for how the Moon was formed. Once a planet reaches sufficient mass, the materials of different densities segregate within, during planetary differentiation. This process can form a stony or metallic core, surrounded by a mantle and an outer surface. The core may include solid and liquid regions, and some planetary cores generate their own magnetic field, which can protect their atmospheres from solar wind stripping. A planet or moon's interior heat is produced from the collisions that created the body, radioactive materials (e.g. uranium, thorium, and 26Al), or tidal heating. Some planets and moons accumulate enough heat to drive geologic processes such as volcanism and tectonics. Those that accumulate or retain an atmosphere can also undergo surface erosion from wind or water. Smaller bodies, without tidal heating, cool more quickly; and their geological activity ceases with the exception of impact cratering.    Stellar astronomy   The study of stars and stellar evolution is fundamental to our understanding of the universe. The astrophysics of stars has been determined through observation and theoretical understanding; and from computer simulations of the interior. Star formation occurs in dense regions of dust and gas, known as giant molecular clouds. When destabilized, cloud fragments can collapse under the influence of gravity, to form a protostar. A sufficiently dense, and hot, core region will trigger nuclear fusion, thus creating a main-sequence star. Almost all elements heavier than hydrogen and helium were created inside the cores of stars. The characteristics of the resulting star depend primarily upon its starting mass. The more massive the star, the greater its luminosity, and the more rapidly it expends the hydrogen fuel in its core. Over time, this hydrogen fuel is completely converted into helium, and the star begins to evolve. The fusion of helium requires a higher core temperature, so that the star both expands in size, and increases in core density. The resulting red giant enjoys a brief life span, before the helium fuel is in turn consumed. Very massive stars can also undergo a series of decreasing evolutionary phases, as they fuse increasingly heavier elements. The final fate of the star depends on its mass, with stars of mass greater than about eight times the Sun becoming core collapse supernovae; while smaller stars form a white dwarf as it ejects matter that forms a planetary nebulae. The remnant of a supernova is a dense neutron star, or, if the stellar mass was at least three times that of the Sun, a black hole. Close binary stars can follow more complex evolutionary paths, such as mass transfer onto a white dwarf companion that can potentially cause a supernova. Planetary nebulae and supernovae are necessary for the distribution of metals to the interstellar medium; without them, all new stars (and their planetary systems) would be formed from hydrogen and helium alone.    Galactic astronomy   Our solar system orbits within the Milky Way, a barred spiral galaxy that is a prominent member of the Local Group of galaxies. It is a rotating mass of gas, dust, stars and other objects, held together by mutual gravitational attraction. As the Earth is located within the dusty outer arms, there are large portions of the Milky Way that are obscured from view. In the center of the Milky Way is the core, a bar-shaped bulge with what is believed to be a supermassive black hole at the center. This is surrounded by four primary arms that spiral from the core. This is a region of active star formation that contains many younger, population I stars. The disk is surrounded by a spheroid halo of older, population II stars, as well as relatively dense concentrations of stars known as globular clusters. Between the stars lies the interstellar medium, a region of sparse matter. In the densest regions, molecular clouds of molecular hydrogen and other elements create star-forming regions. These begin as a compact pre-stellar core or dark nebulae, which concentrate and collapse (in volumes determined by the Jeans length) to form compact protostars. As the more massive stars appear, they transform the cloud into an H II region (ionized atomic hydrogen) of glowing gas and plasma. The stellar wind and supernova explosions from these stars eventually cause the cloud to disperse, often leaving behind one or more young open clusters of stars. These clusters gradually disperse, and the stars join the population of the Milky Way. Kinematic studies of matter in the Milky Way and other galaxies have demonstrated that there is more mass than can be accounted for by visible matter. A dark matter halo appears to dominate the mass, although the nature of this dark matter remains undetermined.    Extragalactic astronomy   The study of objects outside our galaxy is a branch of astronomy concerned with the formation and evolution of Galaxies; their morphology (description) and classification; and the observation of active galaxies, and at a larger scale, the groups and clusters of galaxies. Finally, the latter is important for the understanding of the large-scale structure of the cosmos. Most galaxies are organized into distinct shapes that allow for classification schemes. They are commonly divided into spiral, elliptical and Irregular galaxies. As the name suggests, an elliptical galaxy has the cross-sectional shape of an ellipse. The stars move along random orbits with no preferred direction. These galaxies contain little or no interstellar dust; few star-forming regions; and generally older stars. Elliptical galaxies are more commonly found at the core of galactic clusters, and may have been formed through mergers of large galaxies. A spiral galaxy is organized into a flat, rotating disk, usually with a prominent bulge or bar at the center, and trailing bright arms that spiral outward. The arms are dusty regions of star formation where massive young stars produce a blue tint. Spiral galaxies are typically surrounded by a halo of older stars. Both the Milky Way and our nearest galaxy neighbor, the Andromeda Galaxy, are spiral galaxies. Irregular galaxies are chaotic in appearance, and are neither spiral nor elliptical. About a quarter of all galaxies are irregular, and the peculiar shapes of such galaxies may be the result of gravitational interaction. An active galaxy is a formation that emits a significant amount of its energy from a source other than its stars, dust and gas. It is powered by a compact region at the core, thought to be a super-massive black hole that is emitting radiation from in-falling material. A radio galaxy is an active galaxy that is very luminous in the radio portion of the spectrum, and is emitting immense plumes or lobes of gas. Active galaxies that emit shorter frequency, high-energy radiation include Seyfert galaxies, Quasars, and Blazars. Quasars are believed to be the most consistently luminous objects in the known universe. The large-scale structure of the cosmos is represented by groups and clusters of galaxies. This structure is organized into a hierarchy of groupings, with the largest being the superclusters. The collective matter is formed into filaments and walls, leaving large voids between.    Cosmology   Cosmology (from the Greek κόσμος (kosmos) \"world, universe\" and λόγος (logos) \"word, study\" or literally \"logic\") could be considered the study of the universe as a whole. Observations of the large-scale structure of the universe, a branch known as physical cosmology, have provided a deep understanding of the formation and evolution of the cosmos. Fundamental to modern cosmology is the well-accepted theory of the big bang, wherein our universe began at a single point in time, and thereafter expanded over the course of 13.8 billion years to its present condition. The concept of the big bang can be traced back to the discovery of the microwave background radiation in 1965. In the course of this expansion, the universe underwent several evolutionary stages. In the very early moments, it is theorized that the universe experienced a very rapid cosmic inflation, which homogenized the starting conditions. Thereafter, nucleosynthesis produced the elemental abundance of the early universe. (See also nucleocosmochronology.) When the first neutral atoms formed from a sea of primordial ions, space became transparent to radiation, releasing the energy viewed today as the microwave background radiation. The expanding universe then underwent a Dark Age due to the lack of stellar energy sources. A hierarchical structure of matter began to form from minute variations in the mass density of space. Matter accumulated in the densest regions, forming clouds of gas and the earliest stars, the Population III stars. These massive stars triggered the reionization process and are believed to have created many of the heavy elements in the early universe, which, through nuclear decay, create lighter elements, allowing the cycle of nucleosynthesis to continue longer. Gravitational aggregations clustered into filaments, leaving voids in the gaps. Gradually, organizations of gas and dust merged to form the first primitive galaxies. Over time, these pulled in more matter, and were often organized into groups and clusters of galaxies, then into larger-scale superclusters. Fundamental to the structure of the universe is the existence of dark matter and dark energy. These are now thought to be its dominant components, forming 96% of the mass of the universe. For this reason, much effort is expended in trying to understand the physics of these components.    Interdisciplinary studies  Astronomy and astrophysics have developed significant interdisciplinary links with other major scientific fields. Archaeoastronomy is the study of ancient or traditional astronomies in their cultural context, utilizing archaeological and anthropological evidence. Astrobiology is the study of the advent and evolution of biological systems in the universe, with particular emphasis on the possibility of non-terrestrial life. Astrostatistics is the application of statistics to astrophysics to the analysis of vast amount of observational astrophysical data. The study of chemicals found in space, including their formation, interaction and destruction, is called astrochemistry. These substances are usually found in molecular clouds, although they may also appear in low temperature stars, brown dwarfs and planets. Cosmochemistry is the study of the chemicals found within the Solar System, including the origins of the elements and variations in the isotope ratios. Both of these fields represent an overlap of the disciplines of astronomy and chemistry. As \"forensic astronomy\", finally, methods from astronomy have been used to solve problems of law and history.    Amateur astronomy   Astronomy is one of the sciences to which amateurs can contribute the most. Collectively, amateur astronomers observe a variety of celestial objects and phenomena sometimes with equipment that they build themselves. Common targets of amateur astronomers include the Moon, planets, stars, comets, meteor showers, and a variety of deep-sky objects such as star clusters, galaxies, and nebulae. Astronomy clubs are located throughout the world and many have programs to help their members set up and complete observational programs including those to observe all the objects in the Messier (110 objects) or Herschel 400 catalogues of points of interest in the night sky. One branch of amateur astronomy, amateur astrophotography, involves the taking of photos of the night sky. Many amateurs like to specialize in the observation of particular objects, types of objects, or types of events which interest them. Most amateurs work at visible wavelengths, but a small minority experiment with wavelengths outside the visible spectrum. This includes the use of infrared filters on conventional telescopes, and also the use of radio telescopes. The pioneer of amateur radio astronomy was Karl Jansky, who started observing the sky at radio wavelengths in the 1930s. A number of amateur astronomers use either homemade telescopes or use radio telescopes which were originally built for astronomy research but which are now available to amateurs (e.g. the One-Mile Telescope). Amateur astronomers continue to make scientific contributions to the field of astronomy and it is one of the few scientific disciplines where amateurs can still make significant contributions. Amateurs can make occultation measurements that are used to refine the orbits of minor planets. They can also discover comets, and perform regular observations of variable stars. Improvements in digital technology have allowed amateurs to make impressive advances in the field of astrophotography.    Unsolved problems in astronomy   Although the scientific discipline of astronomy has made tremendous strides in understanding the nature of the universe and its contents, there remain some important unanswered questions. Answers to these may require the construction of new ground- and space-based instruments, and possibly new developments in theoretical and experimental physics. What is the origin of the stellar mass spectrum? That is, why do astronomers observe the same distribution of stellar masses – the initial mass function – apparently regardless of the initial conditions? A deeper understanding of the formation of stars and planets is needed. Is there other life in the Universe? Especially, is there other intelligent life? If so, what is the explanation for the Fermi paradox? The existence of life elsewhere has important scientific and philosophical implications. Is the Solar System normal or atypical? What caused the Universe to form? Is the premise of the Fine-tuned universe hypothesis correct? If so, could this be the result of cosmological natural selection? What caused the cosmic inflation that produced our homogeneous universe? Why is there a baryon asymmetry? What is the nature of dark matter and dark energy? These dominate the evolution and fate of the cosmos, yet their true nature remains unknown. What will be the ultimate fate of the universe? How did the first galaxies form? How did supermassive black holes form? What is creating the ultra-high-energy cosmic rays? Why is the abundance of lithium in the cosmos four times lower than predicted by the standard Big Bang model? What really happens beyond the event horizon?    See also     References     Bibliography  Forbes, George (1909). History of Astronomy. London: Plain Label Books. ISBN 1-60303-159-6.  Available at Project Gutenberg,Google books Harpaz, Amos (1994). Stellar Evolution. A K Peters, Ltd. ISBN 978-1-56881-012-6.  Unsöld, A.; Baschek, B. (2001). The New Cosmos: An Introduction to Astronomy and Astrophysics. Springer. ISBN 3-540-67877-8.     External links  International Year of Astronomy 2009 IYA2009 Main website Cosmic Journey: A History of Scientific Cosmology from the American Institute of Physics Southern Hemisphere Astronomy Celestia Motherlode Educational site for Astronomical journeys through space Prof. Sir Harry Kroto, NL, Astrophysical Chemistry Lecture Series. 8 Freeview Lectures provided by the Vega Science Trust. Core books and core journals in Astronomy, from the Smithsonian/NASA Astrophysics Data System A Journey with Fred Hoyle: Second Edition, by Chandra Wickramasinghe. Early astronomy books from the History of Science Collection at the Linda Hall Library","label":"foo"},{"text":"","label":"foo"},{"text":"Natural philosophy or the philosophy of nature (from Latin philosophia naturalis) was the philosophical study of nature and the physical universe that was dominant before the development of modern science. It is considered to be the precursor of natural sciences. From the ancient world, starting with Aristotle, to the 19th century, the term \"natural philosophy\" was the common term used to describe the practice of studying nature. It was in the 19th century that the concept of \"science\" received its modern shape with new titles emerging such as \"biology\" and \"biologist\", \"physics\" and \"physicist\" among other technical fields and titles; institutions and communities were founded, and unprecedented applications to and interactions with other aspects of society and culture occurred. Isaac Newton's book Philosophiae Naturalis Principia Mathematica (1687), whose title translates to \"Mathematical Principles of Natural Philosophy\", reflects the then-current use of the words \"natural philosophy\", akin to \"systematic study of nature\". Even in the 19th century, a treatise by Lord Kelvin and Peter Guthrie Tait, which helped define much of modern physics, was titled Treatise on Natural Philosophy (1867). In the German tradition, naturphilosophie or nature philosophy persisted into the 18th and 19th century as an attempt to achieve a speculative unity of nature and spirit. Some of the greatest names in German philosophy are associated with this movement, including Spinoza, Goethe, Hegel and Schelling.    Origin and evolution of the termEdit  The term natural philosophy preceded our current natural science (i.e. empirical science). Empirical science historically developed out of philosophy or, more specifically, natural philosophy. Natural philosophy was distinguished from the other precursor of modern science, natural history, in that natural philosophy involved reasoning and explanations about nature (and after Galileo, quantitative reasoning), whereas natural history was essentially qualitative and descriptive. In the 14th and 15th centuries, natural philosophy' was one of many branches of philosophy, but was not a specialized field of study. The first person appointed as a specialist in Natural Philosophy per se was Jacopo Zabarella, at the University of Padua in 1577. Modern meanings of the terms science and scientists date only to the 19th century. Before that, science was a synonym for knowledge or study, in keeping with its Latin origin. The term gained its modern meaning when experimental science and the scientific method became a specialized branch of study apart from natural philosophy. From the mid-19th century, when it became increasingly unusual for scientists to contribute to both physics and chemistry, \"natural philosophy\" came to mean just physics, and the word is still used in that sense in degree titles at the University of Oxford. In general, chairs of Natural Philosophy established long ago at the oldest universities, are nowadays occupied mainly by physics professors. Isaac Newton's book Philosophiae Naturalis Principia Mathematica (1687), whose title translates to \"Mathematical Principles of Natural Philosophy\", reflects the then-current use of the words \"natural philosophy\", akin to \"systematic study of nature\". Even in the 19th century, a treatise by Lord Kelvin and Peter Guthrie Tait, which helped define much of modern physics, was titled Treatise on Natural Philosophy (1867).    Scope of natural philosophyEdit  In Plato's earliest known dialogue, Charmides distinguishes between science or bodies of knowledge that produce a physical result, and those that do not. Natural philosophy has been categorized as a theoretical rather than a practical branch of philosophy (like ethics). Sciences that guide arts and draw on the philosophical knowledge of nature may produce practical results, but these subsidiary sciences (e.g., architecture or medicine) go beyond natural philosophy. The study of natural philosophy seeks to explore the cosmos by any means necessary to understand the universe. Some ideas presuppose that change is a reality. Although this may seem obvious, there have been some philosophers who have denied the concept of metamorphosis, such as Plato's predecessor Parmenides and later Greek philosopher Sextus Empiricus, and perhaps some Eastern philosophers. George Santayana, in his Scepticism and Animal Faith, attempted to show that the reality of change cannot be proven. If his reasoning is sound, it follows that to be a physicist, one must restrain one's skepticism enough to trust one's senses, or else rely on anti-realism. René Descartes' metaphysical system of Cartesian Dualism describes two kinds of substance: matter and mind. According to this system, everything that is \"matter\" is deterministic and natural—and so belongs to natural philosophy—and everything that is \"mind\" is volitional and non-natural, and falls outside the domain of philosophy of nature.    Branches and subject matter of natural philosophyEdit  Major branches of natural philosophy include astronomy and cosmology, the study of nature on the grand scale; etiology, the study of (intrinsic and sometimes extrinsic) causes; the study of chance, probability and randomness; the study of elements; the study of the infinite and the unlimited (virtual or actual); the study of matter; mechanics, the study of translation of motion and change; the study of nature or the various sources of actions; the study of natural qualities; the study of physical quantities; the study of relations between physical entities; and the philosophy of space and time. (Adler, 1993)    History of natural philosophyEdit   Humankind's mental engagement with nature certainly predates civilization and the record of history. Philosophical, and specifically non-religious thought about the natural world, goes back to ancient Greece. These lines of thought began before Socrates, who turned from his philosophical studies from speculations about nature to a consideration of man, viz., political philosophy. The thought of early philosophers such Parmenides, Heraclitus, and Democritus centered on the natural world. Plato followed Socrates in concentrating on man. It was Plato's student, Aristotle, who, in basing his thought on the natural world, returned empiricism to its primary place, while leaving room in the world for man. Martin Heidegger observes that Aristotle was the originator of conception of nature that prevailed in the Middle Ages into the modern era:  The Physics is a lecture in which he seeks to determine beings that arise on their own, τὰ φύσει ὄντα, with regard to their being. Aristotelian \"physics\" is different from what we mean today by this word, not only to the extent that it belongs to antiquity whereas the modern physical sciences belong to modernity, rather above all it is different by virtue of the fact that Aristotle's \"physics\" is philosophy, whereas modern physics is a positive science that presupposes a philosophy.... This book determines the warp and woof of the whole of Western thinking, even at that place where it, as modern thinking, appears to think at odds with ancient thinking. But opposition is invariably comprised of a decisive, and often even perilous, dependence. Without Aristotle's Physics there would have been no Galileo.  Aristotle surveyed the thought of his predecessors and conceived of nature in a way that charted a middle course between their excesses.  Plato's world of eternal and unchanging Forms, imperfectly represented in matter by a divine Artisan, contrasts sharply with the various mechanistic Weltanschauungen, of which atomism was, by the fourth century at least, the most prominent… This debate was to persist throughout the ancient world. Atomistic mechanism got a shot in the arm from Epicurus… while the Stoics adopted a divine teleology… The choice seems simple: either show how a structured, regular world could arise out of undirected processes, or inject intelligence into the system. This was how Aristotle… when still a young acolyte of Plato, saw matters. Cicero… preserves Aristotle's own cave-image: if troglodytes were brought on a sudden into the upper world, they would immediately suppose it to have been intelligently arranged. But Aristotle grew to abandon this view; although he believes in a divine being, the Prime Mover is not the efficient cause of action in the Universe, and plays no part in constructing or arranging it... But, although he rejects the divine Artificer, Aristotle does not resort to a pure mechanism of random forces. Instead he seeks to find a middle way between the two positions, one which relies heavily on the notion of Nature, or phusis.  \"The world we inhabit is an orderly one, in which things generally behave in predictable ways, Aristotle argued, because every natural object has a \"nature\"-an attribute (associated primarily with form)that makes the object behave in its customary fashion...\" Aristotle recommended four causes as appropriate for the business of the natural philosopher, or physicist, “and if he refers his problems back to all of them, he will assign the ‘why’ in the way proper to his science—the matter, the form, the mover, [and] ‘that for the sake of which’”. While the vagrancies of the material cause are subject to circumstance, the formal, efficient and final cause often coincide because in natural kinds, the mature form and final cause are one and the same. The capacity to mature into a specimen of one's kind is directly acquired from “the primary source of motion”, i.e., from one's father, whose seed (sperma) conveys the essential nature (common to the species), as a hypothetical ratio. Material cause   An object's motion will behave in different ways depending on the [substance/essence] from which it is made. (Compare clay, steel, etc.) Formal cause   An object's motion will behave in different ways depending on its material arrangement. (Compare a clay sphere, clay block, etc.) Efficient cause  That which caused the object to come into being; an \"agent of change\" or an \"agent of movement\". Final cause   The reason that caused the object to be brought into existence. From the late Middle Ages into the modern era, the tendency has been to narrow \"science\" to the consideration of efficient or agency-based causes of a particular kind:  The action of an efficient cause may sometimes, but not always, be described in terms of quantitative force. The action of an artist on a block of clay, for instance, can be described in terms of how many pounds of pressure per square inch is exerted on it. The efficient causality of the teacher in directing the activity of the artist, however, cannot be so described… The final cause acts on the agent to influence or induce her to act. If the artist works \"to make money,\" making money is in some way the cause of her action. But we cannot describe this influence in terms of quantitative force. The final cause acts, but it acts according to the mode of final causality, as an end or good that induces the efficient cause to act. The mode of causality proper to the final cause cannot itself be reduced to efficient causality, much less to the mode of efficient causality we call \"force.\"    Medieval philosophy of motionEdit  Medieval thoughts on motion involved much of Aristotle's works Physics and Metaphysics. The issue that medieval philosophers had with motion was the inconsistency found between book 3 of Physics and book 5 of Metaphysics. Aristotle claimed in book 3 of Physics that motion can be categorized by substance, quantity, quality, and place. where in book 5 of Metaphysics he stated that motion is a magnitude of quantity. This disputation led to some important questions to natural philosophers: Which category/categories does motion fit into? Is motion the same thing as a terminus? Is motion separate from real things? These questions asked by medieval philosophers tried to classify motion. William Ockham gives a good concept of motion for many people in the Middle Ages. There is an issue with the vocabulary behind motion which makes people think that there is a correlation between nouns and the qualities that make nouns. Ockham states that this distinction is what will allow people to understand motion, that motion is a property of mobiles, locations, and forms and that is all that is required to define what motion is. A famous example of this is Occam's razor which simplifies vague statements by cutting them into more descriptive examples. \"Every motion derives from an agent.\" becomes \"each thing that is moved, is moved by an agent\" this makes motion a more personal quality referring to individual objects that are moved.    Aristotle's philosophy of natureEdit   \"An acorn is potentially, but not actually, an oak tree. In becoming an oak tree, it becomes actually what it originally was only potentially. this change thus involves passage from potentiality to actuality-not from nonbeing to being but from one kind or degree to being another\"  Aristotle held many important beliefs that started a convergence of thought for natural philosophy. Aristotle believed that attributes of objects belong to the objects themselves, and share traits with other objects that fit them into a category. He uses the example of dogs to press this point an individual dog (ex. one dog can be black and another brown) may have very specific attributes(ex. one dog can be black and another brown,) but also very general ones that classify it as a dog (ex. four legged). This philosophy can be applied to many other objects as well. This idea is different than that of Plato, with whom Aristotle had a direct association. Aristotle argued that objects have properties \"form\" and something that is not part of its properties \"matter\" that defines the object. The form cannot be separated from the matter. Giving the example that you can not separate properties and matter since this is impossible, you cannot collect properties in a pile and matter in another. Aristotle believed that change was a natural occurrence. He used his philosophy of form and matter to argue that when something changes you change its properties with out changing its matter. This change happens but replacing certain properties with other properties. Since this change is always an intentional alteration whether by forced means or but natural ones, change is a controllable order of qualities. He argues that this happens through three categories of being; nonbeing, potential being, and actual being. through these three states the process of changing an object never truly destroys an objects forms during this transition state just blurs the reality between the two states. An example of this could be changing an object from red to blue with a transitional purple phase.    Other significant figures in natural philosophyEdit  Early Greek Philosophers studied motion and the cosmos. Figures like Hesiod regarded the Natural world as offspring of the gods, where others like Leucippus and Democritus regarded to world as lifeless atoms in a vortex. Anaximander deduced that eclipses happen because apertures in rings of celestial fire. Heraclitus believed that the heavenly bodies were made of fire that were contained within bowls, he thought that eclipses happen when the bowl turned away from the earth. Anaximenes is believed to have stated that an underlying element was air, and by manipulating air someone could change its thickness to create fire, water, dirt, and stones. Empedocles identified the elements that make up the world which he termed the roots of all things as Fire, Air. Earth, and Water. Parmenides argued that all change is a logical impossibility. He gives the example that nothing can go from nonexistence to existence. Plato argues that the world is an imperfect replica of an idea that a divine craftsman once held. He also believed that the only way to truly know something was through reason and logic not the study of the object its self, but that changeable matter is a viable course of study. The scientific method has ancient precedents and Galileo exemplifies a mathematical understanding of nature which is the hallmark of modern natural scientists. Galileo proposed that objects falling regardless of their mass would fall at the same rate, as long as the medium they fall in is identical. The 19th-century distinction of a scientific enterprise apart from traditional natural philosophy has its roots in prior centuries. Proposals for a more \"inquisitive\" and practical approach to the study of nature are notable in Francis Bacon, whose ardent convictions did much to popularize his insightful Baconian method. The late 17th-century natural philosopher Robert Boyle wrote a seminal work on the distinction between physics and metaphysics called, A Free Enquiry into the Vulgarly Received Notion of Nature, as well as The Skeptical Chymist, after which the modern science of chemistry is named, (as distinct from proto-scientific studies of alchemy). These works of natural philosophy are representative of a departure from the medieval scholasticism taught in European universities, and anticipate in many ways, the developments which would lead to science as practiced in the modern sense. As Bacon would say, \"vexing nature\" to reveal \"her\" secrets, (scientific experimentation), rather than a mere reliance on largely historical, even anecdotal, observations of empirical phenomena, would come to be regarded as a defining characteristic of modern science, if not the very key to its success. Boyle's biographers, in their emphasis that he laid the foundations of modern chemistry, neglect how steadily he clung to the scholastic sciences in theory, practice and doctrine. However, he meticulously recorded observational detail on practical research, and subsequently advocated not only this practice, but its publication, both for successful and unsuccessful experiments, so as to validate individual claims by replication.  The modern emphasis is less on a broad empiricism (one that includes passive observation of nature's activity), but on a narrow conception of the empirical concentrating on the control exercised through experimental (active) observation for the sake of control of nature. Nature is reduced to a passive recipient of human activity.    Current work in natural philosophyEdit  In the middle of the 20th century, Ernst Mayr's discussions on the teleology of nature brought up issues that were dealt with previously by Aristotle (regarding final cause) and Kant (regarding reflective judgment). Especially since the mid-20th-century European crisis, some thinkers argued the importance of looking at nature from a broad philosophical perspective, rather than what they considered a narrowly positivist approach relying implicitly on a hidden, unexamined philosophy. One line of thought grows from the Aristotelian tradition, especially as developed by Thomas Aquinas. Another line springs from Edmund Husserl, especially as expressed in The Crisis of European Sciences. Students of his such as Jacob Klein and Hans Jonas more fully developed his themes. Last but not least, there is the process philosophy inspired by Alfred North Whitehead's works. Among living scholars, Brian David Ellis, Nancy Cartwright, David Oderberg, and John Dupré are some of the more prominent thinkers who can arguably be classed as generally adopting a more open approach to the natural world. Ellis (2002) observes the rise of a \"New Essentialism.\" David Oderberg (2007) takes issue with other philosophers, including Ellis to a degree, who claim to be essentialists. He revives and defends the Thomistic-Aristotelian tradition from modern attempts to flatten nature to the limp subject of the experimental method.    See alsoEdit   Environmental philosophy Gentleman scientist History of science Natural environment Natural theology Naturalism (philosophy) Nature (philosophy)    ReferencesEdit     Further readingEdit  Adler, Mortimer J. (1993). The Four Dimensions of Philosophy: Metaphysical, Moral, Objective, Categorical. Macmillan. ISBN 0-02-500574-X.  E.A. Burtt, Metaphysical Foundations of Modern Science (Garden City, NY: Doubleday and Company, 1954). Philip Kitcher, Science, Truth, and Democracy. Oxford Studies in Philosophy of Science. Oxford; New York: Oxford University Press, 2001. LCCN:2001036144 ISBN 0-19-514583-6 Bertrand Russell, A History of Western Philosophy and Its Connection with Political and Social Circumstances from the Earliest Times to the Present Day (1945) Simon & Schuster, 1972. Santayana, George (1923). Scepticism and Animal Faith. Dover Publications. pp. 27–41. ISBN 0-486-20236-4.  David Snoke, Natural Philosophy: A Survey of Physics and Western Thought. Access Research Network, 2003. ISBN 1-931796-25-4.[1] [2] Nancy R. Pearcey and Charles B. Thaxton, The Soul of Science: Christian Faith and Natural Philosophy (Crossway Books, 1994, ISBN 0891077669).    External linksEdit  \"Aristotle's Natural Philosophy\", Stanford Encyclopedia of Philosophy Institute for the Study of Nature \"A Bigger Physics,\" a talk at MIT by Michael Augros Other articles","label":"foo"},{"text":"Chemistry is a branch of physical science that studies the composition, structure, properties and change of matter. Chemistry deals with such topics as the properties of individual atoms, how atoms form chemical bonds to create chemical compounds, the interactions of substances through intermolecular forces that give matter its general properties, and the interactions between substances through chemical reactions to form different substances. Chemistry is sometimes called the central science because it bridges other natural sciences, including physics, geology and biology. For the differences between chemistry and physics see Comparison of chemistry and physics. Scholars disagree about the etymology of the word chemistry. The history of chemistry can be traced to alchemy, which had been practiced for several millennia in various parts of the world.    Etymology  The word chemistry comes from the word alchemy, an earlier set of practices that encompassed elements of chemistry, metallurgy, philosophy, astrology, astronomy, mysticism and medicine; it is commonly thought of as the quest to turn lead or another common starting material into gold. Alchemy, which was practiced around 330, is the study of the composition of waters, movement, growth, embodying, disembodying, drawing the spirits from bodies and bonding the spirits within bodies (Zosimos). An alchemist was called a 'chemist' in popular speech, and later the suffix \"-ry\" was added to this to describe the art of the chemist as \"chemistry\". The word alchemy in turn is derived from the Arabic word al-kīmīā (الکیمیاء). In origin, the term is borrowed from the Greek χημία or χημεία. This may have Egyptian origins. Many believe that al-kīmīā is derived from the Greek χημία, which is in turn derived from the word Chemi or Kimi, which is the ancient name of Egypt in Egyptian. Alternately, al-kīmīā may derive from χημεία, meaning \"cast together\".    Definition  In retrospect, the definition of chemistry has changed over time, as new discoveries and theories add to the functionality of the science. The term \"chymistry\", in the view of noted scientist Robert Boyle in 1661, meant the subject of the material principles of mixed bodies. In 1663 the chemist Christopher Glaser described \"chymistry\" as a scientific art, by which one learns to dissolve bodies, and draw from them the different substances on their composition, and how to unite them again, and exalt them to a higher perfection. The 1730 definition of the word \"chemistry\", as used by Georg Ernst Stahl, meant the art of resolving mixed, compound, or aggregate bodies into their principles; and of composing such bodies from those principles. In 1837, Jean-Baptiste Dumas considered the word \"chemistry\" to refer to the science concerned with the laws and effects of molecular forces. This definition further evolved until, in 1947, it came to mean the science of substances: their structure, their properties, and the reactions that change them into other substances - a characterization accepted by Linus Pauling. More recently, in 1998, Professor Raymond Chang broadened the definition of \"chemistry\" to mean the study of matter and the changes it undergoes.    History   Early civilizations, such as the Egyptians Babylonians, Indians amassed practical knowledge concerning the arts of metallurgy, pottery and dyes, but didn't develop a systematic theory. A basic chemical hypothesis first emerged in Classical Greece with the theory of four elements as propounded definitively by Aristotle stating that that fire, air, earth and water were the fundamental elements from which everything is formed as a combination. Greek atomism dates back to 440 BC, arising in works by philosophers such as Democritus and Epicurus. In 50 BC, the Roman philosopher Lucretius expanded upon the theory in his book De rerum natura (On The Nature of Things). Unlike modern concepts of science, Greek atomism was purely philosophical in nature, with little concern for empirical observations and no concern for chemical experiments. In the Hellenistic world the art of alchemy first proliferated, mingling magic and occultism into the study of natural substances with the ultimate goal of transmuting elements into gold and discovering the elixir of eternal life. Alchemy was discovered and practised widely throughout the Arab world after the Muslim conquests, and from there, diffused into medieval and Renaissance Europe through Latin translations.    Chemistry as science  Under the influence of the new empirical methods propounded by Sir Francis Bacon and others, a group of chemists at Oxford, Robert Boyle, Robert Hooke and John Mayow began to reshape the old alchemical traditions into a scientific discipline. Boyle in particular is regarded as the founding father of chemistry due to his most important work, the classic chemistry text The Sceptical Chymist where the differentiation is made between the claims of alchemy and the empirical scientific discoveries of the new chemistry. He formulated Boyle's law, rejected the classical \"four elements\" and proposed a mechanistic alternative of atoms and chemical reactions that could be subject to rigorous experiment.  The theory of phlogiston (a substance at the root of all combustion) was propounded by the German Georg Ernst Stahl in the early 18th century and was only overturned by the end of the century by the French chemist Antoine Lavoisier, the chemical analogue of Newton in physics; who did more than any other to establish the new science on proper theoretical footing, by elucidating the principle of conservation of mass and developing a new system of chemical nomenclature used to this day. Prior to his work, though, many important discoveries had been made, specifically relating to the nature of 'air' which was discovered to be composed of many different gases. The Scottish chemist Joseph Black (the first experimental chemist) and the Dutchman J. B. van Helmont discovered carbon dioxide, or what Black called 'fixed air' in 1754; Henry Cavendish discovered hydrogen and elucidated its properties and Joseph Priestley and, independently, Carl Wilhelm Scheele isolated pure oxygen.  English scientist John Dalton proposed the modern theory of atoms; that all substances are composed of indivisible 'atoms' of matter and that different atoms have varying atomic weights. The development of the electrochemical theory of chemical combinations occurred in the early 19th century as the result of the work of two scientists in particular, J. J. Berzelius and Humphry Davy, made possible by the prior invention of the voltaic pile by Alessandro Volta. Davy discovered nine new elements including the alkali metals by extracting them from their oxides with electric current. British William Prout first proposed ordering all the elements by their atomic weight as all atoms had a weight that was an exact multiple of the atomic weight of hydrogen. J. A. R. Newlands devised an early table of elements, which was then developed into the modern periodic table of elements in the 1860s by Dmitri Mendeleev and independently by several other scientists including Julius Lothar Meyer. The inert gases, later called the noble gases were discovered by William Ramsay in collaboration with Lord Rayleigh at the end of the century, thereby filling in the basic structure of the table. Organic chemistry was developed by Justus von Liebig and others, following Friedrich Wöhler's synthesis of urea which proved that living organisms were, in theory, reducible to chemistry. Other crucial 19th century advances were; an understanding of valence bonding (Edward Frankland in 1852) and the application of thermodynamics to chemistry (J. W. Gibbs and Svante Arrhenius in the 1870s).    Chemical structure   At the turn of the twentieth century the theoretical underpinnings of chemistry were finally understood due to a series of remarkable discoveries that succeeded in probing and discovering the very nature of the internal structure of atoms. In 1897, J. J. Thomson of Cambridge University discovered the electron and soon after the French scientist Becquerel as well as the couple Pierre and Marie Curie investigated the phenomenon of radioactivity. In a series of pioneering scattering experiments Ernest Rutherford at the University of Manchester discovered the internal structure of the atom and the existence of the proton, classified and explained the different types of radioactivity and successfully transmuted the first element by bombarding nitrogen with alpha particles. His work on atomic structure was improved on by his students, the Danish physicist Niels Bohr and Henry Moseley. The electronic theory of chemical bonds and molecular orbitals was developed by the American scientists Linus Pauling and Gilbert N. Lewis. The year 2011 was declared by the United Nations as the International Year of Chemistry. It was an initiative of the International Union of Pure and Applied Chemistry, and of the United Nations Educational, Scientific, and Cultural Organization and involves chemical societies, academics, and institutions worldwide and relied on individual initiatives to organize local and regional activities.    Principles of modern chemistry   The current model of atomic structure is the quantum mechanical model. Traditional chemistry starts with the study of elementary particles, atoms, molecules, substances, metals, crystals and other aggregates of matter. This matter can be studied in solid, liquid, or gas states, in isolation or in combination. The interactions, reactions and transformations that are studied in chemistry are usually the result of interactions between atoms, leading to rearrangements of the chemical bonds which hold atoms together. Such behaviors are studied in a chemistry laboratory. The chemistry laboratory stereotypically uses various forms of laboratory glassware. However glassware is not central to chemistry, and a great deal of experimental (as well as applied/industrial) chemistry is done without it. A chemical reaction is a transformation of some substances into one or more different substances. The basis of such a chemical transformation is the rearrangement of electrons in the chemical bonds between atoms. It can be symbolically depicted through a chemical equation, which usually involves atoms as subjects. The number of atoms on the left and the right in the equation for a chemical transformation is equal. (When the number of atoms on either side is unequal, the transformation is referred to as a nuclear reaction or radioactive decay.) The type of chemical reactions a substance may undergo and the energy changes that may accompany it are constrained by certain basic rules, known as chemical laws. Energy and entropy considerations are invariably important in almost all chemical studies. Chemical substances are classified in terms of their structure, phase, as well as their chemical compositions. They can be analyzed using the tools of chemical analysis, e.g. spectroscopy and chromatography. Scientists engaged in chemical research are known as chemists. Most chemists specialize in one or more sub-disciplines. Several concepts are essential for the study of chemistry; some of them are:    Matter   In chemistry, matter is defined as anything that has rest mass and volume (it takes up space) and is made up of particles. The particles that make up matter have rest mass as well - not all particles have rest mass, such as the photon. Matter can be a pure chemical substance or a mixture of substances.   = Atom =  The atom is the basic unit of chemistry. It consists of a dense core called the atomic nucleus surrounded by a space called the electron cloud. The nucleus is made up of positively charged protons and uncharged neutrons (together called nucleons), while the electron cloud consists of negatively charged electrons which orbit the nucleus. In a neutral atom, the negatively charged electrons balance out the positive charge of the protons. The nucleus is dense; the mass of a nucleon is 1,836 times that of an electron, yet the radius of an atom is about 10,000 times that of its nucleus. The atom is also the smallest entity that can be envisaged to retain the chemical properties of the element, such as electronegativity, ionization potential, preferred oxidation state(s), coordination number, and preferred types of bonds to form (e.g., metallic, ionic, covalent).   = Element =  A chemical element is a pure substance which is composed of a single type of atom, characterized by its particular number of protons in the nuclei of its atoms, known as the atomic number and represented by the symbol Z. The mass number is the sum of the number of protons and neutrons in a nucleus. Although all the nuclei of all atoms belonging to one element will have the same atomic number, they may not necessarily have the same mass number; atoms of an element which have different mass numbers are known as isotopes. For example, all atoms with 6 protons in their nuclei are atoms of the chemical element carbon, but atoms of carbon may have mass numbers of 12 or 13. The standard presentation of the chemical elements is in the periodic table, which orders elements by atomic number. The periodic table is arranged in groups, or columns, and periods, or rows. The periodic table is useful in identifying periodic trends.   = Compound =  A compound is a pure chemical substance composed of more than one element. The properties of a compound bear little similarity to those of its elements. The standard nomenclature of compounds is set by the International Union of Pure and Applied Chemistry (IUPAC). Organic compounds are named according to the organic nomenclature system. Inorganic compounds are named according to the inorganic nomenclature system. In addition the Chemical Abstracts Service has devised a method to index chemical substances. In this scheme each chemical substance is identifiable by a number known as its CAS registry number.   = Molecule =  A molecule is the smallest indivisible portion of a pure chemical substance that has its unique set of chemical properties, that is, its potential to undergo a certain set of chemical reactions with other substances. However, this definition only works well for substances that are composed of molecules, which is not true of many substances (see below). Molecules are typically a set of atoms bound together by covalent bonds, such that the structure is electrically neutral and all valence electrons are paired with other electrons either in bonds or in lone pairs. Thus, molecules exist as electrically neutral units, unlike ions. When this rule is broken, giving the \"molecule\" a charge, the result is sometimes named a molecular ion or a polyatomic ion. However, the discrete and separate nature of the molecular concept usually requires that molecular ions be present only in well-separated form, such as a directed beam in a vacuum in a mass spectrometer. Charged polyatomic collections residing in solids (for example, common sulfate or nitrate ions) are generally not considered \"molecules\" in chemistry.  The \"inert\" or noble gas elements (helium, neon, argon, krypton, xenon and radon) are composed of lone atoms as their smallest discrete unit, but the other isolated chemical elements consist of either molecules or networks of atoms bonded to each other in some way. Identifiable molecules compose familiar substances such as water, air, and many organic compounds like alcohol, sugar, gasoline, and the various pharmaceuticals. However, not all substances or chemical compounds consist of discrete molecules, and indeed most of the solid substances that make up the solid crust, mantle, and core of the Earth are chemical compounds without molecules. These other types of substances, such as ionic compounds and network solids, are organized in such a way as to lack the existence of identifiable molecules per se. Instead, these substances are discussed in terms of formula units or unit cells as the smallest repeating structure within the substance. Examples of such substances are mineral salts (such as table salt), solids like carbon and diamond, metals, and familiar silica and silicate minerals such as quartz and granite. One of the main characteristics of a molecule is its geometry often called its structure. While the structure of diatomic, triatomic or tetra atomic molecules may be trivial, (linear, angular pyramidal etc.) the structure of polyatomic molecules, that are constituted of more than six atoms (of several elements) can be crucial for its chemical nature.   = Substance and mixture = A chemical substance is a kind of matter with a definite composition and set of properties. A collection of substances is called a mixture. Examples of mixtures are air and alloys.   = Mole and amount of substance =  The mole is a unit of measurement that denotes an amount of substance (also called chemical amount). The mole is defined as the number of atoms found in exactly 0.012 kilogram (or 12 grams) of carbon-12, where the carbon-12 atoms are unbound, at rest and in their ground state. The number of entities per mole is known as the Avogadro constant, and is determined empirically to be approximately 6.022×1023 mol−1. Molar concentration is the amount of a particular substance per volume of solution, and is commonly reported in moldm−3.    Phase   In addition to the specific chemical properties that distinguish different chemical classifications, chemicals can exist in several phases. For the most part, the chemical classifications are independent of these bulk phase classifications; however, some more exotic phases are incompatible with certain chemical properties. A phase is a set of states of a chemical system that have similar bulk structural properties, over a range of conditions, such as pressure or temperature. Physical properties, such as density and refractive index tend to fall within values characteristic of the phase. The phase of matter is defined by the phase transition, which is when energy put into or taken out of the system goes into rearranging the structure of the system, instead of changing the bulk conditions. Sometimes the distinction between phases can be continuous instead of having a discrete boundary, in this case the matter is considered to be in a supercritical state. When three states meet based on the conditions, it is known as a triple point and since this is invariant, it is a convenient way to define a set of conditions. The most familiar examples of phases are solids, liquids, and gases. Many substances exhibit multiple solid phases. For example, there are three phases of solid iron (alpha, gamma, and delta) that vary based on temperature and pressure. A principal difference between solid phases is the crystal structure, or arrangement, of the atoms. Another phase commonly encountered in the study of chemistry is the aqueous phase, which is the state of substances dissolved in aqueous solution (that is, in water). Less familiar phases include plasmas, Bose–Einstein condensates and fermionic condensates and the paramagnetic and ferromagnetic phases of magnetic materials. While most familiar phases deal with three-dimensional systems, it is also possible to define analogs in two-dimensional systems, which has received attention for its relevance to systems in biology.    Bonding   Atoms sticking together in molecules or crystals are said to be bonded with one another. A chemical bond may be visualized as the multipole balance between the positive charges in the nuclei and the negative charges oscillating about them. More than simple attraction and repulsion, the energies and distributions characterize the availability of an electron to bond to another atom. A chemical bond can be a covalent bond, an ionic bond, a hydrogen bond or just because of Van der Waals force. Each of these kinds of bonds is ascribed to some potential. These potentials create the interactions which hold atoms together in molecules or crystals. In many simple compounds, valence bond theory, the Valence Shell Electron Pair Repulsion model (VSEPR), and the concept of oxidation number can be used to explain molecular structure and composition. An ionic bond is formed when a metal loses one or more of its electrons, becoming a positively charged cation, and the electrons are then gained by the non-metal atom, becoming a negatively charged anion. The two oppositely charged ions attract one another, and the ionic bond is the electrostatic force of attraction between them. For example, sodium (Na), a metal, loses one electron to become an Na+ cation while chlorine (Cl), a non-metal, gains this electron to become Cl−. The ions are held together due to electrostatic attraction, and that compound sodium chloride (NaCl), or common table salt, is formed.  In a covalent bond, one or more pairs of valence electrons are shared by two atoms: the resulting electrically neutral group of bonded atoms is termed a molecule. Atoms will share valence electrons in such a way as to create a noble gas electron configuration (eight electrons in their outermost shell) for each atom. Atoms that tend to combine in such a way that they each have eight electrons in their valence shell are said to follow the octet rule. However, some elements like hydrogen and lithium need only two electrons in their outermost shell to attain this stable configuration; these atoms are said to follow the duet rule, and in this way they are reaching the electron configuration of the noble gas helium, which has two electrons in its outer shell. Similarly, theories from classical physics can be used to predict many ionic structures. With more complicated compounds, such as metal complexes, valence bond theory is less applicable and alternative approaches, such as the molecular orbital theory, are generally used. See diagram on electronic orbitals.    Energy   In the context of chemistry, energy is an attribute of a substance as a consequence of its atomic, molecular or aggregate structure. Since a chemical transformation is accompanied by a change in one or more of these kinds of structures, it is invariably accompanied by an increase or decrease of energy of the substances involved. Some energy is transferred between the surroundings and the reactants of the reaction in the form of heat or light; thus the products of a reaction may have more or less energy than the reactants. A reaction is said to be exergonic if the final state is lower on the energy scale than the initial state; in the case of endergonic reactions the situation is the reverse. A reaction is said to be exothermic if the reaction releases heat to the surroundings; in the case of endothermic reactions, the reaction absorbs heat from the surroundings. Chemical reactions are invariably not possible unless the reactants surmount an energy barrier known as the activation energy. The speed of a chemical reaction (at given temperature T) is related to the activation energy E, by the Boltzmann's population factor  - that is the probability of a molecule to have energy greater than or equal to E at the given temperature T. This exponential dependence of a reaction rate on temperature is known as the Arrhenius equation. The activation energy necessary for a chemical reaction to occur can be in the form of heat, light, electricity or mechanical force in the form of ultrasound. A related concept free energy, which also incorporates entropy considerations, is a very useful means for predicting the feasibility of a reaction and determining the state of equilibrium of a chemical reaction, in chemical thermodynamics. A reaction is feasible only if the total change in the Gibbs free energy is negative, ; if it is equal to zero the chemical reaction is said to be at equilibrium. There exist only limited possible states of energy for electrons, atoms and molecules. These are determined by the rules of quantum mechanics, which require quantization of energy of a bound system. The atoms/molecules in a higher energy state are said to be excited. The molecules/atoms of substance in an excited energy state are often much more reactive; that is, more amenable to chemical reactions. The phase of a substance is invariably determined by its energy and the energy of its surroundings. When the intermolecular forces of a substance are such that the energy of the surroundings is not sufficient to overcome them, it occurs in a more ordered phase like liquid or solid as is the case with water (H2O); a liquid at room temperature because its molecules are bound by hydrogen bonds. Whereas hydrogen sulfide (H2S) is a gas at room temperature and standard pressure, as its molecules are bound by weaker dipole-dipole interactions. The transfer of energy from one chemical substance to another depends on the size of energy quanta emitted from one substance. However, heat energy is often transferred more easily from almost any substance to another because the phonons responsible for vibrational and rotational energy levels in a substance have much less energy than photons invoked for the electronic energy transfer. Thus, because vibrational and rotational energy levels are more closely spaced than electronic energy levels, heat is more easily transferred between substances relative to light or other forms of electronic energy. For example, ultraviolet electromagnetic radiation is not transferred with as much efficacy from one substance to another as thermal or electrical energy. The existence of characteristic energy levels for different chemical substances is useful for their identification by the analysis of spectral lines. Different kinds of spectra are often used in chemical spectroscopy, e.g. IR, microwave, NMR, ESR, etc. Spectroscopy is also used to identify the composition of remote objects - like stars and distant galaxies - by analyzing their radiation spectra.  The term chemical energy is often used to indicate the potential of a chemical substance to undergo a transformation through a chemical reaction or to transform other chemical substances.    Reaction   When a chemical substance is transformed as a result of its interaction with another substance or with energy, a chemical reaction is said to have occurred. A chemical reaction is therefore a concept related to the \"reaction\" of a substance when it comes in close contact with another, whether as a mixture or a solution; exposure to some form of energy, or both. It results in some energy exchange between the constituents of the reaction as well as with the system environment, which may be designed vessels—often laboratory glassware. Chemical reactions can result in the formation or dissociation of molecules, that is, molecules breaking apart to form two or more smaller molecules, or rearrangement of atoms within or across molecules. Chemical reactions usually involve the making or breaking of chemical bonds. Oxidation, reduction, dissociation, acid-base neutralization and molecular rearrangement are some of the commonly used kinds of chemical reactions. A chemical reaction can be symbolically depicted through a chemical equation. While in a non-nuclear chemical reaction the number and kind of atoms on both sides of the equation are equal, for a nuclear reaction this holds true only for the nuclear particles viz. protons and neutrons. The sequence of steps in which the reorganization of chemical bonds may be taking place in the course of a chemical reaction is called its mechanism. A chemical reaction can be envisioned to take place in a number of steps, each of which may have a different speed. Many reaction intermediates with variable stability can thus be envisaged during the course of a reaction. Reaction mechanisms are proposed to explain the kinetics and the relative product mix of a reaction. Many physical chemists specialize in exploring and proposing the mechanisms of various chemical reactions. Several empirical rules, like the Woodward–Hoffmann rules often come in handy while proposing a mechanism for a chemical reaction. According to the IUPAC gold book, a chemical reaction is \"a process that results in the interconversion of chemical species.\" Accordingly, a chemical reaction may be an elementary reaction or a stepwise reaction. An additional caveat is made, in that this definition includes cases where the interconversion of conformers is experimentally observable. Such detectable chemical reactions normally involve sets of molecular entities as indicated by this definition, but it is often conceptually convenient to use the term also for changes involving single molecular entities (i.e. 'microscopic chemical events').    Ions and salts   An ion is a charged species, an atom or a molecule, that has lost or gained one or more electrons. When an atom loses an electron and thus has more protons than electrons, the atom is a positively charged ion or cation. When an atom gains an electron and thus has more electrons than protons, the atom is a negatively charged ion or anion. Cations and anions can form a crystalline lattice of neutral salts, such as the Na+ and Cl− ions forming sodium chloride, or NaCl. Examples of polyatomic ions that do not split up during acid-base reactions are hydroxide (OH−) and phosphate (PO43−). Plasma is composed of gaseous matter that has been completely ionized, usually through high temperature.    Acidity and basicity   A substance can often be classified as an acid or a base. There are several different theories which explain acid-base behavior. The simplest is Arrhenius theory, which states than an acid is a substance that produces hydronium ions when it is dissolved in water, and a base is one that produces hydroxide ions when dissolved in water. According to Brønsted–Lowry acid–base theory, acids are substances that donate a positive hydrogen ion to another substance in a chemical reaction; by extension, a base is the substance which receives that hydrogen ion. A third common theory is Lewis acid-base theory, which is based on the formation of new chemical bonds. Lewis theory explains that an acid is a substance which is capable of accepting a pair of electrons from another substance during the process of bond formation, while a base is a substance which can provide a pair of electrons to form a new bond. According to this theory, the crucial things being exchanged are charges. There are several other ways in which a substance may be classified as an acid or a base, as is evident in the history of this concept. Acid strength is commonly measured by two methods. One measurement, based on the Arrhenius definition of acidity, is pH, which is a measurement of the hydronium ion concentration in a solution, as expressed on a negative logarithmic scale. Thus, solutions that have a low pH have a high hydronium ion concentration, and can be said to be more acidic. The other measurement, based on the Brønsted–Lowry definition, is the acid dissociation constant (Ka), which measures the relative ability of a substance to act as an acid under the Brønsted–Lowry definition of an acid. That is, substances with a higher Ka are more likely to donate hydrogen ions in chemical reactions than those with lower Ka values.    Redox   Redox (reduction-oxidation) reactions include all chemical reactions in which atoms have their oxidation state changed by either gaining electrons (reduction) or losing electrons (oxidation). Substances that have the ability to oxidize other substances are said to be oxidative and are known as oxidizing agents, oxidants or oxidizers. An oxidant removes electrons from another substance. Similarly, substances that have the ability to reduce other substances are said to be reductive and are known as reducing agents, reductants, or reducers. A reductant transfers electrons to another substance, and is thus oxidized itself. And because it \"donates\" electrons it is also called an electron donor. Oxidation and reduction properly refer to a change in oxidation number—the actual transfer of electrons may never occur. Thus, oxidation is better defined as an increase in oxidation number, and reduction as a decrease in oxidation number.    Equilibrium   Although the concept of equilibrium is widely used across sciences, in the context of chemistry, it arises whenever a number of different states of the chemical composition are possible, as for example, in a mixture of several chemical compounds that can react with one another, or when a substance can be present in more than one kind of phase. A system of chemical substances at equilibrium, even though having an unchanging composition, is most often not static; molecules of the substances continue to react with one another thus giving rise to a dynamic equilibrium. Thus the concept describes the state in which the parameters such as chemical composition remain unchanged over time.    Chemical laws   Chemical reactions are governed by certain laws, which have become fundamental concepts in chemistry. Some of them are:    Practice     Subdisciplines  Chemistry is typically divided into several major sub-disciplines. There are also several main cross-disciplinary and more specialized fields of chemistry. Analytical chemistry is the analysis of material samples to gain an understanding of their chemical composition and structure. Analytical chemistry incorporates standardized experimental methods in chemistry. These methods may be used in all subdisciplines of chemistry, excluding purely theoretical chemistry. Biochemistry is the study of the chemicals, chemical reactions and chemical interactions that take place in living organisms. Biochemistry and organic chemistry are closely related, as in medicinal chemistry or neurochemistry. Biochemistry is also associated with molecular biology and genetics. Inorganic chemistry is the study of the properties and reactions of inorganic compounds. The distinction between organic and inorganic disciplines is not absolute and there is much overlap, most importantly in the sub-discipline of organometallic chemistry. Materials chemistry is the preparation, characterization, and understanding of substances with a useful function. The field is a new breadth of study in graduate programs, and it integrates elements from all classical areas of chemistry with a focus on fundamental issues that are unique to materials. Primary systems of study include the chemistry of condensed phases (solids, liquids, polymers) and interfaces between different phases. Neurochemistry is the study of neurochemicals; including transmitters, peptides, proteins, lipids, sugars, and nucleic acids; their interactions, and the roles they play in forming, maintaining, and modifying the nervous system. Nuclear chemistry is the study of how subatomic particles come together and make nuclei. Modern Transmutation is a large component of nuclear chemistry, and the table of nuclides is an important result and tool for this field. Organic chemistry is the study of the structure, properties, composition, mechanisms, and reactions of organic compounds. An organic compound is defined as any compound based on a carbon skeleton. Physical chemistry is the study of the physical and fundamental basis of chemical systems and processes. In particular, the energetics and dynamics of such systems and processes are of interest to physical chemists. Important areas of study include chemical thermodynamics, chemical kinetics, electrochemistry, statistical mechanics, spectroscopy, and more recently, astrochemistry. Physical chemistry has large overlap with molecular physics. Physical chemistry involves the use of infinitesimal calculus in deriving equations. It is usually associated with quantum chemistry and theoretical chemistry. Physical chemistry is a distinct discipline from chemical physics, but again, there is very strong overlap. Theoretical chemistry is the study of chemistry via fundamental theoretical reasoning (usually within mathematics or physics). In particular the application of quantum mechanics to chemistry is called quantum chemistry. Since the end of the Second World War, the development of computers has allowed a systematic development of computational chemistry, which is the art of developing and applying computer programs for solving chemical problems. Theoretical chemistry has large overlap with (theoretical and experimental) condensed matter physics and molecular physics. Other disciplines within chemistry are traditionally grouped by the type of matter being studied or the kind of study. These include inorganic chemistry, the study of inorganic matter; organic chemistry, the study of organic (carbon-based) matter; biochemistry, the study of substances found in biological organisms; physical chemistry, the study of chemical processes using physical concepts such as thermodynamics and quantum mechanics; and analytical chemistry, the analysis of material samples to gain an understanding of their chemical composition and structure. Many more specialized disciplines have emerged in recent years, e.g. neurochemistry the chemical study of the nervous system (see subdisciplines). Other fields include agrochemistry, astrochemistry (and cosmochemistry), atmospheric chemistry, chemical engineering, chemical biology, chemo-informatics, electrochemistry, environmental chemistry, femtochemistry, flavor chemistry, flow chemistry, geochemistry, green chemistry, histochemistry, history of chemistry, hydrogenation chemistry, immunochemistry, marine chemistry, materials science, mathematical chemistry, mechanochemistry, medicinal chemistry, molecular biology, molecular mechanics, nanotechnology, natural product chemistry, oenology, organometallic chemistry, petrochemistry, pharmacology, photochemistry, physical organic chemistry, phytochemistry, polymer chemistry, radiochemistry, solid-state chemistry, sonochemistry, supramolecular chemistry, surface chemistry, synthetic chemistry, thermochemistry, and many others.    Chemical industry   The chemical industry represents an important economic activity worldwide. The global top 50 chemical producers in 2013 had sales of US$980.5 billion with a profit margin of 10.3%.    Professional societies     See also   Outline of chemistry Glossary of chemistry terms Common chemicals International Year of Chemistry List of chemists List of compounds List of important publications in chemistry List of software for molecular mechanics modeling List of unsolved problems in chemistry Periodic Systems of Small Molecules Philosophy of chemistry    References     Bibliography  Atkins, Peter; de Paula, Julio (2009) [1992]. Elements of Physical Chemistry (5th ed.). New York: Oxford University Press. ISBN 978-0-19-922672-6.  Burrows, Andrew; Holman, John; Parsons, Andrew; Pilling, Gwen; Price, Gareth (2009). Chemistry3. Italy: Oxford University Press. ISBN 978-0-19-927789-6.  Housecroft, Catherine E.; Sharpe, Alan G. (2008) [2001]. Inorganic Chemistry (3rd ed.). Harlow, Essex: Pearson Education. ISBN 978-0-13-175553-6.     Further reading  Popular reading Atkins, P.W. Galileo's Finger (Oxford University Press) ISBN 0-19-860941-8 Atkins, P.W. Atkins' Molecules (Cambridge University Press) ISBN 0-521-82397-8 Kean, Sam. The Disappearing Spoon - and other true tales from the Periodic Table (Black Swan) London, 2010 ISBN 978-0-552-77750-6 Levi, Primo The Periodic Table (Penguin Books) [1975] translated from the Italian by Raymond Rosenthal (1984) ISBN 978-0-14-139944-7 Stwertka, A. A Guide to the Elements (Oxford University Press) ISBN 0-19-515027-9 \"Dictionary of the History of Ideas\".  Introductory undergraduate text books Atkins, P.W., Overton, T., Rourke, J., Weller, M. and Armstrong, F. Shriver and Atkins inorganic chemistry (4th edition) 2006 (Oxford University Press) ISBN 0-19-926463-5 Chang, Raymond. Chemistry 6th ed. Boston: James M. Smith, 1998. ISBN 0-07-115221-0. Clayden, Jonathan; Greeves, Nick; Warren, Stuart; Wothers, Peter (2001). Organic Chemistry (1st ed.). Oxford University Press. ISBN 978-0-19-850346-0.  Voet and Voet Biochemistry (Wiley) ISBN 0-471-58651-X Advanced undergraduate-level or graduate text books Atkins, P.W. Physical Chemistry (Oxford University Press) ISBN 0-19-879285-9 Atkins, P.W. et al. Molecular Quantum Mechanics (Oxford University Press) McWeeny, R. Coulson's Valence (Oxford Science Publications) ISBN 0-19-855144-4 Pauling, L. The Nature of the chemical bond (Cornell University Press) ISBN 0-8014-0333-2 Pauling, L., and Wilson, E. B. Introduction to Quantum Mechanics with Applications to Chemistry (Dover Publications) ISBN 0-486-64871-0 Smart and Moore Solid State Chemistry: An Introduction (Chapman and Hall) ISBN 0-412-40040-5 Stephenson, G. Mathematical Methods for Science Students (Longman) ISBN 0-582-44416-0","label":"foo"},{"text":"Mathematics (from Greek μάθημα máthēma, “knowledge, study, learning”) is the study of topics such as quantity (numbers), structure, space, and change. There is a range of views among mathematicians and philosophers as to the exact scope and definition of mathematics. Mathematicians seek out patterns and use them to formulate new conjectures. Mathematicians resolve the truth or falsity of conjectures by mathematical proof. When mathematical structures are good models of real phenomena, then mathematical reasoning can provide insight or predictions about nature. Through the use of abstraction and logic, mathematics developed from counting, calculation, measurement, and the systematic study of the shapes and motions of physical objects. Practical mathematics has been a human activity for as far back as written records exist. The research required to solve mathematical problems can take years or even centuries of sustained inquiry. Rigorous arguments first appeared in Greek mathematics, most notably in Euclid's Elements. Since the pioneering work of Giuseppe Peano (1858–1932), David Hilbert (1862–1943), and others on axiomatic systems in the late 19th century, it has become customary to view mathematical research as establishing truth by rigorous deduction from appropriately chosen axioms and definitions. Mathematics developed at a relatively slow pace until the Renaissance, when mathematical innovations interacting with new scientific discoveries led to a rapid increase in the rate of mathematical discovery that has continued to the present day. Galileo Galilei (1564–1642) said, \"The universe cannot be read until we have learned the language and become familiar with the characters in which it is written. It is written in mathematical language, and the letters are triangles, circles and other geometrical figures, without which means it is humanly impossible to comprehend a single word. Without these, one is wandering about in a dark labyrinth.\" Carl Friedrich Gauss (1777–1855) referred to mathematics as \"the Queen of the Sciences\". Benjamin Peirce (1809–1880) called mathematics \"the science that draws necessary conclusions\". David Hilbert said of mathematics: \"We are not speaking here of arbitrariness in any sense. Mathematics is not like a game whose tasks are determined by arbitrarily stipulated rules. Rather, it is a conceptual system possessing internal necessity that can only be so and by no means otherwise.\" Albert Einstein (1879–1955) stated that \"as far as the laws of mathematics refer to reality, they are not certain; and as far as they are certain, they do not refer to reality.\" French mathematician Claire Voisin states \"There is creative drive in mathematics, it's all about movement trying to express itself.\"  Mathematics is used throughout the world as an essential tool in many fields, including natural science, engineering, medicine, finance and the social sciences. Applied mathematics, the branch of mathematics concerned with application of mathematical knowledge to other fields, inspires and makes use of new mathematical discoveries, which has led to the development of entirely new mathematical disciplines, such as statistics and game theory. Mathematicians also engage in pure mathematics, or mathematics for its own sake, without having any application in mind. There is no clear line separating pure and applied mathematics, and practical applications for what began as pure mathematics are often discovered.    HistoryEdit     EvolutionEdit   The evolution of mathematics can be seen as an ever-increasing series of abstractions. The first abstraction, which is shared by many animals, was probably that of numbers: the realization that a collection of two apples and a collection of two oranges (for example) have something in common, namely quantity of their members.  As evidenced by tallies found on bone, in addition to recognizing how to count physical objects, prehistoric peoples may have also recognized how to count abstract quantities, like time – days, seasons, years. More complex mathematics did not appear until around 3000 BC, when the Babylonians and Egyptians began using arithmetic, algebra and geometry for taxation and other financial calculations, for building and construction, and for astronomy. The earliest uses of mathematics were in trading, land measurement, painting and weaving patterns and the recording of time. In Babylonian mathematics elementary arithmetic (addition, subtraction, multiplication and division) first appears in the archaeological record. Numeracy pre-dated writing and numeral systems have been many and diverse, with the first known written numerals created by Egyptians in Middle Kingdom texts such as the Rhind Mathematical Papyrus. Between 600 and 300 BC the Ancient Greeks began a systematic study of mathematics in its own right with Greek mathematics. Mathematics has since been greatly extended, and there has been a fruitful interaction between mathematics and science, to the benefit of both. Mathematical discoveries continue to be made today. According to Mikhail B. Sevryuk, in the January 2006 issue of the Bulletin of the American Mathematical Society, \"The number of papers and books included in the Mathematical Reviews database since 1940 (the first year of operation of MR) is now more than 1.9 million, and more than 75 thousand items are added to the database each year. The overwhelming majority of works in this ocean contain new mathematical theorems and their proofs.\"    EtymologyEdit  The word mathematics comes from the Greek μάθημα (máthēma), which, in the ancient Greek language, means \"that which is learnt\", \"what one gets to know\", hence also \"study\" and \"science\", and in modern Greek just \"lesson\". The word máthēma is derived from μανθάνω (manthano), while the modern Greek equivalent is μαθαίνω (mathaino), both of which mean \"to learn\". In Greece, the word for \"mathematics\" came to have the narrower and more technical meaning \"mathematical study\" even in Classical times. Its adjective is μαθηματικός (mathēmatikós), meaning \"related to learning\" or \"studious\", which likewise further came to mean \"mathematical\". In particular, μαθηματικὴ τέχνη (mathēmatikḗ tékhnē), Latin: ars mathematica, meant \"the mathematical art\". In Latin, and in English until around 1700, the term mathematics more commonly meant \"astrology\" (or sometimes \"astronomy\") rather than \"mathematics\"; the meaning gradually changed to its present one from about 1500 to 1800. This has resulted in several mistranslations: a particularly notorious one is Saint Augustine's warning that Christians should beware of mathematici meaning astrologers, which is sometimes mistranslated as a condemnation of mathematicians. The apparent plural form in English, like the French plural form les mathématiques (and the less commonly used singular derivative la mathématique), goes back to the Latin neuter plural mathematica (Cicero), based on the Greek plural τα μαθηματικά (ta mathēmatiká), used by Aristotle (384–322 BC), and meaning roughly \"all things mathematical\"; although it is plausible that English borrowed only the adjective mathematic(al) and formed the noun mathematics anew, after the pattern of physics and metaphysics, which were inherited from the Greek. In English, the noun mathematics takes singular verb forms. It is often shortened to maths or, in English-speaking North America, math.    Definitions of mathematicsEdit   Aristotle defined mathematics as \"the science of quantity\", and this definition prevailed until the 18th century. Starting in the 19th century, when the study of mathematics increased in rigor and began to address abstract topics such as group theory and projective geometry, which have no clear-cut relation to quantity and measurement, mathematicians and philosophers began to propose a variety of new definitions. Some of these definitions emphasize the deductive character of much of mathematics, some emphasize its abstractness, some emphasize certain topics within mathematics. Today, no consensus on the definition of mathematics prevails, even among professionals. There is not even consensus on whether mathematics is an art or a science. A great many professional mathematicians take no interest in a definition of mathematics, or consider it undefinable. Some just say, \"Mathematics is what mathematicians do.\" Three leading types of definition of mathematics are called logicist, intuitionist, and formalist, each reflecting a different philosophical school of thought. All have severe problems, none has widespread acceptance, and no reconciliation seems possible. An early definition of mathematics in terms of logic was Benjamin Peirce's \"the science that draws necessary conclusions\" (1870). In the Principia Mathematica, Bertrand Russell and Alfred North Whitehead advanced the philosophical program known as logicism, and attempted to prove that all mathematical concepts, statements, and principles can be defined and proven entirely in terms of symbolic logic. A logicist definition of mathematics is Russell's \"All Mathematics is Symbolic Logic\" (1903). Intuitionist definitions, developing from the philosophy of mathematician L.E.J. Brouwer, identify mathematics with certain mental phenomena. An example of an intuitionist definition is \"Mathematics is the mental activity which consists in carrying out constructs one after the other.\" A peculiarity of intuitionism is that it rejects some mathematical ideas considered valid according to other definitions. In particular, while other philosophies of mathematics allow objects that can be proven to exist even though they cannot be constructed, intuitionism allows only mathematical objects that one can actually construct. Formalist definitions identify mathematics with its symbols and the rules for operating on them. Haskell Curry defined mathematics simply as \"the science of formal systems\". A formal system is a set of symbols, or tokens, and some rules telling how the tokens may be combined into formulas. In formal systems, the word axiom has a special meaning, different from the ordinary meaning of \"a self-evident truth\". In formal systems, an axiom is a combination of tokens that is included in a given formal system without needing to be derived using the rules of the system.    Mathematics as scienceEdit   Gauss referred to mathematics as \"the Queen of the Sciences\". In the original Latin Regina Scientiarum, as well as in German Königin der Wissenschaften, the word corresponding to science means a \"field of knowledge\", and this was the original meaning of \"science\" in English, also; mathematics is in this sense a field of knowledge. The specialization restricting the meaning of \"science\" to natural science follows the rise of Baconian science, which contrasted \"natural science\" to scholasticism, the Aristotelean method of inquiring from first principles. The role of empirical experimentation and observation is negligible in mathematics, compared to natural sciences such as psychology, biology, or physics. Albert Einstein stated that \"as far as the laws of mathematics refer to reality, they are not certain; and as far as they are certain, they do not refer to reality.\" More recently, Marcus du Sautoy has called mathematics \"the Queen of Science ... the main driving force behind scientific discovery\". Many philosophers believe that mathematics is not experimentally falsifiable, and thus not a science according to the definition of Karl Popper. However, in the 1930s Gödel's incompleteness theorems convinced many mathematicians that mathematics cannot be reduced to logic alone, and Karl Popper concluded that \"most mathematical theories are, like those of physics and biology, hypothetico-deductive: pure mathematics therefore turns out to be much closer to the natural sciences whose hypotheses are conjectures, than it seemed even recently.\" Other thinkers, notably Imre Lakatos, have applied a version of falsificationism to mathematics itself. An alternative view is that certain scientific fields (such as theoretical physics) are mathematics with axioms that are intended to correspond to reality. The theoretical physicist J.M. Ziman proposed that science is public knowledge, and thus includes mathematics. Mathematics shares much in common with many fields in the physical sciences, notably the exploration of the logical consequences of assumptions. Intuition and experimentation also play a role in the formulation of conjectures in both mathematics and the (other) sciences. Experimental mathematics continues to grow in importance within mathematics, and computation and simulation are playing an increasing role in both the sciences and mathematics. The opinions of mathematicians on this matter are varied. Many mathematicians feel that to call their area a science is to downplay the importance of its aesthetic side, and its history in the traditional seven liberal arts; others feel that to ignore its connection to the sciences is to turn a blind eye to the fact that the interface between mathematics and its applications in science and engineering has driven much development in mathematics. One way this difference of viewpoint plays out is in the philosophical debate as to whether mathematics is created (as in art) or discovered (as in science). It is common to see universities divided into sections that include a division of Science and Mathematics, indicating that the fields are seen as being allied but that they do not coincide. In practice, mathematicians are typically grouped with scientists at the gross level but separated at finer levels. This is one of many issues considered in the philosophy of mathematics.    Inspiration, pure and applied mathematics, and aestheticsEdit   Mathematics arises from many different kinds of problems. At first these were found in commerce, land measurement, architecture and later astronomy; today, all sciences suggest problems studied by mathematicians, and many problems arise within mathematics itself. For example, the physicist Richard Feynman invented the path integral formulation of quantum mechanics using a combination of mathematical reasoning and physical insight, and today's string theory, a still-developing scientific theory which attempts to unify the four fundamental forces of nature, continues to inspire new mathematics. Some mathematics is relevant only in the area that inspired it, and is applied to solve further problems in that area. But often mathematics inspired by one area proves useful in many areas, and joins the general stock of mathematical concepts. A distinction is often made between pure mathematics and applied mathematics. However pure mathematics topics often turn out to have applications, e.g. number theory in cryptography. This remarkable fact, that even the \"purest\" mathematics often turns out to have practical applications, is what Eugene Wigner has called \"the unreasonable effectiveness of mathematics\". As in most areas of study, the explosion of knowledge in the scientific age has led to specialization: there are now hundreds of specialized areas in mathematics and the latest Mathematics Subject Classification runs to 46 pages. Several areas of applied mathematics have merged with related traditions outside of mathematics and become disciplines in their own right, including statistics, operations research, and computer science. For those who are mathematically inclined, there is often a definite aesthetic aspect to much of mathematics. Many mathematicians talk about the elegance of mathematics, its intrinsic aesthetics and inner beauty. Simplicity and generality are valued. There is beauty in a simple and elegant proof, such as Euclid's proof that there are infinitely many prime numbers, and in an elegant numerical method that speeds calculation, such as the fast Fourier transform. G.H. Hardy in A Mathematician's Apology expressed the belief that these aesthetic considerations are, in themselves, sufficient to justify the study of pure mathematics. He identified criteria such as significance, unexpectedness, inevitability, and economy as factors that contribute to a mathematical aesthetic. Mathematicians often strive to find proofs that are particularly elegant, proofs from \"The Book\" of God according to Paul Erdős. The popularity of recreational mathematics is another sign of the pleasure many find in solving mathematical questions.    Notation, language, and rigorEdit   Most of the mathematical notation in use today was not invented until the 16th century. Before that, mathematics was written out in words, a painstaking process that limited mathematical discovery. Euler (1707–1783) was responsible for many of the notations in use today. Modern notation makes mathematics much easier for the professional, but beginners often find it daunting. It is extremely compressed: a few symbols contain a great deal of information. Like musical notation, modern mathematical notation has a strict syntax (which to a limited extent varies from author to author and from discipline to discipline) and encodes information that would be difficult to write in any other way. Mathematical language can be difficult to understand for beginners. Words such as or and only have more precise meanings than in everyday speech. Moreover, words such as open and field have been given specialized mathematical meanings. Technical terms such as homeomorphism and integrable have precise meanings in mathematics. Additionally, shorthand phrases such as iff for \"if and only if\" belong to mathematical jargon. There is a reason for special notation and technical vocabulary: mathematics requires more precision than everyday speech. Mathematicians refer to this precision of language and logic as \"rigor\". Mathematical proof is fundamentally a matter of rigor. Mathematicians want their theorems to follow from axioms by means of systematic reasoning. This is to avoid mistaken \"theorems\", based on fallible intuitions, of which many instances have occurred in the history of the subject. The level of rigor expected in mathematics has varied over time: the Greeks expected detailed arguments, but at the time of Isaac Newton the methods employed were less rigorous. Problems inherent in the definitions used by Newton would lead to a resurgence of careful analysis and formal proof in the 19th century. Misunderstanding the rigor is a cause for some of the common misconceptions of mathematics. Today, mathematicians continue to argue among themselves about computer-assisted proofs. Since large computations are hard to verify, such proofs may not be sufficiently rigorous. Axioms in traditional thought were \"self-evident truths\", but that conception is problematic. At a formal level, an axiom is just a string of symbols, which has an intrinsic meaning only in the context of all derivable formulas of an axiomatic system. It was the goal of Hilbert's program to put all of mathematics on a firm axiomatic basis, but according to Gödel's incompleteness theorem every (sufficiently powerful) axiomatic system has undecidable formulas; and so a final axiomatization of mathematics is impossible. Nonetheless mathematics is often imagined to be (as far as its formal content) nothing but set theory in some axiomatization, in the sense that every mathematical statement or proof could be cast into formulas within set theory.    Fields of mathematicsEdit   Mathematics can, broadly speaking, be subdivided into the study of quantity, structure, space, and change (i.e. arithmetic, algebra, geometry, and analysis). In addition to these main concerns, there are also subdivisions dedicated to exploring links from the heart of mathematics to other fields: to logic, to set theory (foundations), to the empirical mathematics of the various sciences (applied mathematics), and more recently to the rigorous study of uncertainty. While some areas might seem unrelated, the Langlands program has found connections between areas previously thought unconnected, such as Galois groups, Riemann surfaces and number theory.    Foundations and philosophyEdit  In order to clarify the foundations of mathematics, the fields of mathematical logic and set theory were developed. Mathematical logic includes the mathematical study of logic and the applications of formal logic to other areas of mathematics; set theory is the branch of mathematics that studies sets or collections of objects. Category theory, which deals in an abstract way with mathematical structures and relationships between them, is still in development. The phrase \"crisis of foundations\" describes the search for a rigorous foundation for mathematics that took place from approximately 1900 to 1930. Some disagreement about the foundations of mathematics continues to the present day. The crisis of foundations was stimulated by a number of controversies at the time, including the controversy over Cantor's set theory and the Brouwer–Hilbert controversy. Mathematical logic is concerned with setting mathematics within a rigorous axiomatic framework, and studying the implications of such a framework. As such, it is home to Gödel's incompleteness theorems which (informally) imply that any effective formal system that contains basic arithmetic, if sound (meaning that all theorems that can be proven are true), is necessarily incomplete (meaning that there are true theorems which cannot be proved in that system). Whatever finite collection of number-theoretical axioms is taken as a foundation, Gödel showed how to construct a formal statement that is a true number-theoretical fact, but which does not follow from those axioms. Therefore, no formal system is a complete axiomatization of full number theory. Modern logic is divided into recursion theory, model theory, and proof theory, and is closely linked to theoretical computer science, as well as to category theory. Theoretical computer science includes computability theory, computational complexity theory, and information theory. Computability theory examines the limitations of various theoretical models of the computer, including the most well-known model – the Turing machine. Complexity theory is the study of tractability by computer; some problems, although theoretically solvable by computer, are so expensive in terms of time or space that solving them is likely to remain practically unfeasible, even with the rapid advancement of computer hardware. A famous problem is the \"P = NP?\" problem, one of the Millennium Prize Problems. Finally, information theory is concerned with the amount of data that can be stored on a given medium, and hence deals with concepts such as compression and entropy.    Pure mathematicsEdit    = QuantityEdit = The study of quantity starts with numbers, first the familiar natural numbers and integers (\"whole numbers\") and arithmetical operations on them, which are characterized in arithmetic. The deeper properties of integers are studied in number theory, from which come such popular results as Fermat's Last Theorem. The twin prime conjecture and Goldbach's conjecture are two unsolved problems in number theory. As the number system is further developed, the integers are recognized as a subset of the rational numbers (\"fractions\"). These, in turn, are contained within the real numbers, which are used to represent continuous quantities. Real numbers are generalized to complex numbers. These are the first steps of a hierarchy of numbers that goes on to include quaternions and octonions. Consideration of the natural numbers also leads to the transfinite numbers, which formalize the concept of \"infinity\". Another area of study is size, which leads to the cardinal numbers and then to another conception of infinity: the aleph numbers, which allow meaningful comparison of the size of infinitely large sets.   = StructureEdit = Many mathematical objects, such as sets of numbers and functions, exhibit internal structure as a consequence of operations or relations that are defined on the set. Mathematics then studies properties of those sets that can be expressed in terms of that structure; for instance number theory studies properties of the set of integers that can be expressed in terms of arithmetic operations. Moreover, it frequently happens that different such structured sets (or structures) exhibit similar properties, which makes it possible, by a further step of abstraction, to state axioms for a class of structures, and then study at once the whole class of structures satisfying these axioms. Thus one can study groups, rings, fields and other abstract systems; together such studies (for structures defined by algebraic operations) constitute the domain of abstract algebra. By its great generality, abstract algebra can often be applied to seemingly unrelated problems; for instance a number of ancient problems concerning compass and straightedge constructions were finally solved using Galois theory, which involves field theory and group theory. Another example of an algebraic theory is linear algebra, which is the general study of vector spaces, whose elements called vectors have both quantity and direction, and can be used to model (relations between) points in space. This is one example of the phenomenon that the originally unrelated areas of geometry and algebra have very strong interactions in modern mathematics. Combinatorics studies ways of enumerating the number of objects that fit a given structure.   = SpaceEdit = The study of space originates with geometry – in particular, Euclidean geometry. Trigonometry is the branch of mathematics that deals with relationships between the sides and the angles of triangles and with the trigonometric functions; it combines space and numbers, and encompasses the well-known Pythagorean theorem. The modern study of space generalizes these ideas to include higher-dimensional geometry, non-Euclidean geometries (which play a central role in general relativity) and topology. Quantity and space both play a role in analytic geometry, differential geometry, and algebraic geometry. Convex and discrete geometry were developed to solve problems in number theory and functional analysis but now are pursued with an eye on applications in optimization and computer science. Within differential geometry are the concepts of fiber bundles and calculus on manifolds, in particular, vector and tensor calculus. Within algebraic geometry is the description of geometric objects as solution sets of polynomial equations, combining the concepts of quantity and space, and also the study of topological groups, which combine structure and space. Lie groups are used to study space, structure, and change. Topology in all its many ramifications may have been the greatest growth area in 20th-century mathematics; it includes point-set topology, set-theoretic topology, algebraic topology and differential topology. In particular, instances of modern-day topology are metrizability theory, axiomatic set theory, homotopy theory, and Morse theory. Topology also includes the now solved Poincaré conjecture, and the still unsolved areas of the Hodge conjecture. Other results in geometry and topology, including the four color theorem and Kepler conjecture, have been proved only with the help of computers.   = ChangeEdit = Understanding and describing change is a common theme in the natural sciences, and calculus was developed as a powerful tool to investigate it. Functions arise here, as a central concept describing a changing quantity. The rigorous study of real numbers and functions of a real variable is known as real analysis, with complex analysis the equivalent field for the complex numbers. Functional analysis focuses attention on (typically infinite-dimensional) spaces of functions. One of many applications of functional analysis is quantum mechanics. Many problems lead naturally to relationships between a quantity and its rate of change, and these are studied as differential equations. Many phenomena in nature can be described by dynamical systems; chaos theory makes precise the ways in which many of these systems exhibit unpredictable yet still deterministic behavior.    Applied mathematicsEdit  Applied mathematics concerns itself with mathematical methods that are typically used in science, engineering, business, and industry. Thus, \"applied mathematics\" is a mathematical science with specialized knowledge. The term applied mathematics also describes the professional specialty in which mathematicians work on practical problems; as a profession focused on practical problems, applied mathematics focuses on the \"formulation, study, and use of mathematical models\" in science, engineering, and other areas of mathematical practice. In the past, practical applications have motivated the development of mathematical theories, which then became the subject of study in pure mathematics, where mathematics is developed primarily for its own sake. Thus, the activity of applied mathematics is vitally connected with research in pure mathematics.   = Statistics and other decision sciencesEdit = Applied mathematics has significant overlap with the discipline of statistics, whose theory is formulated mathematically, especially with probability theory. Statisticians (working as part of a research project) \"create data that makes sense\" with random sampling and with randomized experiments; the design of a statistical sample or experiment specifies the analysis of the data (before the data be available). When reconsidering data from experiments and samples or when analyzing data from observational studies, statisticians \"make sense of the data\" using the art of modelling and the theory of inference – with model selection and estimation; the estimated models and consequential predictions should be tested on new data. Statistical theory studies decision problems such as minimizing the risk (expected loss) of a statistical action, such as using a procedure in, for example, parameter estimation, hypothesis testing, and selecting the best. In these traditional areas of mathematical statistics, a statistical-decision problem is formulated by minimizing an objective function, like expected loss or cost, under specific constraints: For example, designing a survey often involves minimizing the cost of estimating a population mean with a given level of confidence. Because of its use of optimization, the mathematical theory of statistics shares concerns with other decision sciences, such as operations research, control theory, and mathematical economics.   = Computational mathematicsEdit = Computational mathematics proposes and studies methods for solving mathematical problems that are typically too large for human numerical capacity. Numerical analysis studies methods for problems in analysis using functional analysis and approximation theory; numerical analysis includes the study of approximation and discretization broadly with special concern for rounding errors. Numerical analysis and, more broadly, scientific computing also study non-analytic topics of mathematical science, especially algorithmic matrix and graph theory. Other areas of computational mathematics include computer algebra and symbolic computation.    Mathematical awardsEdit  Arguably the most prestigious award in mathematics is the Fields Medal, established in 1936 and now awarded every four years. The Fields Medal is often considered a mathematical equivalent to the Nobel Prize. The Wolf Prize in Mathematics, instituted in 1978, recognizes lifetime achievement, and another major international award, the Abel Prize, was introduced in 2003. The Chern Medal was introduced in 2010 to recognize lifetime achievement. These accolades are awarded in recognition of a particular body of work, which may be innovational, or provide a solution to an outstanding problem in an established field. A famous list of 23 open problems, called \"Hilbert's problems\", was compiled in 1900 by German mathematician David Hilbert. This list achieved great celebrity among mathematicians, and at least nine of the problems have now been solved. A new list of seven important problems, titled the \"Millennium Prize Problems\", was published in 2000. A solution to each of these problems carries a $1 million reward, and only one (the Riemann hypothesis) is duplicated in Hilbert's problems.    See alsoEdit   Mathematics and art Mathematics education Relationship between mathematics and physics STEM fields    NotesEdit     ReferencesEdit     Further readingEdit     External linksEdit","label":"foo"},{"text":"Biology is a natural science concerned with the study of life and living organisms, including their structure, function, growth, evolution, distribution, and taxonomy. Modern biology is a vast and eclectic field, composed of many branches and subdisciplines. However, despite the broad scope of biology, there are certain general and unifying concepts within it that govern all study and research, consolidating it into single, coherent fields. In general, biology recognizes the cell as the basic unit of life, genes as the basic unit of heredity, and evolution as the engine that propels the synthesis and creation of new species. It is also understood today that all organisms survive by consuming and transforming energy and by regulating their internal environment to maintain a stable and vital condition. Subdisciplines of biology are defined by the scale at which organisms are studied, the kinds of organisms studied, and the methods used to study them: biochemistry examines the rudimentary chemistry of life; molecular biology studies the complex interactions among biological molecules; botany studies the biology of plants; cellular biology examines the basic building-block of all life, the cell; physiology examines the physical and chemical functions of tissues, organs, and organ systems of an organism; evolutionary biology examines the processes that produced the diversity of life; and ecology examines how organisms interact in their environment.    History   The term biology is derived from the Greek word βίος, bios, \"life\" and the suffix -λογία, -logia, \"study of.\" The Latin form of the term first appeared in 1736 when Swedish scientist Carl Linnaeus (Carl von Linné) used biologi in his Bibliotheca botanica. It was used again in 1766 in a work entitled Philosophiae naturalis sive physicae: tomus III, continens geologian, biologian, phytologian generalis, by Michael Christoph Hanov, a disciple of Christian Wolff. The first German use, Biologie, was in a 1771 translation of Linnaeus' work. In 1797, Theodor Georg August Roose used the term in a book, Grundzüge der Lehre van der Lebenskraft, in the preface. Karl Friedrich Burdach used the term in 1800 in a more restricted sense of the study of human beings from a morphological, physiological and psychological perspective (Propädeutik zum Studien der gesammten Heilkunst). The term came into its modern usage with the six-volume treatise Biologie, oder Philosophie der lebenden Natur (1802–22) by Gottfried Reinhold Treviranus, who announced: The objects of our research will be the different forms and manifestations of life, the conditions and laws under which these phenomena occur, and the causes through which they have been effected. The science that concerns itself with these objects we will indicate by the name biology [Biologie] or the doctrine of life [Lebenslehre]. Although modern biology is a relatively recent development, sciences related to and included within it have been studied since ancient times. Natural philosophy was studied as early as the ancient civilizations of Mesopotamia, Egypt, the Indian subcontinent, and China. However, the origins of modern biology and its approach to the study of nature are most often traced back to ancient Greece. While the formal study of medicine dates back to Hippocrates (ca. 460 BC – ca. 370 BC), it was Aristotle (384 BC – 322 BC) who contributed most extensively to the development of biology. Especially important are his History of Animals and other works where he showed naturalist leanings, and later more empirical works that focused on biological causation and the diversity of life. Aristotle's successor at the Lyceum, Theophrastus, wrote a series of books on botany that survived as the most important contribution of antiquity to the plant sciences, even into the Middle Ages. Scholars of the medieval Islamic world who wrote on biology included al-Jahiz (781–869), Al-Dīnawarī (828–896), who wrote on botany, and Rhazes (865–925) who wrote on anatomy and physiology. Medicine was especially well studied by Islamic scholars working in Greek philosopher traditions, while natural history drew heavily on Aristotelian thought, especially in upholding a fixed hierarchy of life. Biology began to quickly develop and grow with Anton van Leeuwenhoek's dramatic improvement of the microscope. It was then that scholars discovered spermatozoa, bacteria, infusoria and the diversity of microscopic life. Investigations by Jan Swammerdam led to new interest in entomology and helped to develop the basic techniques of microscopic dissection and staining. Advances in microscopy also had a profound impact on biological thinking. In the early 19th century, a number of biologists pointed to the central importance of the cell. Then, in 1838, Schleiden and Schwann began promoting the now universal ideas that (1) the basic unit of organisms is the cell and (2) that individual cells have all the characteristics of life, although they opposed the idea that (3) all cells come from the division of other cells. Thanks to the work of Robert Remak and Rudolf Virchow, however, by the 1860s most biologists accepted all three tenets of what came to be known as cell theory. Meanwhile, taxonomy and classification became the focus of natural historians. Carl Linnaeus published a basic taxonomy for the natural world in 1735 (variations of which have been in use ever since), and in the 1750s introduced scientific names for all his species. Georges-Louis Leclerc, Comte de Buffon, treated species as artificial categories and living forms as malleable—even suggesting the possibility of common descent. Though he was opposed to evolution, Buffon is a key figure in the history of evolutionary thought; his work influenced the evolutionary theories of both Lamarck and Darwin. Serious evolutionary thinking originated with the works of Jean-Baptiste Lamarck, who was the first to present a coherent theory of evolution. He posited that evolution was the result of environmental stress on properties of animals, meaning that the more frequently and rigorously an organ was used, the more complex and efficient it would become, thus adapting the animal to its environment. Lamarck believed that these acquired traits could then be passed on to the animal's offspring, who would further develop and perfect them. However, it was the British naturalist Charles Darwin, combining the biogeographical approach of Humboldt, the uniformitarian geology of Lyell, Malthus's writings on population growth, and his own morphological expertise and extensive natural observations, who forged a more successful evolutionary theory based on natural selection; similar reasoning and evidence led Alfred Russel Wallace to independently reach the same conclusions. Although it was the subject of controversy (which continues to this day), Darwin's theory quickly spread through the scientific community and soon became a central axiom of the rapidly developing science of biology. The discovery of the physical representation of heredity came along with evolutionary principles and population genetics. In the 1940s and early 1950s, experiments pointed to DNA as the component of chromosomes that held the trait-carrying units that had become known as genes. A focus on new kinds of model organisms such as viruses and bacteria, along with the discovery of the double helical structure of DNA in 1953, marked the transition to the era of molecular genetics. From the 1950s to present times, biology has been vastly extended in the molecular domain. The genetic code was cracked by Har Gobind Khorana, Robert W. Holley and Marshall Warren Nirenberg after DNA was understood to contain codons. Finally, the Human Genome Project was launched in 1990 with the goal of mapping the general human genome. This project was essentially completed in 2003, with further analysis still being published. The Human Genome Project was the first step in a globalized effort to incorporate accumulated knowledge of biology into a functional, molecular definition of the human body and the bodies of other organisms.    Foundations of modern biology     Cell theory   Cell theory states that the cell is the fundamental unit of life, and that all living things are composed of one or more cells or the secreted products of those cells (e.g. shells, hairs and nails etc.). All cells arise from other cells through cell division. In multicellular organisms, every cell in the organism's body derives ultimately from a single cell in a fertilized egg. The cell is also considered to be the basic unit in many pathological processes. In addition, the phenomenon of energy flow occurs in cells in processes that are part of the function known as metabolism. Finally, cells contain hereditary information (DNA), which is passed from cell to cell during cell division.    Evolution   A central organizing concept in biology is that life changes and develops through evolution, and that all life-forms known have a common origin. The theory of evolution postulates that all organisms on the Earth, both living and extinct, have descended from a common ancestor or an ancestral gene pool. This last universal common ancestor of all organisms is believed to have appeared about 3.5 billion years ago. Biologists generally regard the universality and ubiquity of the genetic code as definitive evidence in favor of the theory of universal common descent for all bacteria, archaea, and eukaryotes (see: origin of life). Introduced into the scientific lexicon by Jean-Baptiste de Lamarck in 1809, evolution was established by Charles Darwin fifty years later as a viable scientific model when he articulated its driving force: natural selection. (Alfred Russel Wallace is recognized as the co-discoverer of this concept as he helped research and experiment with the concept of evolution.) Evolution is now used to explain the great variations of life found on Earth. Darwin theorized that species and breeds developed through the processes of natural selection and artificial selection or selective breeding. Genetic drift was embraced as an additional mechanism of evolutionary development in the modern synthesis of the theory. The evolutionary history of the species—which describes the characteristics of the various species from which it descended—together with its genealogical relationship to every other species is known as its phylogeny. Widely varied approaches to biology generate information about phylogeny. These include the comparisons of DNA sequences conducted within molecular biology or genomics, and comparisons of fossils or other records of ancient organisms in paleontology. Biologists organize and analyze evolutionary relationships through various methods, including phylogenetics, phenetics, and cladistics. (For a summary of major events in the evolution of life as currently understood by biologists, see evolutionary timeline.)    Genetics   Genes are the primary units of inheritance in all organisms. A gene is a unit of heredity and corresponds to a region of DNA that influences the form or function of an organism in specific ways. All organisms, from bacteria to animals, share the same basic machinery that copies and translates DNA into proteins. Cells transcribe a DNA gene into an RNA version of the gene, and a ribosome then translates the RNA into a protein, a sequence of amino acids. The translation code from RNA codon to amino acid is the same for most organisms, but slightly different for some. For example, a sequence of DNA that codes for insulin in humans also codes for insulin when inserted into other organisms, such as plants. DNA usually occurs as linear chromosomes in eukaryotes, and circular chromosomes in prokaryotes. A chromosome is an organized structure consisting of DNA and histones. The set of chromosomes in a cell and any other hereditary information found in the mitochondria, chloroplasts, or other locations is collectively known as its genome. In eukaryotes, genomic DNA is located in the cell nucleus, along with small amounts in mitochondria and chloroplasts. In prokaryotes, the DNA is held within an irregularly shaped body in the cytoplasm called the nucleoid. The genetic information in a genome is held within genes, and the complete assemblage of this information in an organism is called its genotype.    Homeostasis   Homeostasis is the ability of an open system to regulate its internal environment to maintain stable conditions by means of multiple dynamic equilibrium adjustments controlled by interrelated regulation mechanisms. All living organisms, whether unicellular or multicellular, exhibit homeostasis. To maintain dynamic equilibrium and effectively carry out certain functions, a system must detect and respond to perturbations. After the detection of a perturbation, a biological system normally responds through negative feedback. This means stabilizing conditions by either reducing or increasing the activity of an organ or system. One example is the release of glucagon when sugar levels are too low.    Energy  The survival of a living organism depends on the continuous input of energy. Chemical reactions that are responsible for its structure and function are tuned to extract energy from substances that act as its food and transform them to help form new cells and sustain them. In this process, molecules of chemical substances that constitute food play two roles; first, they contain energy that can be transformed for biological chemical reactions; second, they develop new molecular structures made up of biomolecules. The organisms responsible for the introduction of energy into an ecosystem are known as producers or autotrophs. Nearly all of these organisms originally draw energy from the sun. Plants and other phototrophs use solar energy via a process known as photosynthesis to convert raw materials into organic molecules, such as ATP, whose bonds can be broken to release energy. A few ecosystems, however, depend entirely on energy extracted by chemotrophs from methane, sulfides, or other non-luminal energy sources. Some of the captured energy is used to produce biomass to sustain life and provide energy for growth and development. The majority of the rest of this energy is lost as heat and waste molecules. The most important processes for converting the energy trapped in chemical substances into energy useful to sustain life are metabolism and cellular respiration.    Study and research     Structural   Molecular biology is the study of biology at a molecular level. This field overlaps with other areas of biology, particularly with genetics and biochemistry. Molecular biology chiefly concerns itself with understanding the interactions between the various systems of a cell, including the interrelationship of DNA, RNA, and protein synthesis and learning how these interactions are regulated. Cell biology studies the structural and physiological properties of cells, including their behaviors, interactions, and environment. This is done on both the microscopic and molecular levels, for unicellular organisms such as bacteria, as well as the specialized cells in multicellular organisms such as humans. Understanding the structure and function of cells is fundamental to all of the biological sciences. The similarities and differences between cell types are particularly relevant to molecular biology. Anatomy considers the forms of macroscopic structures such as organs and organ systems. Genetics is the science of genes, heredity, and the variation of organisms. Genes encode the information necessary for synthesizing proteins, which in turn play a central role in influencing the final phenotype of the organism. In modern research, genetics provides important tools in the investigation of the function of a particular gene, or the analysis of genetic interactions. Within organisms, genetic information generally is carried in chromosomes, where it is represented in the chemical structure of particular DNA molecules. Developmental biology studies the process by which organisms grow and develop. Originating in embryology, modern developmental biology studies the genetic control of cell growth, differentiation, and \"morphogenesis,\" which is the process that progressively gives rise to tissues, organs, and anatomy. Model organisms for developmental biology include the round worm Caenorhabditis elegans, the fruit fly Drosophila melanogaster, the zebrafish Danio rerio, the mouse Mus musculus, and the weed Arabidopsis thaliana. (A model organism is a species that is extensively studied to understand particular biological phenomena, with the expectation that discoveries made in that organism provide insight into the workings of other organisms.)    Physiological   Physiology studies the mechanical, physical, and biochemical processes of living organisms by attempting to understand how all of the structures function as a whole. The theme of \"structure to function\" is central to biology. Physiological studies have traditionally been divided into plant physiology and animal physiology, but some principles of physiology are universal, no matter what particular organism is being studied. For example, what is learned about the physiology of yeast cells can also apply to human cells. The field of animal physiology extends the tools and methods of human physiology to non-human species. Plant physiology borrows techniques from both research fields. Physiology studies how for example nervous, immune, endocrine, respiratory, and circulatory systems, function and interact. The study of these systems is shared with medically oriented disciplines such as neurology and immunology.    Evolutionary  Evolutionary research is concerned with the origin and descent of species, as well as their change over time, and includes scientists from many taxonomically oriented disciplines. For example, it generally involves scientists who have special training in particular organisms such as mammalogy, ornithology, botany, or herpetology, but use those organisms as systems to answer general questions about evolution. Evolutionary biology is partly based on paleontology, which uses the fossil record to answer questions about the mode and tempo of evolution, and partly on the developments in areas such as population genetics. In the 1980s, developmental biology re-entered evolutionary biology from its initial exclusion from the modern synthesis through the study of evolutionary developmental biology. Related fields often considered part of evolutionary biology are phylogenetics, systematics, and taxonomy.    Systematic   Multiple speciation events create a tree structured system of relationships between species. The role of systematics is to study these relationships and thus the differences and similarities between species and groups of species. However, systematics was an active field of research long before evolutionary thinking was common. Traditionally, living things have been divided into five kingdoms: Monera; Protista; Fungi; Plantae; Animalia. However, many scientists now consider this five-kingdom system outdated. Modern alternative classification systems generally begin with the three-domain system: Archaea (originally Archaebacteria); Bacteria (originally Eubacteria) and Eukaryota (including protists, fungi, plants, and animals) These domains reflect whether the cells have nuclei or not, as well as differences in the chemical composition of key biomolecules such as ribosomes. Further, each kingdom is broken down recursively until each species is separately classified. The order is: Domain; Kingdom; Phylum; Class; Order; Family; Genus; Species. Outside of these categories, there are obligate intracellular parasites that are \"on the edge of life\" in terms of metabolic activity, meaning that many scientists do not actually classify these structures as alive, due to their lack of at least one or more of the fundamental functions or characteristics that define life. They are classified as viruses, viroids, prions, or satellites. The scientific name of an organism is generated from its genus and species. For example, humans are listed as Homo sapiens. Homo is the genus, and sapiens the species. When writing the scientific name of an organism, it is proper to capitalize the first letter in the genus and put all of the species in lowercase. Additionally, the entire term may be italicized or underlined. The dominant classification system is called the Linnaean taxonomy. It includes ranks and binomial nomenclature. How organisms are named is governed by international agreements such as the International Code of Nomenclature for algae, fungi, and plants (ICN), the International Code of Zoological Nomenclature (ICZN), and the International Code of Nomenclature of Bacteria (ICNB). The classification of viruses, viroids, prions, and all other sub-viral agents that demonstrate biological characteristics is conducted by the International Committee on Taxonomy of Viruses (ICTV) and is known as the International Code of Viral Classification and Nomenclature (ICVCN). However, several other viral classification systems do exist. A merging draft, BioCode, was published in 1997 in an attempt to standardize nomenclature in these three areas, but has yet to be formally adopted. The BioCode draft has received little attention since 1997; its originally planned implementation date of January 1, 2000, has passed unnoticed. A revised BioCode that, instead of replacing the existing codes, would provide a unified context for them, was proposed in 2011. However, the International Botanical Congress of 2011 declined to consider the BioCode proposal. The ICVCN remains outside the BioCode, which does not include viral classification.    Ecological and environmental   Ecology studies the distribution and abundance of living organisms, and the interactions between organisms and their environment. The habitat of an organism can be described as the local abiotic factors such as climate and ecology, in addition to the other organisms and biotic factors that share its environment. One reason that biological systems can be difficult to study is that so many different interactions with other organisms and the environment are possible, even on small scales. A microscopic bacterium in a local sugar gradient is responding to its environment as much as a lion searching for food in the African savanna. For any species, behaviors can be co-operative, competitive, parasitic, or symbiotic. Matters become more complex when two or more species interact in an ecosystem. Ecological systems are studied at several different levels, from individuals and populations to ecosystems and the biosphere. The term population biology is often used interchangeably with population ecology, although population biology is more frequently used when studying diseases, viruses, and microbes, while population ecology is more commonly used when studying plants and animals. Ecology draws on many subdisciplines. Ethology studies animal behavior (particularly that of social animals such as primates and canids), and is sometimes considered a branch of zoology. Ethologists have been particularly concerned with the evolution of behavior and the understanding of behavior in terms of the theory of natural selection. In one sense, the first modern ethologist was Charles Darwin, whose book, The Expression of the Emotions in Man and Animals, influenced many ethologists to come. Biogeography studies the spatial distribution of organisms on the Earth, focusing on topics like plate tectonics, climate change, dispersal and migration, and cladistics.    Basic unresolved problems in biology   Despite the profound advances made over recent decades in our understanding of life's fundamental processes, some basic problems have remained unresolved. For example, one of the major unresolved problems in biology is the primary adaptive function of sex, and particularly its key processes in eukaryotes, meiosis and homologous recombination. One view is that sex evolved primarily as an adaptation for increasing genetic diversity (see references e.g.). An alternative view is that sex is an adaptation for promoting accurate DNA repair in germ-line DNA, and that increased genetic diversity is primarily a byproduct that may be useful in the long run. (See also Evolution of sexual reproduction). Another basic unresolved problem in biology is the biologic basis of aging. At present, there is no consensus view on the underlying cause of aging. Various competing theories are outlined in Ageing Theories.    Branches  These are the main branches of biology: Aerobiology – the study of airborne organic particles Agriculture – the study of producing crops and raising livestock, with an emphasis on practical applications Anatomy – the study of form and function, in plants, animals, and other organisms, or specifically in humans Histology – the study of cells and tissues, a microscopic branch of anatomy  Astrobiology (also known as exobiology, exopaleontology, and bioastronomy) – the study of evolution, distribution, and future of life in the universe Biochemistry – the study of the chemical reactions required for life to exist and function, usually a focus on the cellular level Bioengineering – the study of biology through the means of engineering with an emphasis on applied knowledge and especially related to biotechnology Biogeography – the study of the distribution of species spatially and temporally Bioinformatics – the use of information technology for the study, collection, and storage of genomic and other biological data Biomathematics (or Mathematical biology) – the quantitative or mathematical study of biological processes, with an emphasis on modeling Biomechanics – often considered a branch of medicine, the study of the mechanics of living beings, with an emphasis on applied use through prosthetics or orthotics Biomedical research – the study of health and disease Pharmacology – the study and practical application of preparation, use, and effects of drugs and synthetic medicines  Biomusicology – the study of music from a biological point of view. Biophysics – the study of biological processes through physics, by applying the theories and methods traditionally used in the physical sciences Biosemiotics – the study of biological processes through semiotics, by applying the models of meaning-making and communication Biotechnology – the study of the manipulation of living matter, including genetic modification and synthetic biology Synthetic biology – research integrating biology and engineering; construction of biological functions not found in nature  Building biology – the study of the indoor living environment Botany – the study of plants Cell biology – the study of the cell as a complete unit, and the molecular and chemical interactions that occur within a living cell Cognitive biology – the study of cognition as a biological function Conservation biology – the study of the preservation, protection, or restoration of the natural environment, natural ecosystems, vegetation, and wildlife Cryobiology – the study of the effects of lower than normally preferred temperatures on living beings Developmental biology – the study of the processes through which an organism forms, from zygote to full structure Embryology – the study of the development of embryo (from fecundation to birth)  Ecology – the study of the interactions of living organisms with one another and with the non-living elements of their environment Environmental biology – the study of the natural world, as a whole or in a particular area, especially as affected by human activity Epidemiology – a major component of public health research, studying factors affecting the health of populations Evolutionary biology – the study of the origin and descent of species over time Genetics – the study of genes and heredity. Epigenetics – the study of heritable changes in gene expression or cellular phenotype caused by mechanisms other than changes in the underlying DNA sequence  Hematology (also known as Haematology) – the study of blood and blood-forming organs. Integrative biology – the study of whole organisms Limnology – the study of inland waters Marine biology (or Biological oceanography) – the study of ocean ecosystems, plants, animals, and other living beings Microbiology – the study of microscopic organisms (microorganisms) and their interactions with other living things Bacteriology - the study of bacteria Mycology – the study of fungi Parasitology – the study of parasites and parasitism Virology – the study of viruses and some other virus-like agents  Molecular biology – the study of biology and biological functions at the molecular level, some cross over with biochemistry Nanobiology - the study of how nanotechnology can be used in biology, and the study of living organisms and parts on the nanoscale level of organization Neurobiology – the study of the nervous system, including anatomy, physiology and pathology Population biology – the study of groups of conspecific organisms, including Population ecology – the study of how population dynamics and extinction Population genetics – the study of changes in gene frequencies in populations of organisms  Paleontology – the study of fossils and sometimes geographic evidence of prehistoric life Pathobiology or pathology – the study of diseases, and the causes, processes, nature, and development of disease Physiology – the study of the functioning of living organisms and the organs and parts of living organisms Phytopathology – the study of plant diseases (also called Plant Pathology) Psychobiology – the study of the biological bases of psychology Quantum biology - the study of quantum mechanics to biological objects and problems. Sociobiology – the study of the biological bases of sociology Structural biology – a branch of molecular biology, biochemistry, and biophysics concerned with the molecular structure of biological macromolecules Zoology – the study of animals, including classification, physiology, development, and behavior, including: Ethology – the study of animal behavior Entomology – the study of insects Herpetology – the study of reptiles and amphibians Ichthyology – the study of fish Mammalogy – the study of mammals Ornithology – the study of birds    See also   Glossary of biology List of biological websites List of biologists List of biology topics List of omics topics in biology List of biology journals Outline of biology Reproduction Terminology of biology Periodic table of life sciences in Tinbergen's four questions    References     Further reading     External links  Biology at DMOZ OSU's Phylocode Biology Online – Wiki Dictionary MIT video lecture series on biology Biology and Bioethics. Biological Systems – Idaho National Laboratory The Tree of Life: A multi-authored, distributed Internet project containing information about phylogeny and biodiversity. The Study of Biology Using the Biological Literature Web Resources Journal links PLos Biology A peer-reviewed, open-access journal published by the Public Library of Science Current Biology General journal publishing original research from all areas of biology Biology Letters A high-impact Royal Society journal publishing peer-reviewed Biology papers of general interest Science Magazine Internationally Renowned AAAS Science Publication – See Sections of the Life Sciences International Journal of Biological Sciences A biological journal publishing significant peer-reviewed scientific papers Perspectives in Biology and Medicine An interdisciplinary scholarly journal publishing essays of broad relevance Life Science Log","label":"foo"},{"text":"The scientific revolution was the emergence of modern science during the early modern period, when developments in mathematics, physics, astronomy, biology (including human anatomy) and chemistry transformed views of society and nature. The scientific revolution began in Europe towards the end of the Renaissance period and continued through the late 18th century, influencing the intellectual social movement known as the Enlightenment. While its dates are disputed, the publication in 1543 of Nicolaus Copernicus's De revolutionibus orbium coelestium (On the Revolutions of the Heavenly Spheres) is often cited as marking the beginning of the scientific revolution. A first phase of the scientific revolution, focused on the recovery of the knowledge of the ancients, can be described as the Scientific Renaissance and is considered to have ended in 1632 with publication of Galileo's Dialogue Concerning the Two Chief World Systems. The completion of the scientific revolution is attributed to the \"grand synthesis\" of Isaac Newton's 1687 Principia, that formulated the laws of motion and universal gravitation. By the end of the 18th century, the scientific revolution had given way to the \"Age of Reflection\". The concept of a scientific revolution taking place over an extended period emerged in the eighteenth century in the work of Jean Sylvain Bailly, who saw a two-stage process of sweeping away the old and establishing the new.    IntroductionEdit  Advances in science have been termed \"revolutions\" since the 18th century. In 1747, Clairaut wrote that \"Newton was said in his own lifetime to have created a revolution\". The word was also used in the preface to Lavoisier's 1789 work announcing the discovery of oxygen. \"Few revolutions in science have immediately excited so much general notice as the introduction of the theory of oxygen ... Lavoisier saw his theory accepted by all the most eminent men of his time, and established over a great part of Europe within a few years from its first promulgation.\" In the 19th century, William Whewell established the notion of a revolution in science itself (or the scientific method) that had taken place in the 15th–16th century. \"Among the most conspicuous of the revolutions which opinions on this subject have undergone, is the transition from an implicit trust in the internal powers of man's mind to a professed dependence upon external observation; and from an unbounded reverence for the wisdom of the past, to a fervid expectation of change and improvement.\" This gave rise to the common view of the scientific revolution today: \"A new view of nature emerged, replacing the Greek view that had dominated science for almost 2,000 years. Science became an autonomous discipline, distinct from both philosophy and technology and came to be regarded as having utilitarian goals.\"  It is traditionally assumed to start with the Copernican Revolution (initiated in 1543) and to be complete in the \"grand synthesis\" of Isaac Newton's 1687 Principia. Much of the change of attitude came from Francis Bacon whose \"confident and emphatic announcement\" in the modern progress of science inspired the creation of scientific societies such as the Royal Society, and Galileo who championed Copernicus and developed the science of motion. In the 20th century, Alexandre Koyré introduced the term \"Scientific Revolution\", centering his analysis on Galileo, and the term was popularized by Butterfield in his Origins of Modern Science. Thomas Kuhn's 1962 work The Structure of Scientific Revolutions emphasized that different theoretical frameworks—such as Einstein's relativity theory and Newton's theory of gravity, which it replaced—cannot be directly compared.    SignificanceEdit  The period saw a fundamental transformation in scientific ideas across mathematics, physics, astronomy, and biology in institutions supporting scientific investigation and in the more widely held picture of the universe. The scientific revolution led to the establishment of several modern sciences. In 1984, Joseph Ben-David wrote:  Rapid accumulation of knowledge, which has characterized the development of science since the 17th century, had never occurred before that time. The new kind of scientific activity emerged only in a few countries of Western Europe, and it was restricted to that small area for about two hundred years. (Since the 19th century, scientific knowledge has been assimilated by the rest of the world).  Many contemporary writers and modern historians claim that there was a revolutionary change in world view. In 1611 the English poet, John Donne, wrote:  [The] new Philosophy calls all in doubt, The Element of fire is quite put out; The Sun is lost, and th'earth, and no man's wit Can well direct him where to look for it.  Mid-20th century historian Herbert Butterfield was less disconcerted, but nevertheless saw the change as fundamental:  Since that revolution turned the authority in English not only of the Middle Ages but of the ancient world—since it started not only in the eclipse of scholastic philosophy but in the destruction of Aristotelian physics—it outshines everything since the rise of Christianity and reduces the Renaissance and Reformation to the rank of mere episodes, mere internal displacements within the system of medieval Christendom.... [It] looms so large as the real origin both of the modern world and of the modern mentality that our customary periodization of European history has become an anachronism and an encumbrance.  The history professor Peter Harrison attributes Christianity to having contributed to the rise of the scientific revolution:  historians of science have long known that religious factors played a significantly positive role in the emergence and persistence of modern science in the West. Not only were many of the key figures in the rise of science individuals with sincere religious commitments, but the new approaches to nature that they pioneered were underpinned in various ways by religious assumptions. ... Yet, many of the leading figures in the scientific revolution imagined themselves to be champions of a science that was more compatible with Christianity than the medieval ideas about the natural world that they replaced.  Although historians of science continue to debate the exact meaning of the term, and even its validity, the scientific revolution still remains a useful concept to interpret the many changes in science itself.    Ancient and medieval backgroundEdit   The scientific revolution was built upon the foundation of ancient Greek learning and science in the Middle Ages, as it had been elaborated and further developed by Roman/Byzantine science and medieval Islamic science. Some scholars have noted a direct tie between \"particular aspects of traditional Christianity\" and the rise of science. The \"Aristotelian tradition\" was still an important intellectual framework in by the 17th century, although by that time natural philosophers had moved away from much of it. Key scientific ideas dating back to classical antiquity had changed drastically over the years, and in many cases been discredited. The ideas that remained, which were transformed fundamentally during the scientific revolution, include: Aristotle's cosmology that placed the Earth at the center of a spherical hierarchic cosmos. The terrestrial and celestial regions were made up of different elements which had different kinds of natural movement. The terrestrial region, according to Aristotle, consisted of concentric spheres of the four elements—earth, water, air, and fire. All bodies naturally moved in straight lines until they reached the sphere appropriate to their elemental composition—their natural place. All other terrestrial motions were non-natural, or violent. The celestial region was made up of the fifth element, aether, which was unchanging and moved naturally with uniform circular motion. In the Aristotelian tradition, astronomical theories sought to explain the observed irregular motion of celestial objects through the combined effects of multiple uniform circular motions.  The Ptolemaic model of planetary motion: based on the geometrical model of Eudoxus of Cnidus, Ptolemy's Almagest, demonstrated that calculations could compute the exact positions of the Sun, Moon, stars, and planets in the future and in the past, and showed how these computational models were derived from astronomical observations. As such they formed the model for later astronomical developments. The physical basis for Ptolemaic models invoked layers of spherical shells, though the most complex models were inconsistent with this physical explanation. It is important to note that ancient precedent existed for alternative theories and developments which prefigured later discoveries in the area of physics and mechanics; but in light of the limited number of works to survive translation in a period when many books were lost to warfare, such developments remained obscure for centuries and are traditionally held to have had little effect on the re-discovery of such phenomena; whereas the invention of the printing press made the wide dissemination of such incremental advances of knowledge commonplace. Meanwhile, however, significant progress in geometry, mathematics, and astronomy was made in medieval times, particularly in the Islamic world as well as Europe. It is also true that many of the important figures of the scientific revolution shared in the general Renaissance respect for ancient learning and cited ancient pedigrees for their innovations. Nicolaus Copernicus (1473–1543), Kepler (1571–1630), Newton (1642–1727), and Galileo Galilei (1564–1642) all traced different ancient and medieval ancestries for the heliocentric system. In the Axioms Scholium of his Principia, Newton said its axiomatic three laws of motion were already accepted by mathematicians such as Huygens (1629–1695), Wallace, Wren and others. While preparing a revised edition of his Principia, Newton attributed his law of gravity and his first law of motion to a range of historical figures. Despite these qualifications, the standard theory of the history of the scientific revolution claims that the 17th century was a period of revolutionary scientific changes. Not only were there revolutionary theoretical and experimental developments, but that even more importantly, the way in which scientists worked was radically changed. For instance, although intimations of the concept of inertia are suggested sporadically in ancient discussion of motion, the salient point is that Newton's theory differed from ancient understandings in key ways, such as an external force being a requirement for violent motion in Aristotle's theory.    Scientific methodEdit  Under the scientific method that was defined and applied in the 17th century, natural and artificial circumstances were abandoned, and a research tradition of systematic experimentation was slowly accepted throughout the scientific community. The philosophy of using an inductive approach to nature — to abandon assumption and to attempt to simply observe with an open mind — was in strict contrast with the earlier, Aristotelian approach of deduction, by which analysis of known facts produced further understanding. In practice, of course, many scientists (and philosophers) believed that a healthy mix of both was needed — the willingness to question assumptions, yet also to interpret observations assumed to have some degree of validity. By the end of the scientific revolution the qualitative world of book-reading philosophers had been changed into a mechanical, mathematical world to be known through experimental research. Though it is certainly not true that Newtonian science was like modern science in all respects, it conceptually resembled ours in many ways. Many of the hallmarks of modern science, especially with regard to its institution and profession, did not become standard until the mid-19th century.    EmpiricismEdit  The Aristotelian scientific tradition's primary mode of interacting with the world was through observation and searching for \"natural\" circumstances through reasoning. Coupled with this approach was the belief that rare events which seemed to contradict theoretical models were aberrations, telling nothing about nature as it \"naturally\" was. During the scientific revolution, changing perceptions about the role of the scientist in respect to nature, the value of evidence, experimental or observed, led towards a scientific methodology in which empiricism played a large, but not absolute, role. By the start of the scientific revolution, empiricism had already become an important component of science and natural philosophy. Prior thinkers, including the early 14th century nominalist philosopher William of Ockham, had begun the intellectual movement toward empiricism. The term British empiricism came into use to describe philosophical differences perceived between two of its founders Francis Bacon, described as empiricist, and René Descartes, who was described as a rationalist. Thomas Hobbes, George Berkeley, and David Hume were the philosophy's primary exponents, who developed a sophisticated empirical tradition as the basis of human knowledge. The recognized founder of empiricism was John Locke who proposed in An Essay Concerning Human Understanding (1689) that the only true knowledge that could be accessible to the human mind was that which was based on experience. He argued that the human mind was created as a tabula rasa, a \"blank tablet,\" upon which sensory impressions were recorded and built up knowledge through a process of reflection.    Baconian scienceEdit   The philosophical underpinnings of the scientific revolution were laid out by Francis Bacon, who has been called the father of empiricism. His works established and popularised inductive methodologies for scientific inquiry, often called the Baconian method, or simply the scientific method. His demand for a planned procedure of investigating all things natural marked a new turn in the rhetorical and theoretical framework for science, much of which still surrounds conceptions of proper methodology today. Bacon proposed a great reformation of all process of knowledge for the advancement of learning divine and human, which he called Instauratio Magna (The Great Instauration). For Bacon, this reformation would lead to a great advancement in science and a progeny of new inventions that would relieve mankind's miseries and needs. His Novum Organum was published in 1620. He argued that man is \"the minister and interpreter of nature\", that \"knowledge and human power are synonymous\", that \"effects are produced by the means of instruments and helps\", and that \"man while operating can only apply or withdraw natural bodies; nature internally performs the rest\", and later that \"nature can only be commanded by obeying her\". Here is an abstract of the philosophy of this work, that by the knowledge of nature and the using of instruments, man can govern or direct the natural work of nature to produce definite results. Therefore, that man, by seeking knowledge of nature, can reach power over it – and thus reestablish the \"Empire of Man over creation\", which had been lost by the Fall together with man's original purity. In this way, he believed, would mankind be raised above conditions of helplessness, poverty and misery, while coming into a condition of peace, prosperity and security. For this purpose of obtaining knowledge of and power over nature, Bacon outlined in this work a new system of logic he believed to be superior to the old ways of syllogism, developing his scientific method, consisting of procedures for isolating the formal cause of a phenomenon (heat, for example) through eliminative induction. For him, the philosopher should proceed through inductive reasoning from fact to axiom to physical law. Before beginning this induction, though, the enquirer must free his or her mind from certain false notions or tendencies which distort the truth. In particular, he found that philosophy was too preoccupied with words, particularly discourse and debate, rather than actually observing the material world: \"For while men believe their reason governs words, in fact, words turn back and reflect their power upon the understanding, and so render philosophy and science sophistical and inactive.\" Bacon considered that it is of greatest importance to science not to keep doing intellectual discussions or seeking merely contemplative aims, but that it should work for the bettering of mankind's life by bringing forth new inventions, having even stated that \"inventions are also, as it were, new creations and imitations of divine works\". He explored the far-reaching and world-changing character of inventions, such as the printing press, gunpowder and the compass.    Scientific experimentationEdit  Bacon first described the experimental method.  William Gilbert was an early advocate of this methodology. He passionately rejected both the prevailing Aristotelian philosophy and the Scholastic method of university teaching. His book De Magnete was written in 1600, and he is regarded by some as the father of electricity and magnetism. In this work, he describes many of his experiments with his model Earth called the terrella. From these experiments, he concluded that the Earth was itself magnetic and that this was the reason compasses point north.  De Magnete was influential not only because of the inherent interest of its subject matter, but also for the rigorous way in which Gilbert described his experiments and his rejection of ancient theories of magnetism. According to Thomas Thomson, \"Gilbert['s]... book on magnetism published in 1600, is one of the finest examples of inductive philosophy that has ever been presented to the world. It is the more remarkable, because it preceded the Novum Organum of Bacon, in which the inductive method of philosophizing was first explained.\" Galileo Galilei has been called the \"father of modern observational astronomy\", the \"father of modern physics\", the \"father of science\", and \"the Father of Modern Science\". His original contributions to the science of motion were made through an innovative combination of experiment and mathematics.  Galileo was one of the first modern thinkers to clearly state that the laws of nature are mathematical. In The Assayer he wrote \"Philosophy is written in this grand book, the universe ... It is written in the language of mathematics, and its characters are triangles, circles, and other geometric figures;....\" His mathematical analyses are a further development of a tradition employed by late scholastic natural philosophers, which Galileo learned when he studied philosophy. He displayed a peculiar ability to ignore established authorities, most notably Aristotelianism. In broader terms, his work marked another step towards the eventual separation of science from both philosophy and religion; a major development in human thought. He was often willing to change his views in accordance with observation. In order to perform his experiments, Galileo had to set up standards of length and time, so that measurements made on different days and in different laboratories could be compared in a reproducible fashion. This provided a reliable foundation on which to confirm mathematical laws using inductive reasoning. Galileo showed a remarkably modern appreciation for the proper relationship between mathematics, theoretical physics, and experimental physics. He understood the parabola, both in terms of conic sections and in terms of the ordinate (y) varying as the square of the abscissa (x). Galilei further asserted that the parabola was the theoretically ideal trajectory of a uniformly accelerated projectile in the absence of friction and other disturbances. He conceded that there are limits to the validity of this theory, noting on theoretical grounds that a projectile trajectory of a size comparable to that of the Earth could not possibly be a parabola, but he nevertheless maintained that for distances up to the range of the artillery of his day, the deviation of a projectile's trajectory from a parabola would be only very slight.    MathematizationEdit  Scientific knowledge, according to the Aristotelians, was concerned with establishing true and necessary causes of things. To the extent that medieval natural philosophers used mathematical problems, they limited social studies to theoretical analyses of local speed and other aspects of life. The actual measurement of a physical quantity, and the comparison of that measurement to a value computed on the basis of theory, was largely limited to the mathematical disciplines of astronomy and optics in Europe. In the 16th and 17th centuries, European scientists began increasingly applying quantitative measurements to the measurement of physical phenomena on the Earth. Galileo maintained strongly that mathematics provided a kind of necessary certainty that could be compared to God's: \"...with regard to those few [mathematical propositions] which the human intellect does understand, I believe its knowledge equals the Divine in objective certainty...\" Galileo anticipates the concept of a systematic mathematical interpretation of the world in his book Il Saggiatore:  Philosophy [i.e., physics] is written in this grand book—I mean the universe—which stands continually open to our gaze, but it cannot be understood unless one first learns to comprehend the language and interpret the characters in which it is written. It is written in the language of mathematics, and its characters are triangles, circles, and other geometrical figures, without which it is humanly impossible to understand a single word of it; without these, one is wandering around in a dark labyrinth.    The mechanical philosophyEdit   Aristotle recognized four kinds of causes, and where applicable, the most important of them is the \"final cause\". The final cause was the aim, goal, or purpose of some natural process or man-made thing. Until the scientific revolution, it was very natural to see such aims, such as a child's growth, for example, leading to a mature adult. Intelligence was assumed only in the purpose of man-made artifacts; it was not attributed to other animals or to nature. In \"mechanical philosophy\" no field or action at a distance is permitted, particles or corpuscles of matter are fundamentally inert. Motion is caused by direct physical collision. Where natural substances had previously been understood organically, the mechanical philosophers viewed them as machines. As a result, Isaac Newton's theory seemed like some kind of throwback to \"spooky action at a distance\". According to Thomas Kuhn, he and Descartes held the teleological principle that God conserved the amount of motion in the universe:  Gravity, interpreted as an innate attraction between every pair of particles of matter, was an occult quality in the same sense as the scholastics' \"tendency to fall\" had been.... By the mid eighteenth century that interpretation had been almost universally accepted, and the result was a genuine reversion (which is not the same as a retrogression) to a scholastic standard. Innate attractions and repulsions joined size, shape, position and motion as physically irreducible primary properties of matter.  Newton had also specifically attributed the inherent power of inertia to matter, against the mechanist thesis that matter has no inherent powers. But whereas Newton vehemently denied gravity was an inherent power of matter, his collaborator Roger Cotes made gravity also an inherent power of matter, as set out in his famous preface to the Principia's 1713 second edition which he edited, and contra Newton himself. And it was Cotes's interpretation of gravity rather than Newton's that came to be accepted. (See also Entropic gravity).    InstitutionalizationEdit   The first moves towards the institutionalization of scientific investigation and dissemination took the form of the establishment of societies, where new discoveries were aired, discussed and published. The first scientific society to be established was the Royal Society of England. This grew out of an earlier group, centred around Gresham College in the 1640s and 1650s. According to a history of the College:  the scientific network which centred on Gresham College played a crucial part in the meetings which led to the formation of the Royal Society.  These physicians and natural philosophers were influenced by the \"new science\", as promoted by Francis Bacon in his New Atlantis, from approximately 1645 onwards. A group known as The Philosophical Society of Oxford was run under a set of rules still retained by the Bodleian Library. On 28 November 1660, the 1660 committee of 12 announced the formation of a \"College for the Promoting of Physico-Mathematical Experimental Learning\", which would meet weekly to discuss science and run experiments. At the second meeting, Robert Moray announced that the King approved of the gatherings, and a Royal Charter was signed on 15 July 1662 which created the \"Royal Society of London\", with Lord Brouncker serving as the first President. A second Royal Charter was signed on 23 April 1663, with the King noted as the Founder and with the name of \"the Royal Society of London for the Improvement of Natural Knowledge\"; Robert Hooke was appointed as Curator of Experiments in November. This initial royal favour has continued, and since then every monarch has been the patron of the Society.  The Society's first Secretary was Henry Oldenburg. Its early meetings included experiments performed first by Robert Hooke and then by Denis Papin, who was appointed in 1684. These experiments varied in their subject area, and were both important in some cases and trivial in others. The society began publication of Philosophical Transactions from 1665, making it the oldest and longest-running scientific journal in the world, and the first journal to establish the tradition of peer review. and B, which deals with the biological sciences. The French established the Academy of Sciences in 1666. In contrast to the private origins of its British counterpart, the Academy was founded as a government body by Jean-Baptiste Colbert. Its rules were set down in 1699 by King Louis XIV, when it received the name of 'Royal Academy of Sciences' and was installed in the Louvre in Paris.    New ideasEdit  As the scientific revolution was not marked by any single change, the following new ideas contributed to what is called the scientific revolution. Many of them were revolutions in their own fields.    HeliocentrismEdit   For almost five millennia, the geocentric model of the Earth as the center of the universe had been accepted by all but a few astronomers. In Aristotle's cosmology, Earth's central location was perhaps less significant than its identification as a realm of imperfection, inconstancy, irregularity and change, as opposed to the \"heavens\", (Moon, Sun, planets, stars) which were regarded as perfect, permanent, unchangeable, and in religious thought, the realm of heavenly beings. The Earth was even composed of different material, the four elements \"earth\", \"water\", \"fire\", and \"air\", while sufficiently far above its surface (roughly the Moon's orbit), the heavens were composed of different substance called \"aether\". The heliocentric model that replaced it involved not only the radical displacement of the earth to an orbit around the sun, but its sharing a placement with the other planets implied a universe of heavenly components made from the same changeable substances as the Earth. Heavenly motions no longer needed to be governed by a theoretical perfection, confined to circular orbits. Copernicus' 1543 work on the heliocentric model of the solar system tried to demonstrate that the sun was the center of the universe. Few were bothered by this suggestion, and the pope and several archbishops were interested enough by it to want more detail. His model was later used to create the calendar of Pope Gregory XIII. However, the idea that the earth moved around the sun was doubted by most of Copernicus' contemporaries. It contradicted not only empirical observation, due to the absence of an observable stellar parallax, but more significantly at the time, the authority of Aristotle. The discoveries of Johannes Kepler and Galileo gave the theory credibility. Kepler was an astronomer who, using the accurate observations of Tycho Brahe, proposed that the planets move around the sun not in circular orbits, but in elliptical ones. Together with his other laws of planetary motion, this allowed him to create a model of the solar system that was an improvement over Copernicus' original system. Galileo's main contributions to the acceptance of the heliocentric system were his mechanics, the observations he made with his telescope, as well as his detailed presentation of the case for the system. Using an early theory of inertia, Galileo could explain why rocks dropped from a tower fall straight down even if the earth rotates. His observations of the moons of Jupiter, the phases of Venus, the spots on the sun, and mountains on the moon all helped to discredit the Aristotelian philosophy and the Ptolemaic theory of the solar system. Through their combined discoveries, the heliocentric system gained support, and at the end of the 17th century it was generally accepted by astronomers. This work culminated in the work of Isaac Newton. Newton's Principia formulated the laws of motion and universal gravitation, which dominated scientists' view of the physical universe for the next three centuries. By deriving Kepler's laws of planetary motion from his mathematical description of gravity, and then using the same principles to account for the trajectories of comets, the tides, the precession of the equinoxes, and other phenomena, Newton removed the last doubts about the validity of the heliocentric model of the cosmos. This work also demonstrated that the motion of objects on Earth and of celestial bodies could be described by the same principles. His prediction that the Earth should be shaped as an oblate spheroid was later vindicated by other scientists. His laws of motion were to be the solid foundation of mechanics; his law of universal gravitation combined terrestrial and celestial mechanics into one great system that seemed to be able to describe the whole world in mathematical formulae.    GravitationEdit   As well as proving the heliocentric model, Newton also developed the theory of gravitation. In 1679, Newton began to consider gravitation and its effect on the orbits of planets with reference to Kepler's laws of planetary motion. This followed stimulation by a brief exchange of letters in 1679–80 with Robert Hooke, who had been appointed to manage the Royal Society's correspondence, and who opened a correspondence intended to elicit contributions from Newton to Royal Society transactions.  Newton's reawakening interest in astronomical matters received further stimulus by the appearance of a comet in the winter of 1680–1681, on which he corresponded with John Flamsteed. After the exchanges with Hooke, Newton worked out proof that the elliptical form of planetary orbits would result from a centripetal force inversely proportional to the square of the radius vector (see Newton's law of universal gravitation – History and De motu corporum in gyrum). Newton communicated his results to Edmond Halley and to the Royal Society in De motu corporum in gyrum, in 1684. This tract contained the nucleus that Newton developed and expanded to form the Principia. The Principia was published on 5 July 1687 with encouragement and financial help from Edmond Halley. In this work, Newton stated the three universal laws of motion that contributed to many advances during the Industrial Revolution which soon followed and were not to be improved upon for more than 200 years. Many of these advancements continue to be the underpinnings of non-relativistic technologies in the modern world. He used the Latin word gravitas (weight) for the effect that would become known as gravity, and defined the law of universal gravitation. Newton's postulate of an invisible force able to act over vast distances led to him being criticised for introducing \"occult agencies\" into science. Later, in the second edition of the Principia (1713), Newton firmly rejected such criticisms in a concluding General Scholium, writing that it was enough that the phenomena implied a gravitational attraction, as they did; but they did not so far indicate its cause, and it was both unnecessary and improper to frame hypotheses of things that were not implied by the phenomena. (Here Newton used what became his famous expression \"hypotheses non fingo\").    Medical discoveriesEdit   The writings of Greek physician Galen had dominated European thinking in the subject for over a millennium. It was the publicized findings of the Italian scholar Vesalius that first demonstrated the mistakes in the Galenic model. His anatomical teachings were based upon the dissection of human corpses, rather than the animal dissections that Galen had used as a guide. Published in 1543, Vesalius' De humani corporis fabrica was a groundbreaking work of human anatomy. It emphasized the priority of dissection and what has come to be called the \"anatomical\" view of the body, seeing human internal functioning as an essentially corporeal structure filled with organs arranged in three-dimensional space. This was in stark contrast to many of the anatomical models used previously, which had strong Galenic/Aristotelean elements, as well as elements of astrology. Besides the first good description of the sphenoid bone, he showed that the sternum consists of three portions and the sacrum of five or six; and described accurately the vestibule in the interior of the temporal bone. He not only verified the observation of Etienne on the valves of the hepatic veins, but he described the vena azygos, and discovered the canal which passes in the fetus between the umbilical vein and the vena cava, since named ductus venosus. He described the omentum, and its connections with the stomach, the spleen and the colon; gave the first correct views of the structure of the pylorus; observed the small size of the caecal appendix in man; gave the first good account of the mediastinum and pleura and the fullest description of the anatomy of the brain yet advanced. He did not understand the inferior recesses; and his account of the nerves is confused by regarding the optic as the first pair, the third as the fifth and the fifth as the seventh. Further groundbreaking work was carried out by William Harvey, who published De Motu Cordis in 1628. Harvey made a detailed analysis of the overall structure of the heart, going on to an analysis of the arteries, showing how their pulsation depends upon the contraction of the left ventricle, while the contraction of the right ventricle propels its charge of blood into the pulmonary artery. He noticed that the two ventricles move together almost simultaneously and not independently like had been thought previously by his predecessors.  In the eighth chapter, Harvey estimated the capacity of the heart, how much blood is expelled through each pump of the heart, and the number of times the heart beats in a half an hour. From these estimations, he demonstrated that according to Gaelen's theory that blood was continually produced in the liver, the absurdly large figure of 540 pounds of blood would have to be produced every day. Having this simple but essential mathematical proportion at hand – which proved the overall impossible aforementioned role of the liver – Harvey went on to prove how the blood circulated in a circle by means of countless experiments initially done on serpents and fish: tying their veins and arteries in separate periods of time, Harvey noticed the modifications which occurred; indeed, as he tied the veins, the heart would become empty, while as he did the same to the arteries, the organ would swell up. This process was later performed on the human body (in the image on the left): the physician tied a tight ligature onto the upper arm of a person. This would cut off blood flow from the arteries and the veins. When this was done, the arm below the ligature was cool and pale, while above the ligature it was warm and swollen. The ligature was loosened slightly, which allowed blood from the arteries to come into the arm, since arteries are deeper in the flesh than the veins. When this was done, the opposite effect was seen in the lower arm. It was now warm and swollen. The veins were also more visible, since now they were full of blood. Various other advances in medical understanding and practice were made. French physician Pierre Fauchard started dentistry science as we know it today, and he has been named \"the father of modern dentistry\". Surgeon Ambroise Paré (c.1510–1590) was a leader in surgical techniques and battlefield medicine, especially the treatment of wounds, and Herman Boerhaave (1668–1738) is sometimes referred to as a \"father of physiology\" due to his exemplary teaching in Leiden and his textbook Institutiones medicae (1708).    ChemistryEdit   Chemistry, and its antecedent alchemy, became an increasingly important aspect of scientific thought in the course of the 16th and 17th centuries. The importance of chemistry is indicated by the range of important scholars who actively engaged in chemical research. Among them were the astronomer Tycho Brahe, the chemical physician Paracelsus, Robert Boyle, Thomas Browne and Isaac Newton. Unlike the mechanical philosophy, the chemical philosophy stressed the active powers of matter, which alchemists frequently expressed in terms of vital or active principles—of spirits operating in nature. Practical attempts to improve the refining of ores and their extraction to smelt metals was an important source of information for early chemists in the 16th century, among them Georg Agricola (1494–1555), who published his great work De re metallica in 1556. His work describes the highly developed and complex processes of mining metal ores, metal extraction and metallurgy of the time. His approach removed the mysticism associated with the subject, creating the practical base upon which others could build. English chemist Robert Boyle (1627–1691) is considered to have refined the modern scientific method for alchemy and to have separated chemistry further from alchemy. Although his research clearly has its roots in the alchemical tradition, Boyle is largely regarded today as the first modern chemist, and therefore one of the founders of modern chemistry, and one of the pioneers of modern experimental scientific method. Although Boyle was not the original discover, he is best known for Boyle's law, which he presented in 1662: the law describes the inversely proportional relationship between the absolute pressure and volume of a gas, if the temperature is kept constant within a closed system. Boyle is also credited for his landmark publication The Sceptical Chymist in 1661, which is seen as a cornerstone book in the field of chemistry. In the work, Boyle presents his hypothesis that every phenomenon was the result of collisions of particles in motion. Boyle appealed to chemists to experiment and asserted that experiments denied the limiting of chemical elements to only the classic four: earth, fire, air, and water. He also pleaded that chemistry should cease to be subservient to medicine or to alchemy, and rise to the status of a science. Importantly, he advocated a rigorous approach to scientific experiment: he believed all theories must be proved experimentally before being regarded as true. The work contains some of the earliest modern ideas of atoms, molecules, and chemical reaction, and marks the beginning of the history of modern chemistry.    OpticsEdit   Important work was done in the field of optics. Johannes Kepler published Astronomiae Pars Optica (The Optical Part of Astronomy) in 1604. In it, he described the inverse-square law governing the intensity of light, reflection by flat and curved mirrors, and principles of pinhole cameras, as well as the astronomical implications of optics such as parallax and the apparent sizes of heavenly bodies. Astronomiae Pars Optica is generally recognized as the foundation of modern optics (though the law of refraction is conspicuously absent). Willebrord Snellius (1580–1626) found the mathematical law of refraction, now known as Snell's law, in 1621. Subsequently René Descartes (1596–1650) showed, by using geometric construction and the law of refraction (also known as Descartes' law), that the angular radius of a rainbow is 42° (i.e. the angle subtended at the eye by the edge of the rainbow and the rainbow's centre is 42°). He also independently discovered the law of reflection, and his essay on optics was the first published mention of this law. Christiaan Huygens (1629–1695) wrote several works in the area of optics. These included the Opera reliqua (also known as Christiani Hugenii Zuilichemii, dum viveret Zelhemii toparchae, opuscula posthuma) and the Traité de la lumière. Isaac Newton investigated the refraction of light, demonstrating that a prism could decompose white light into a spectrum of colours, and that a lens and a second prism could recompose the multicoloured spectrum into white light. He also showed that the coloured light does not change its properties by separating out a coloured beam and shining it on various objects. Newton noted that regardless of whether it was reflected or scattered or transmitted, it stayed the same colour. Thus, he observed that colour is the result of objects interacting with already-coloured light rather than objects generating the colour themselves. This is known as Newton's theory of colour. From this work he concluded that any refracting telescope would suffer from the dispersion of light into colours. The interest of the Royal Society encouraged him to publish his notes On Colour (later expanded into Opticks). Newton argued that light is composed of particles or corpuscles and were refracted by accelerating toward the denser medium, but he had to associate them with waves to explain the diffraction of light. In his Hypothesis of Light of 1675, Newton posited the existence of the ether to transmit forces between particles. In 1704, Newton published Opticks, in which he expounded his corpuscular theory of light. He considered light to be made up of extremely subtle corpuscles, that ordinary matter was made of grosser corpuscles and speculated that through a kind of alchemical transmutation \"Are not gross Bodies and Light convertible into one another, ...and may not Bodies receive much of their Activity from the Particles of Light which enter their Composition?\"    ElectricityEdit   Dr. William Gilbert, in De Magnete, invented the New Latin word electricus from ἤλεκτρον (elektron), the Greek word for \"amber\". Gilbert undertook a number of careful electrical experiments, in the course of which he discovered that many substances other than amber, such as sulphur, wax, glass, etc., were capable of manifesting electrical properties. Gilbert also discovered that a heated body lost its electricity and that moisture prevented the electrification of all bodies, due to the now well-known fact that moisture impaired the insulation of such bodies. He also noticed that electrified substances attracted all other substances indiscriminately, whereas a magnet only attracted iron. The many discoveries of this nature earned for Gilbert the title of founder of the electrical science. By investigating the forces on a light metallic needle, balanced on a point, he extended the list of electric bodies, and found also that many substances, including metals and natural magnets, showed no attractive forces when rubbed. He noticed that dry weather with north or east wind was the most favourable atmospheric condition for exhibiting electric phenomena—an observation liable to misconception until the difference between conductor and insulator was understood. Robert Boyle also worked frequently at the new science of electricity, and added several substances to Gilbert's list of electrics. He left a detailed account of his researches under the title of Experiments on the Origin of Electricity. Boyle, in 1675, stated that electric attraction and repulsion can act across a vacuum. One of his important discoveries was that electrified bodies in a vacuum would attract light substances, this indicating that the electrical effect did not depend upon the air as a medium. He also added resin to the then known list of electrics. This was followed in 1660 by Otto von Guericke, who invented an early electrostatic generator. By the end of the 17th Century, researchers had developed practical means of generating electricity by friction with an electrostatic generator, but the development of electrostatic machines did not begin in earnest until the 18th century, when they became fundamental instruments in the studies about the new science of electricity. The first usage of the word electricity is ascribed to Sir Thomas Browne in his 1646 work, Pseudodoxia Epidemica. In 1729 Stephen Gray (1666–1736) demonstrated that electricity could be \"transmitted\" through metal filaments.    New mechanical devicesEdit   As an aid to scientific investigation, various tools, measuring aids and calculating devices were developed in this period. Refracting telescopes first appeared in the Netherlands in 1608. The spectacle makers Hans Lippershey, Zacharias Janssen and Jacob Metius of Alkmaar all contributed to its invention. Galileo was one of the first scientists to use this new tool for his astronomical observations in 1609. The reflecting telescope was described by James Gregory in his book Optica Promota (1663). He argued that a mirror shaped like the part of a conic section, would correct the spherical aberration that flawed the accuracy of refracting telescopes. His design, the \"Gregorian telescope\", however, remained un-built. In 1666, Isaac Newton argued that the faults of the refracting telescope were fundamental because the lens refracted light of different colors differently. He concluded that light could not be refracted through a lens without causing chromatic aberrations From these experiments Newton concluded that no improvement could be made in the refracting telescope. However, he was able to demonstrate that the angle of reflection remained the same for all colors, so he decided to build a reflecting telescope. It was completed in 1668 and is the earliest known functional reflecting telescope. 50 years later, John Hadley developed ways to make precision aspheric and parabolic objective mirrors for reflecting telescopes, building the first parabolic Newtonian telescope and a Gregorian telescope with accurately shaped mirrors. These were successfully demonstrated to the Royal Society. The invention of the vacuum pump paved the way for the experiments of Robert Boyle and Robert Hooke into the nature of vacuum and atmospheric pressure. The first such device was made by Otto von Guericke in 1654. It consisted of a piston and an air gun cylinder with flaps that could suck the air from any vessel that that it was connected to. In 1657, he pumped the air out of two cojoined hemispheres and demonstrated that a team of sixteen horses were incapable of pulling it apart. The air pump construction was greatly improved by Robert Hooke in 1658. Evangelista Torricelli (1607–1647) was best known for his invention of the mercury barometer. The motivation for the invention was to improve on the suction pumps that were used to raise water out of the mines. Torricelli constructed a sealed tube filled with mercury, set vertically into a basin of the same substance. The column of mercury fell downwards, leaving a Torricellian vacuum above.    Calculating devicesEdit   John Napier invented logarithms as a powerful mathematical tool. With the help of the prominent mathematician Henry Briggs their logarithmic tables embodied a computational advance that made calculations by hand much quicker. His Napier's bones used a set of numbered rods as a multiplication tool using the system of lattice multiplication. The way was opened to later scientific advances, particularly in astronomy and dynamics. At Oxford University, Edmund Gunter built the first analog device to aid computation. The 'Gunter's scale' was a large plane scale, engraved with various scales, or lines. Natural lines, such as the line of chords, the line of sines and tangents are placed on one side of the scale and the corresponding artificial or logarithmic ones were on the other side. This calculating aid was a predecessor of the slide rule. It was William Oughtred (1575–1660) who first used two such scales sliding by one another to perform direct multiplication and division, and thus is credited as the inventor of the slide rule in 1622. Blaise Pascal (1623–1662) invented the mechanical calculator in 1642. The introduction of his Pascaline in 1645 launched the development of mechanical calculators first in Europe and then all over the world. Gottfried Leibniz (1646–1716), building on Pascal's work, became one of the most prolific inventors in the field of mechanical calculators; he was the first to describe a pinwheel calculator, in 1685, and invented the Leibniz wheel, used in the arithmometer, the first mass-produced mechanical calculator. He also refined the binary number system, foundation of virtually all modern computer architectures. John Hadley (1682–1744) was the inventor of the octant, the precursor to the sextant (invented by John Bird), which greatly improved the science of navigation.    Industrial machinesEdit   Denis Papin (1647–1712) was best known for his pioneering invention of the steam digester, the forerunner of the steam engine. The first working steam engine was patented in 1698 by the inventor Thomas Savery, as a \"...new invention for raising of water and occasioning motion to all sorts of mill work by the impellent force of fire, which will be of great use and advantage for drayning mines, serveing townes with water, and for the working of all sorts of mills where they have not the benefitt of water nor constant windes.\" [sic] The invention was demonstrated to the Royal Society on 14 June 1699 and the machine was described by Savery in his book The Miner's Friend; or, An Engine to Raise Water by Fire (1702), in which he claimed that it could pump water out of mines. Thomas Newcomen (1664–1729) perfected the practical steam engine for pumping water, the Newcomen steam engine. Consequently, he can be regarded as a forefather of the Industrial Revolution. Abraham Darby I (1678–1717) was the first, and most famous, of three generations of the Darby family who played an important role in the Industrial Revolution. He developed a method of producing high-grade iron in a blast furnace fueled by coke rather than charcoal. This was a major step forward in the production of iron as a raw material for the Industrial Revolution.    Scientific developmentsEdit  Key ideas and people that emerged from the 16th and 17th centuries: First printed edition of Euclid's Elements in 1482. Nicolaus Copernicus (1473–1543) published On the Revolutions of the Heavenly Spheres in 1543, which advanced the heliocentric theory of cosmology. Andreas Vesalius (1514–1564) published De Humani Corporis Fabrica (On the Structure of the Human Body) (1543), which discredited Galen's views. He found that the circulation of blood resolved from pumping of the heart. He also assembled the first human skeleton from cutting open cadavers. Franciscus Vieta (1540–1603) published In Artem Analycitem Isagoge (1591), which gave the first symbolic notation of parameters in literal algebra. William Gilbert (1544–1603) published On the Magnet and Magnetic Bodies, and on the Great Magnet the Earth in 1600, which laid the foundations of a theory of magnetism and electricity. Tycho Brahe (1546–1601) made extensive and more accurate naked eye observations of the planets in the late 16th century. These became the basic data for Kepler's studies. Sir Francis Bacon (1561–1626) published Novum Organum in 1620, which outlined a new system of logic based on the process of reduction, which he offered as an improvement over Aristotle's philosophical process of syllogism. This contributed to the development of what became known as the scientific method. Galileo Galilei (1564–1642) improved the telescope, with which he made several important astronomical observations, including the four largest moons of Jupiter, the phases of Venus, and the rings of Saturn, and made detailed observations of sunspots. He developed the laws for falling bodies based on pioneering quantitative experiments which he analyzed mathematically. Johannes Kepler (1571–1630) published the first two of his three laws of planetary motion in 1609. William Harvey (1578–1657) demonstrated that blood circulates, using dissections and other experimental techniques. René Descartes (1596–1650) published his Discourse on the Method in 1637, which helped to establish the scientific method. Antonie van Leeuwenhoek (1632–1723) constructed powerful single lens microscopes and made extensive observations that he published around 1660, opening up the micro-world of biology. Isaac Newton (1643–1727) built upon the work of Kepler and Galileo. He showed that an inverse square law for gravity explained the elliptical orbits of the planets, and advanced the law of universal gravitation. His development of infinitesimal calculus opened up new applications of the methods of mathematics to science. Newton taught that scientific theory should be coupled with rigorous experimentation, which became the keystone of modern science.    Contrary viewsEdit   Not all historians of science agree that there was any revolution in the 16th or 17th century. The continuity thesis is the hypothesis that there was no radical discontinuity between the intellectual development of the Middle Ages and the developments in the Renaissance and early modern period. Thus the idea of an intellectual or scientific revolution following the Renaissance is—according to the continuity thesis—a myth. Some continuity theorists point to earlier intellectual revolutions occurring in the Middle Ages, usually referring to either a European \"Renaissance of the 12th century\" or a medieval \"Muslim scientific revolution\", as a sign of continuity. Another contrary view has been recently proposed by Arun Bala in his dialogical history of the birth of modern science. Bala proposes that the changes involved in the Scientific Revolution—the mathematical realist turn, the mechanical philosophy, the atomism, the central role assigned to the Sun in Copernican heliocentrism—have to be seen as rooted in multicultural influences on Europe. He sees specific influences in Alhazen's physical optical theory, Chinese mechanical technologies leading to the perception of the world as a machine, the Hindu-Arabic numeral system, which carried implicitly a new mode of mathematical atomic thinking, and the heliocentrism rooted in ancient Egyptian religious ideas associated with Hermeticism. Bala argues that by ignoring such multicultural impacts we have been led to a Eurocentric conception of the scientific revolution. However he clearly states: \"The makers of the revolution – Copernicus, Kepler, Galileo, Descartes, Newton, and many others – had to selectively appropriate relevant ideas, transform them, and create new auxiliary concepts in order to complete their task... In the ultimate analysis, even if the revolution was rooted upon a multicultural base it is the accomplishment of Europeans in Europe.\" Critics note that lacking documentary evidence of transmission of specific scientific ideas, Bala's model will remain \"a working hypothesis, not a conclusion\". A third approach takes the term \"Renaissance\" literally as a \"rebirth\". A closer study of Greek Philosophy and Greek Mathematics demonstrates that nearly all of the so-called revolutionary results of the so-called scientific revolution were in actuality restatements of ideas that were in many cases older than those of Aristotle and in nearly all cases at least as old as Archimedes. Aristotle even explicitly argues against some of the ideas that were demonstrated during the scientific revolution, such as heliocentrism. The basic ideas of the scientific method were well known to Archimedes and his contemporaries, as demonstrated in the well known discovery of buoyancy. Atomism was first thought of by Leucippus and Democritus. This view of the scientific revolution reduces it to a period of relearning classical ideas that is very much an extension of the Renaissance. This view of the scientific revolution does not deny that a change occurred but argues that it was a reassertion of previous knowledge (a renaissance) and not the creation of new knowledge. It cites statements from Newton, Copernicus and others in favour of the Pythagorean worldview as evidence.     See alsoEdit     RevolutionsEdit  Revolution British Agricultural Revolution/Neolithic Revolution Industrial Revolution Commercial Revolution Digital Revolution Chemical Revolution Information Revolution    ReferencesEdit     SourcesEdit","label":"foo"},{"text":"Natural science is a branch of science concerned with the description, prediction, and understanding of natural phenomena, based on observational and empirical evidence. Validity, accuracy, and social mechanisms ensuring quality control, such as peer review and repeatability of findings, are amongst the criteria and methods used for this purpose. Natural science can be broken into two main branches: life science (or biological science) and physical science. Physical science is further broken down into branches, including physics, astronomy, chemistry, and Earth science. All of these branches of natural science are divided into many further specialized branches (also known as fields), and each of these is known as a \"natural science\". In Western society's analytic tradition, the empirical and especially natural sciences use tools from formal sciences, such as mathematics and logic, converting information about nature into measurements which can be explained as clear statements about the \"laws of nature\". The social sciences also use such methods, but rely more on qualitative research, so that they are sometimes called \"soft science\", whereas natural sciences, insofar as emphasizing quantifiable data produced, tested, and confirmed through the scientific method are sometimes called \"hard science\". Modern natural science succeeded more classical approaches to natural philosophy, usually traced to ancient Greece. Galileo, Descartes, Francis Bacon, and Newton debated the benefits of using approaches which were more mathematical and more experimental in a methodical way. Still, philosophical perspectives, conjectures, and presuppositions, often overlooked, remain requisite in natural science. Systematic data collection, including discovery science, succeed natural history, which emerged in the 16th century by describing and classifying plants, animals, minerals, and so on. Yet today, natural history suggests observational descriptions aimed at popular audiences.    CriteriaEdit   Philosophers of science have suggested a number of criteria, including the Karl Popper's controversial falsifiability criterion, to help them differentiate scientific endeavors from a non-scientific ones. Validity, accuracy, and social mechanisms ensuring quality control, such as peer review and repeatability of findings, are amongst the most respected criteria in present-day global scientific community.    Branches of natural scienceEdit     BiologyEdit   This field encompasses a set of disciplines that examines phenomena related to living organisms. The scale of study can range from sub-component biophysics up to complex ecologies. Biology is concerned with the characteristics, classification and behaviors of organisms, as well as how species were formed and their interactions with each other and the environment. The biological fields of botany, zoology, and medicine date back to early periods of civilization, while microbiology was introduced in the 17th century with the invention of the microscope. However, it was not until the 19th century that biology became a unified science. Once scientists discovered commonalities between all living things, it was decided they were best studied as a whole. Some key developments in biology were the discovery of genetics; Darwin's theory of evolution through natural selection; the germ theory of disease and the application of the techniques of chemistry and physics at the level of the cell or organic molecule. Modern biology is divided into subdisciplines by the type of organism and by the scale being studied. Molecular biology is the study of the fundamental chemistry of life, while cellular biology is the examination of the cell; the basic building block of all life. At a higher level, physiology looks at the internal structure of organism, while ecology looks at how various organisms interrelate.    ChemistryEdit   Constituting the scientific study of matter at the atomic and molecular scale, chemistry deals primarily with collections of atoms, such as gases, molecules, crystals, and metals. The composition, statistical properties, transformations and reactions of these materials are studied. Chemistry also involves understanding the properties and interactions of individual atoms and molecules for use in larger-scale applications. Most chemical processes can be studied directly in a laboratory, using a series of (often well-tested) techniques for manipulating materials, as well as an understanding of the underlying processes. Chemistry is often called \"the central science\" because of its role in connecting the other natural sciences. Early experiments in chemistry had their roots in the system of Alchemy, a set of beliefs combining mysticism with physical experiments. The science of chemistry began to develop with the work of Robert Boyle, the discoverer of gas, and Antoine Lavoisier, who developed the theory of the Conservation of mass. The discovery of the chemical elements and atomic theory began to systematize this science, and researchers developed a fundamental understanding of states of matter, ions, chemical bonds and chemical reactions. The success of this science led to a complementary chemical industry that now plays a significant role in the world economy.    PhysicsEdit   Physics embodies the study of the fundamental constituents of the universe, the forces and interactions they exert on one another, and the results produced by these interactions. In general, physics is regarded as the fundamental science, because all other natural sciences use and obey the principles and laws set down by the field. Physics relies heavily on mathematics as the logical framework for formulation and quantification of principles. The study of the principles of the universe has a long history and largely derives from direct observation and experimentation. The formulation of theories about the governing laws of the universe has been central to the study of physics from very early on, with philosophy gradually yielding to systematic, quantitative experimental testing and observation as the source of verification. Key historical developments in physics include Isaac Newton's theory of universal gravitation and classical mechanics, an understanding of electricity and its relation to magnetism, Einstein's theories of special and general relativity, the development of thermodynamics, and the quantum mechanical model of atomic and subatomic physics. The field of physics is extremely broad, and can include such diverse studies as quantum mechanics and theoretical physics, applied physics and optics. Modern physics is becoming increasingly specialized, where researchers tend to focus on a particular area rather than being \"universalists\" like Isaac Newton, Albert Einstein and Lev Landau, who worked in multiple areas.   = AstronomyEdit =  This discipline is the science of celestial objects and phenomena that originate outside the Earth's atmosphere. It is concerned with the evolution, physics, chemistry, meteorology, and motion of celestial objects, as well as the formation and development of the universe. Astronomy includes the examination, study and modeling of stars, planets, comets, galaxies and the cosmos. Most of the information used by astronomers is gathered by remote observation, although some laboratory reproduction of celestial phenomena has been performed (such as the molecular chemistry of the interstellar medium). While the origins of the study of celestial features and phenomena can be traced back to antiquity, the scientific methodology of this field began to develop in the middle of the 17th century. A key factor was Galileo's introduction of the telescope to examine the night sky in more detail. The mathematical treatment of astronomy began with Newton's development of celestial mechanics and the laws of gravitation, although it was triggered by earlier work of astronomers such as Kepler. By the 19th century, astronomy had developed into a formal science, with the introduction of instruments such as the spectroscope and photography, along with much-improved telescopes and the creation of professional observatories.    Earth scienceEdit   Earth science (also known as geoscience), is an all-embracing term for the sciences related to the planet Earth, including geology, geophysics, hydrology, meteorology, physical geography, oceanography, and soil science. Although mining and precious stones have been human interests throughout the history of civilization, the development of the related sciences of economic geology and mineralogy did not occur until the 18th century. The study of the earth, particularly palaeontology, blossomed in the 19th century. The growth of other disciplines, such as geophysics, in the 20th century led to the development of the theory of plate tectonics in the 1960s, which has had a similar effect on the Earth sciences as the theory of evolution had on biology. Earth sciences today are closely linked to petroleum and mineral resources, climate research and to environmental assessment and remediation.   = Atmospheric scienceEdit =  Though sometimes considered in conjunction with the earth sciences, due to the independent development of its concepts, techniques and practices and also the fact of it having a wide range of sub disciplines under its wing, the atmospheric science is also considered a separate branch of natural science. This field studies the characteristics of different layers of the atmosphere from ground level to the edge of the time. The timescale of study also varies from days to centuries. Sometimes the field also includes the study of climatic patterns on planets other than earth.   = OceanographyEdit =  The serious study of oceans began in the early to mid-20th century. As a field of natural science, it is relatively young but stand-alone programs offer specializations in the subject. Though some controversies remain as to the categorization of the field under earth sciences, interdisciplinary sciences or as a separate field in its own right, most modern workers in the field agree that it has matured to a state that it has its own paradigms and practices. As such a big family of related studies spanning every aspect of the oceans is now classified under this field.    Interdisciplinary studiesEdit  The distinctions between the natural science disciplines are not always sharp, and they share a number of cross-discipline fields. Physics plays a significant role in the other natural sciences, as represented by astrophysics, geophysics, chemical physics and biophysics. Likewise chemistry is represented by such fields as biochemistry, chemical biology, geochemistry and astrochemistry. A particular example of a scientific discipline that draws upon multiple natural sciences is environmental science. This field studies the interactions of physical, chemical, geological, and biological components of the environment, with a particular regard to the effect of human activities and the impact on biodiversity and sustainability. This science also draws upon expertise from other fields such as economics, law and social sciences. A comparable discipline is oceanography, as it draws upon a similar breadth of scientific disciplines. Oceanography is sub-categorized into more specialized cross-disciplines, such as physical oceanography and marine biology. As the marine ecosystem is very large and diverse, marine biology is further divided into many subfields, including specializations in particular species. There are also a subset of cross-disciplinary fields which, by the nature of the problems that they address, have strong currents that run counter to specialization. Put another way: In some fields of integrative application, specialists in more than one field are a key part of most dialog. Such integrative fields, for example, include nanoscience, astrobiology, and complex system informatics.    Materials scienceEdit   Materials science is a relatively new, interdisciplinary field which deals with the study of matter and their properties; as well as the discovery and design of new materials. Originally developed through the field of metallurgy, the study of the properties of materials and solids has now expanded into all materials. The field covers the chemistry, physics and engineering applications of materials including metals, ceramics, artificial polymers, and many others. The core of the field deals with relating structure of material with it properties. It is at the forefront of research in science and engineering. It is an important part of forensic engineering (the investigation of materials, products, structures or components that fail or do not operate or function as intended, causing personal injury or damage to property) and failure analysis, the latter being the key to understanding, for example, the cause of various aviation accidents. Many of the most pressing scientific problems that are faced today are due to the limitations of the materials that are available and, as a result, breakthroughs in this field are likely to have a significant impact on the future of technology. The basis of materials science involves studying the structure of materials, and relating them to their properties. Once, a materials scientists knows about this structure-property correlation, he/she can then go on to study the relative performance of a material in a certain application. The major determinants of the structure of a material and thus of its properties are its constituent chemical elements and the way in which it has been processed into its final form. These characteristics, taken together and related through the laws of thermodynamics and kinetics, govern a material's microstructure, and thus its properties.    HistoryEdit   Some scholars trace the origins of natural science as far back as pre-literate human societies, where understanding the natural world was necessary for survival. People observed and built up knowledge about the behavior of animals and the usefulness of plants as food and medicine, which was passed down from generation to generation. These primitive understandings gave way to more formalized inquiry around 3500 to 3000 BC in Mesopotamian and Ancient Egyptian cultures, which produced the first known written evidence of natural philosophy, the precursor of natural science. While the writings show an interest in astronomy, mathematics and other aspects of the physical world, the ultimate aim of inquiry about nature's workings was in all cases religious or mythological, not scientific. A tradition of scientific inquiry also emerged in Ancient China, where Taoist alchemists and philosophers experimented with elixirs to extend life and cure ailments. They focused on the yin and yang, or contrasting elements in nature; the yin was associated with femininity and coldness, while yang was associated with masculinity and warmth. The five phases – fire, earth, metal, wood and water – described a cycle of transformations in nature. Water turned into wood, which turned into fire when it burned. The ashes left by fire were earth. Using these principles, Chinese philosophers and doctors explored human anatomy, characterizing organs as predominantly yin or yang and understood the relationship between the pulse, the heart and the flow of blood in the body centuries before it became accepted in the West. Little evidence survives of how Ancient Indian cultures around the Indus River understood nature, but some of their perspectives may be reflected in the Vedas, a set of sacred Hindu texts. They reveal a conception of the universe as ever-expanding and constantly being recycled and reformed. Surgeons in the Ayurvedic tradition saw health and illness as a combination of three humors: wind, bile and phlegm. A healthy life was the result of a balance between these humors. In Ayurvedic thought, the body consisted of five elements: earth, water, fire, wind and empty space. Ayurvedic surgeons performed complex surgeries and developed a detailed understanding of human anatomy. Pre-Socratic philosophers in Ancient Greek culture brought natural philosophy a step closer to direct inquiry about cause and effect in nature between 600 and 400 BC, although an element of magic and mythology remained. Natural phenomena such as earthquakes and eclipses were explained increasingly in the context of nature itself instead of being attributed to angry gods. Thales of Miletus, an early philosopher who lived from 625 to 546 BC, explained earthquakes by theorizing that the world floated on water and that water was the fundamental element in nature. In the 5th century BC, Leucippus was an early exponent of atomism, the idea that the world is made up of fundamental indivisible particles. Pythagoras applied Greek innovations in mathematics to astronomy, and suggested that the earth was spherical. Specific practical discoveries, useful and impressive as they may be, neither qualify as science nor do they succeed in establishing a paradigm. Awareness of the sine qua non relation of formal science to observation and, mainly, quantitative studies has proven to be difficult to emerge. Pythagoreans came close to it when they applied numbers to the study of natural phenomena, however they do not appear to have been aware of the importance of methodology as distinct from concrete manipulations. A recent interpretation of Parmenides presents evidence that he was the philosopher who first proposed a method for doing natural science. Although 'peri physeos' is a poem, it may be viewed as an epistemological essay, an essay on method. Parmenides' ἐὸν may refer to a formal system, a calculus which, if 'superimposed' on empirical observations, can describe nature more precisely than natural languages. 'Physis' may be identical to ἐὸν.    Aristotelian natural philosophy (400 BC–1100 AD)Edit  Later Socratic and Platonic thought focused on ethics, morals and art and did not attempt an investigation of the physical world; Plato criticized pre-Socratic thinkers as materialists and anti-religionists. Aristotle, however, a student of Plato who lived from 384 to 322 BC, paid closer attention to the natural world in his philosophy. In his History of Animals, he described the inner workings of 110 species, including the stingray, catfish and bee. He investigated chick embryos by breaking open eggs and observing them at various stages of development. Aristotle's works were influential through the 19th century, and he is considered to be the father of biology. He also presented philosophies about physics, nature and astronomy using inductive reasoning in his works Physics and Meteorology.  While Aristotle considered natural philosophy more seriously than his predecessors, he approached it as a theoretical branch of science. Still, inspired by his work, Ancient Roman philosophers of the early 1st century AD, including Lucretius, Seneca and Pliny the Elder, wrote treatises that dealt with the rules of the natural world in varying degrees of depth. Many Ancient Roman Neoplatonists of the 3rd to the 6th centuries also adapted Aristotle's teachings on the physical world to a philosophy that emphasized spiritualism. Early medieval philosophers including Macrobius, Calcidius and Martianus Capella also examined the physical world, largely from a cosmological and cosmographical perspective, putting forth theories on the arrangement of celestial bodies and the heavens, which were posited as being composed of aether. Aristotle's works on natural philosophy continued to be translated and studied amid the rise of the Byzantine Empire and Islam in the Middle East. A revival in mathematics and science took place during the time of the Abbasid Caliphate from the 9th century onward, when Muslim scholars expanded upon Greek and Indian natural philosophy. The words alcohol, algebra and zenith all have Arabic roots.    Medieval natural philosophy (1100–1600)Edit  Aristotle's works and other Greek natural philosophy did not reach the West until about the middle of the 12th century, when works were translated from Greek and Arabic into Latin. The development of European civilization later in the Middle Ages brought with it further advances in natural philosophy. European inventions such as the horseshoe, horse collar and crop rotation allowed for rapid population growth, eventually giving way to urbanization and the foundation of schools connected to monasteries and cathedrals in modern-day France and England. Aided by the schools, an approach to Christian theology developed that sought to answer questions about nature and other subjects using logic. This approach, however, was seen by some detractors as heresy. By the 12th century, Western European scholars and philosophers came into contact with a body of knowledge of which they had previously been ignorant: a large corpus of works in Greek and Arabic that were preserved by Islamic scholars. Through translation into Latin, Western Europe was introduced to Aristotle and his natural philosophy. These works were taught at new universities in Paris and Oxford by the early 13th century, although the practice was frowned upon by the Catholic church. A 1210 decree from the Synod of Paris ordered that \"no lectures are to be held in Paris either publicly or privately using Aristotle's books on natural philosophy or the commentaries, and we forbid all this under pain of excommunication.\" In the late Middle Ages, Spanish philosopher Dominicus Gundissalinus translated a treatise by the earlier Arab scholar Al-Farabi called On the Sciences into Latin, calling the study of the mechanics of nature scientia naturalis, or natural science. Gundissalinus also proposed his own classification of the natural sciences in his 1150 work On the Division of Philosophy. This was the first detailed classification of the sciences based on Greek and Arab philosophy to reach Western Europe. Gundissalinus defined natural science as \"the science considering only things unabstracted and with motion,\" as opposed to mathematics and sciences that rely on mathematics. Following Al-Farabi, he then separated the sciences into eight parts, including physics, cosmology, meteorology, minerals science and plant and animal science. Later philosophers made their own classifications of the natural sciences. Robert Kilwardby wrote On the Order of the Sciences in the 13th century that classed medicine as a mechanical science, along with agriculture, hunting and theater while defining natural science as the science that deals with bodies in motion. Roger Bacon, an English friar and philosopher, wrote that natural science dealt with \"a principle of motion and rest, as in the parts of the elements of fire, air, earth and water, and in all inanimate things made from them.\" These sciences also covered plants, animals and celestial bodies. Later in the 13th century, Catholic priest and theologian Thomas Aquinas defined natural science as dealing with \"mobile beings\" and \"things which depend on matter not only for their existence, but also for their definition.\" There was wide agreement among scholars in medieval times that natural science was about bodies in motion, although there was division about the inclusion of fields including medicine, music and perspective. Philosophers pondered questions including the existence of a vacuum, whether motion could produce heat, the colors of rainbows, the motion of the earth, whether elemental chemicals exist and where in the atmosphere rain is formed. In the centuries up through the end of the Middle Ages, natural science was often mingled with philosophies about magic and the occult. Natural philosophy appeared in a wide range of forms, from treatises to encyclopedias to commentaries on Aristotle. The interaction between natural philosophy and Christianity was complex during this period; some early theologians, including Tatian and Eusebius, considered natural philosophy an outcropping of pagan Greek science and were suspicious of it. Although some later Christian philosophers, including Aquinas, came to see natural science as a means of interpreting scripture, this suspicion persisted until the 12th and 13th centuries. The Condemnation of 1277, which forbade setting philosophy on a level equal with theology and the debate of religious constructs in a scientific context, showed the persistence with which Catholic leaders resisted the development of natural philosophy even from a theological perspective. Aquinas and Albertus Magnus, another Catholic theologian of the era, sought to distance theology from science in their works. \"I don't see what one's interpretation of Aristotle has to do with the teaching of the faith,\" he wrote in 1271.    Newton and the scientific revolution (1600–1800)Edit  By the 16th and 17th centuries, natural philosophy underwent an evolution beyond commentary on Aristotle as more early Greek philosophy was uncovered and translated. The invention of the printing press in the 15th century, the invention of the microscope and telescope, and the Protestant Reformation fundamentally altered the social context in which scientific inquiry evolved in the West. Christopher Columbus's discovery of a new world changed perceptions about the physical makeup of the world, while observations by Copernicus, Tyco Brahe and Galileo brought a more accurate picture of the solar system as heliocentric and proved many of Aristotle's theories about the heavenly bodies false. A number of 17th-century philosophers, including Thomas Hobbes, John Locke and Francis Bacon made a break from the past by rejecting Aristotle and his medieval followers outright, calling their approach to natural philosophy as superficial. The titles of Galileo's work Two New Sciences and Johannes Kepler's New Astronomy underscored the atmosphere of change that took hold in the 17th century as Aristotle was dismissed in favor of novel methods of inquiry into the natural world. Bacon was instrumental in popularizing this change; he argued that people should use the arts and sciences to gain dominion over nature. To achieve this, he wrote that \"human life [must] be endowed with new discoveries and powers.\" He defined natural philosophy as \"the knowledge of Causes and secret motions of things; and enlarging the bounds of Human Empire, to the effecting of all things possible.\" Bacon proposed scientific inquiry supported by the state and fed by the collaborative research of scientists, a vision that was unprecedented in its scope, ambition and form at the time. Natural philosophers came to view nature increasingly as a mechanism that could be taken apart and understood, much like a complex clock. Natural philosophers including Isaac Newton, Evangelista Torricelli and Francesco Redi conducted experiments focusing on the flow of water, measuring atmospheric pressure using a barometer and disproving spontaneous generation. Scientific societies and scientific journals emerged and were spread widely through the printing press, touching off the scientific revolution. Newton in 1687 published his The Mathematical Principles of Natural Philosophy, or Principia Mathematica, which set the groundwork for physical laws that remained current until the 19th century. Some modern scholars, including Andrew Cunningham, Perry Williams and Floris Cohen, argue that natural philosophy is not properly called a science, and that genuine scientific inquiry began only with the scientific revolution. According to Cohen, \"the emancipation of science from an overarching entity called 'natural philosophy' is one defining characteristic of the Scientific Revolution.\" Other historians of science, including Edward Grant, contend that the scientific revolution that blossomed in the 17th, 18th and 19th centuries occurred when principles learned in the exact sciences of optics, mechanics and astronomy began to be applied to questions raised by natural philosophy. Grant argues that Newton attempted to expose the mathematical basis of nature – the immutable rules it obeyed – and in doing so joined natural philosophy and mathematics for the first time, producing an early work of modern physics.  The scientific revolution, which began to take hold in the 17th century, represented a sharp break from Aristotelian modes of inquiry. One of its principal advances was the use of the scientific method to investigate nature. Data was collected and repeatable measurements made in experiments. Scientists then formed hypotheses to explain the results of these experiments. The hypothesis was then tested using the principle of falsifiability to prove or disprove its accuracy. The natural sciences continued to be called natural philosophy, but the adoption of the scientific method took science beyond the realm of philosophical conjecture and introduced a more structured way of examining nature. Newton, an English mathematician and physicist, was the seminal figure in the scientific revolution. Drawing on advances made in astronomy by Copernicus, Brahe and Kepler, Newton derived the universal law of gravitation and laws of motion. These laws applied both on earth and in outer space, uniting two spheres of the physical world previously thought to function independently of each other, according to separate physical rules. Newton, for example, showed that the tides were caused by the gravitational pull of the moon. Another of Newton's advances was to make mathematics a powerful explanatory tool for natural phenomena. While natural philosophers had long used mathematics as a means of measurement and analysis, its principles were not used as a means of understanding cause and effect in nature until Newton. In the 18th century and 19th century, scientists including Charles-Augustin de Coulomb, Alessandro Volta, and Michael Faraday built upon Newtonian mechanics by exploring electromagnetism, or the interplay of forces with positive and negative charges on electrically charged particles. Faraday proposed that forces in nature operated in \"fields\" that filled space. The idea of fields contrasted with the Newtonian construct of gravitation as simply \"action at a distance\", or the attraction of objects with nothing in the space between them to intervene. James Clerk Maxwell in the 19th century unified these discoveries in a coherent theory of electrodynamics. Using mathematical equations and experimentation, Maxwell discovered that space was filled with charged particles that could act upon themselves and each other, and that they were a medium for the transmission of charged waves. Significant advances in chemistry also took place during the scientific revolution. Antoine Lavoisier, a French chemist, refuted the phlogiston theory, which posited that things burned by releasing \"phlogiston\" into the air. Joseph Priestley had discovered oxygen in the 18th century, but Lavoisier discovered that combustion was the result of oxidation. He also constructed a table of 33 elements and invented modern chemical nomenclature. Formal biological science remained in its infancy in the 18th century, when the focus lay upon the classification and categorization of natural life. This growth in natural history was led by Carolus Linnaeus, whose 1735 taxonomy of the natural world is still in use. Linnaeus in the 1750s introduced scientific names for all his species.    19th-century developments (1800–1900)Edit   By the 19th century, the study of science had come into the purview of professionals and institutions. In so doing, it gradually acquired the more modern name of natural science. The term scientist was coined by William Whewell in an 1834 review of Mary Somerville's On the Connexion of the Sciences. But the word did not enter general use until nearly the end of the same century.    Modern natural science (1900–present)Edit  According to a famous 1923 textbook Thermodynamics and the Free Energy of Chemical Substances by the American chemist Gilbert N. Lewis and the American physical chemist Merle Randall, the natural sciences contain three great branches:  Aside from the logical and mathematical sciences, there are three great branches of natural science which stand apart by reason of the variety of far reaching deductions drawn from a small number of primary postulates — they are mechanics, electrodynamics, and thermodynamics.  Today, natural sciences are more commonly divided into life sciences, such as botany and zoology; and physical sciences, which include physics, chemistry, geology, astronomy and materials science.    See alsoEdit   Empiricism Branches of science List of academic disciplines and sub-disciplines    ReferencesEdit     BibliographyEdit     Further readingEdit  Defining Natural Sciences Ledoux,S. F., 2002: Defining Natural Sciences, Behaviorology Today, 5(1), 34-36.    External linksEdit  The History of Recent Science and Technology Natural Sciences Information on the Natural Sciences degree programme at Durham University. Natural Sciences Contains updated information on research in the Natural Sciences including biology, geography and the applied life and earth sciences. Natural Sciences Information on the Natural Sciences degree programme at the University of Bath which includes the Biological Sciences, Chemistry, Pharmacology, Physics and Environmental Studies. Reviews of Books About Natural Science This site contains over 50 previously published reviews of books about natural science, plus selected essays on timely topics in natural science. Scientific Grant Awards Database Contains details of over 2,000,000 scientific research projects conducted over the past 25 years. Natural Sciences Tripos Provides information on the framework within which most of the natural science is taught at the University of Cambridge. E!Science Up-to-date science news aggregator from major sources including universities.","label":"foo"},{"text":"Research comprises \"creative work undertaken on a systematic basis in order to increase the stock of knowledge, including knowledge of humans, culture and society, and the use of this stock of knowledge to devise new applications.\" It is used to establish or confirm facts, reaffirm the results of previous work, solve new or existing problems, support theorems, or develop new theories. A research project may also be an expansion on past work in the field. To test the validity of instruments, procedures, or experiments, research may replicate elements of prior projects, or the project as a whole. The primary purposes of basic research (as opposed to applied research) are documentation, discovery, interpretation, or the research and development (R&D) of methods and systems for the advancement of human knowledge. Approaches to research depend on epistemologies, which vary considerably both within and between humanities and sciences. There are several forms of research: scientific, humanities, artistic, economic, social, business, marketing, practitioner research, etc.    Forms of researchEdit  Scientific research is a systematic way of gathering data and harnessing curiosity. This research provides scientific information and theories for the explanation of the nature and the properties of the world. It makes practical applications possible. Scientific research is funded by public authorities, by charitable organizations and by private groups, including many companies. Scientific research can be subdivided into different classifications according to their academic and application disciplines. Scientific research is a widely used criterion for judging the standing of an academic institution, such as business schools, but some argue that such is an inaccurate assessment of the institution, because the quality of research does not tell about the quality of teaching (these do not necessarily correlate totally). Research in the humanities involves different methods such as for example hermeneutics and semiotics, and a different, more relativist epistemology. Humanities scholars usually do not search for the ultimate correct answer to a question, but instead explore the issues and details that surround it. Context is always important, and context can be social, historical, political, cultural or ethnic. An example of research in the humanities is historical research, which is embodied in historical method. Historians use primary sources and other evidence to systematically investigate a topic, and then to write histories in the form of accounts of the past. Artistic research, also seen as 'practice-based research', can take form when creative works are considered both the research and the object of research itself. It is the debatable body of thought which offers an alternative to purely scientific methods in research in its search for knowledge and truth. https://upload.wikimedia.org/wikipedia/en/thumb/9/9a/Flag_of_Spain.svg/750px-Flag_of_Spain.svg.png    EtymologyEdit   The word research is derived from the Middle French \"recherche\", which means \"to go about seeking\", the term itself being derived from the Old French term \"recerchier\" a compound word from \"re-\" + \"cerchier\", or \"sercher\", meaning 'search'. The earliest recorded use of the term was in 1577.    DefinitionsEdit  Research has been defined in a number of different ways. A broad definition of research is given by Martyn Shuttleworth - \"In the broadest sense of the word, the definition of research includes any gathering of data, information and facts for the advancement of knowledge.\" Another definition of research is given by Creswell who states that - \"Research is a process of steps used to collect and analyze information to increase our understanding of a topic or issue\". It consists of three steps: Pose a question, collect data to answer the question, and present an answer to the question. The Merriam-Webster Online Dictionary defines research in more detail as \"a studious inquiry or examination; especially investigation or experimentation aimed at the discovery and interpretation of facts, revision of accepted theories or laws in the light of new facts, or practical application of such new or revised theories or laws\".    Steps in conducting researchEdit  Research is often conducted using the hourglass model structure of research. The hourglass model starts with a broad spectrum for research, focusing in on the required information through the method of the project (like the neck of the hourglass), then expands the research in the form of discussion and results. The major steps in conducting research are: Identification of research problem Literature review Specifying the purpose of research Determine specific research questions Specification of a Conceptual framework - Usually a set of hypotheses  Choice of a methodology (for data collection) Data collection Analyzing and interpreting the data Reporting and evaluating research Communicating the research findings and, possibly, recommendations The steps generally represent the overall process, however they should be viewed as an ever-changing iterative process rather than a fixed set of steps. Most researches begin with a general statement of the problem, or rather, the purpose for engaging in the study. The literature review identifies flaws or holes in previous research which provides justification for the study. Often, a literature review is conducted in a given subject area before a research question is identified. A gap in the current literature, as identified by a researcher, then engenders a research question. The research question may be parallel to the hypothesis. The hypothesis is the supposition to be tested. The researcher(s) collects data to test the hypothesis. The researcher(s) then analyzes and interprets the data via a variety of statistical methods, engaging in what is known as Empirical research. The results of the data analysis in confirming or failing to reject the Null hypothesis are then reported and evaluated. At the end, the researcher may discuss avenues for further research. However, some researchers advocate for the flip approach: starting with articulating findings and discussion of them, moving \"up\" to identification research problem that emerging in the findings and literature review introducing the findings. The flip approach is justified by the transactional nature of the research endeavor where research inquiry, research questions, research method, relevant research literature, and so on are not fully known until the findings fully emerged and interpreted. Rudolph Rummel says, \"... no researcher should accept any one or two tests as definitive. It is only when a range of tests are consistent over many kinds of data, researchers, and methods can one have confidence in the results.\" Plato in Meno talks about an inherent difficulty, if not a paradox, of doing research that can be paraphrase in the following way, \"If you know what you're searching for, why do you search for it?! [i.e., you have already found it] If you don't know what you're searching for, what are you searching for?!\"    Scientific researchEdit   Generally, research is understood to follow a certain structural process. Though step order may vary depending on the subject matter and researcher, the following steps are usually part of most formal research, both basic and applied: Observations and Formation of the topic: Consists of the subject area of ones interest and following that subject area to conduct subject related research. The subject area should not be randomly chosen since it requires reading a vast amount of literature on the topic to determine the gap in the literature the researcher intends to narrow. A keen interest in the chosen subject area is advisable. The research will have to be justified by linking its importance to already existing knowledge about the topic. Hypothesis: A testable prediction which designates the relationship between two or more variables. Conceptual definition: Description of a concept by relating it to other concepts. Operational definition: Details in regards to defining the variables and how they will be measured/assessed in the study. Gathering of data: Consists of identifying a population and selecting samples, gathering information from and/or about these samples by using specific research instruments. The instruments used for data collection must be valid and reliable. Analysis of data: Involves breaking down the individual pieces of data in order to draw conclusions about it. Data Interpretation: This can be represented through tables, figures and pictures, and then described in words. Test, revising of hypothesis Conclusion, reiteration if necessary A common misconception is that a hypothesis will be proven (see, rather, Null hypothesis). Generally a hypothesis is used to make predictions that can be tested by observing the outcome of an experiment. If the outcome is inconsistent with the hypothesis, then the hypothesis is rejected (see falsifiability). However, if the outcome is consistent with the hypothesis, the experiment is said to support the hypothesis. This careful language is used because researchers recognize that alternative hypotheses may also be consistent with the observations. In this sense, a hypothesis can never be proven, but rather only supported by surviving rounds of scientific testing and, eventually, becoming widely thought of as true. A useful hypothesis allows prediction and within the accuracy of observation of the time, the prediction will be verified. As the accuracy of observation improves with time, the hypothesis may no longer provide an accurate prediction. In this case, a new hypothesis will arise to challenge the old, and to the extent that the new hypothesis makes more accurate predictions than the old, the new will supplant it. Researchers can also use a null hypothesis, which state no relationship or difference between the independent or dependent variables. A null hypothesis uses a sample of all possible people to make a conclusion about the population.    Historical methodEdit   The historical method comprises the techniques and guidelines by which historians use historical sources and other evidence to research and then to write history. There are various history guidelines that are commonly used by historians in their work, under the headings of external criticism, internal criticism, and synthesis. This includes lower criticism and sensual criticism. Though items may vary depending on the subject matter and researcher, the following concepts are part of most formal historical research: Identification of origin date Evidence of localization Recognition of authorship Analysis of data Identification of integrity Attribution of credibility    Research methodsEdit   The goal of the research process is to produce new knowledge or deepen understanding of a topic or issue. This process takes three main forms (although, as previously discussed, the boundaries between them may be obscure): Exploratory research, which helps to identify and define a problem or question. Constructive research, which tests theories and proposes solutions to a problem or question. Empirical research, which tests the feasibility of a solution using empirical evidence. There are two major types of empirical research design: qualitative research and quantitative research. Researchers choose qualitative or quantitative methods according to the nature of the research topic they want to investigate and the research questions they aim to answer: Qualitative research Understanding of human behavior and the reasons that govern such behavior. Asking a broad question and collecting data in the form of words, images, video etc that is analyzed and searching for themes. This type of research aims to investigate a question without attempting to quantifiably measure variables or look to potential relationships between variables. It is viewed as more restrictive in testing hypotheses because it can be expensive and time consuming, and typically limited to a single set of research subjects. Qualitative research is often used as a method of exploratory research as a basis for later quantitative research hypotheses. Qualitative research is linked with the philosophical and theoretical stance of social constructionism. Quantitative research Systematic empirical investigation of quantitative properties and phenomena and their relationships. Asking a narrow question and collecting numerical data to analyze utilizing statistical methods. The quantitative research designs are experimental, correlational, and survey (or descriptive). Statistics derived from quantitative research can be used to establish the existence of associative or causal relationships between variables. Quantitative research is linked with the philosophical and theoretical stance of positivism. The quantitative data collection methods rely on random sampling and structured data collection instruments that fit diverse experiences into predetermined response categories. These methods produce results that are easy to summarize, compare, and generalize. Quantitative research is concerned with testing hypotheses derived from theory and/or being able to estimate the size of a phenomenon of interest. Depending on the research question, participants may be randomly assigned to different treatments (this is the only way that a quantitative study can be considered a true experiment). If this is not feasible, the researcher may collect data on participant and situational characteristics in order to statistically control for their influence on the dependent, or outcome, variable. If the intent is to generalize from the research participants to a larger population, the researcher will employ probability sampling to select participants. In either qualitative or quantitative research, the researcher(s) may collect primary or secondary data. Primary data is data collected specifically for the research, such as through interviews or questionnaires. Secondary data is data that already exists, such as census data, which can be re-used for the research. It is good ethical research practice to use secondary data wherever possible. Mixed-method research, i.e. research that includes qualitative and quantitative elements, using both primary and secondary data, is becoming more common. Big data has brought big impacts on research methods that now researchers do not put much effort on data collection, and also methods to analyze easily available huge amount of data have also changed.    Research method controversiesEdit  There have been many controversies about research methods stemmed from a philosophical positivism promise to distinguish the science from other practices (especially religion) by its method. This promise leads to methodological hegemony and methodology wars where diverse researchers, often coming from opposing paradigms, try to impose their own methodology on the entire field or even on the science practice in general as the only legitimate.   = Quantitative vs. Qualitative warEdit =   = Anti-methodologyEdit = According to this view, general scientific methodology does not exist and attempts to impose it on scientists is counterproductive. Each particular research with its emerging particular inquiries requires and should produce its own way (method) of researching. Similar to the art practice, the notion of methodology has to be replaced with the notion of research mastery.    = Methodological academic imperialismEdit = Epistemologies of different national sciences and cultural communities may differ and, thus, they may produce different methods of research. For example, psychological research in Russia tends to be rooted in philosophy while in the US and UK in empirism.  Rich countries (and dominant cultural communities within them) and their national sciences may dominate scientific discourse through funding and publications. This academic hegemony can translate into impositions of certain research methodologies through the gatekeeping process of international academic publications, conference presentation selection, institutional review boards, and funding.    Professionalisation Edit   In several national and private academic systems, the professionalization of research has resulted in formal job titles.    In RussiaEdit  In Russia, former Soviet Union and some Post-Soviet states the term researcher (Russian: Научный сотрудник, nauchny sotrudnik) is both a generic term for a person who carried out scientific research, as well as a job position within the frameworks of the USSR Academy of Sciences, Soviet universities, and in other research-oriented establishments. The term is also sometimes translated as research fellow, research associate, etc. The following ranks are known: Junior Researcher (Junior Research Associate) Researcher (Research Associate) Senior Researcher (Senior Research Associate) Leading Researcher (Leading Research Associate) Chief Researcher (Chief Research Associate)    PublishingEdit   Academic publishing describes a system that is necessary in order for academic scholars to peer review the work and make it available for a wider audience. The system varies widely by field, and is also always changing, if often slowly. Most academic work is published in journal article or book form. There is also a large body of research that exists in either a thesis or dissertation form. These forms of research can be found in databases explicitly for theses and dissertations. In publishing, STM publishing is an abbreviation for academic publications in science, technology, and medicine. Most established academic fields have their own scientific journals and other outlets for publication, though many academic journals are somewhat interdisciplinary, and publish work from several distinct fields or subfields. The kinds of publications that are accepted as contributions of knowledge or research vary greatly between fields; from the print to the electronic format. A study suggests that researchers should not give great consideration to findings that are not replicated frequently. It has also been suggested that all published studies should be subjected to some measure for assessing the validity or reliability of its factors in order to prevent the publication of unproven findings. Business models are different in the electronic environment. Since about the early 1990s, licensing of electronic resources, particularly journals, has been very common. Presently, a major trend, particularly with respect to scholarly journals, is open access. There are two main forms of open access: open access publishing, in which the articles or the whole journal is freely available from the time of publication, and self-archiving, where the author makes a copy of their own work freely available on the web.    Research fundingEdit   Most funding for scientific research comes from three major sources: corporate research and development departments; private foundations, for example, the Bill and Melinda Gates Foundation; and government research councils such as the National Institutes of Health in the USA and the Medical Research Council in the UK. These are managed primarily through universities and in some cases through military contractors. Many senior researchers (such as group leaders) spend a significant amount of their time applying for grants for research funds. These grants are necessary not only for researchers to carry out their research, but also as a source of merit. The Social Psychology Network provides a comprehensive list of U.S. Government and private foundation funding sources.    Original researchEdit   Original research is research that is not exclusively based on a summary, review or synthesis of earlier publications on the subject of research. This material is of a primary source character. The purpose of the original research is to produce new knowledge, rather than to present the existing knowledge in a new form (e.g., summarized or classified).    Different formsEdit  Original research can take a number of forms, depending on the discipline it pertains to. In experimental work, it typically involves direct or indirect observation of the researched subject(s), e.g., in the laboratory or in the field, documents the methodology, results, and conclusions of an experiment or set of experiments, or offers a novel interpretation of previous results. In analytical work, there are typically some new (for example) mathematical results produced, or a new way of approaching an existing problem. In some subjects which do not typically carry out experimentation or analysis of this kind, the originality is in the particular way existing understanding is changed or re-interpreted based on the outcome of the work of the researcher. The degree of originality of the research is among major criteria for articles to be published in academic journals and usually established by means of peer review. Graduate students are commonly required to perform original research as part of a dissertation.    Artistic researchEdit  The controversial trend of artistic teaching becoming more academics-oriented is leading to artistic research being accepted as the primary mode of enquiry in art as in the case of other disciplines. One of the characteristics of artistic research is that it must accept subjectivity as opposed to the classical scientific methods. As such, it is similar to the social sciences in using qualitative research and intersubjectivity as tools to apply measurement and critical analysis. Artistic research has been defined by the University of Dance and Circus (Dans och Cirkushögskolan, DOCH), Stockholm in the following manner - \"Artistic research is to investigate and test with the purpose of gaining knowledge within and for our artistic disciplines. It is based on artistic practices, methods and criticality. Through presented documentation, the insights gained shall be placed in a context.\" Artistic research aims to enhance knowledge and understanding with presentation of the arts. For a survey of the central problematics of today's Artistic Research, see Giaco Schiesser. According to artist Hakan Topal in artistic research, \"perhaps more so than other disciplines, intuition is utilized as a method to identify a wide range of new and unexpected productive modalities\". Most writers, whether of fiction or non-fiction books, also have to do research to support their creative work. This may be factual, historical, or background research. Background research could include, for example, geographical or procedural research. The Society for Artistic Research (SAR) publishes the triannual Journal for Artistic Research (JAR), an international, online, open access and peer-reviewed journal for the identification, publication and dissemination of artistic research and its methodologies, from all arts disciplines and it runs the Research Catalogue (RC), a searchable, documentary database of artistic research, to which anyone can contribute.    See alsoEdit  European Charter for Researchers Undergraduate research Internet research List of countries by research and development spending Open research Operations research Participatory action research Primary research Psychological research methods Research-intensive cluster Scholarly research Secondary research Society for Artistic Research Timeline of the history of scientific method    ReferencesEdit     Further readingEdit  Gorard, S. (2013) Research Design: Robust approaches for the social sciences, London:SAGE, ISBN 978-1446249024, 218 pages Video of a lecture on research design Freshwater, D., Sherwood, G. & Drury, V. (2006) International research collaboration. Issues, benefits and challenges of the global network. Journal of Research in Nursing, 11 (4), pp 9295–303. Cohen, N. & Arieli, T. (2011) Field research in conflict environments: Methodological challenges and snowball sampling. Journal of Peace Research 48 (4), pp. 423–436. Soeters, Joseph; Shields, Patricia and Rietjens, Sebastiaan. 2014. Handbook of Research Methods in Military Studies New York: Routledge.    External linksEdit   The dictionary definition of research at Wiktionary  Quotations related to Research at Wikiquote","label":"foo"},{"text":"","label":"foo"},{"text":"Interdisciplinarity involves the combining of two or more academic disciplines into one activity (e.g., a research project). It is about creating something new by crossing boundaries, and thinking across them. It is related to an interdiscipline or an interdisciplinary field, which is an organizational unit that crosses traditional boundaries between academic disciplines or schools of thought, as new needs and professions emerge. The term interdisciplinary is applied within education and training pedagogies to describe studies that use methods and insights of several established disciplines or traditional fields of study. Interdisciplinarity involves researchers, students, and teachers in the goals of connecting and integrating several academic schools of thought, professions, or technologies—along with their specific perspectives—in the pursuit of a common task. The epidemiology of AIDS or global warming require understanding of diverse disciplines to solve complex problems. Interdisciplinary may be applied where the subject is felt to have been neglected or even misrepresented in the traditional disciplinary structure of research institutions, for example, women's studies or ethnic area studies. Interdisciplinarity can likewise be applied to complex subjects that can only be understood by combining the perspectives of two or more fields. The adjective interdisciplinary is most often used in educational circles when researchers from two or more disciplines pool their approaches and modify them so that they are better suited to the problem at hand, including the case of the team-taught course where students are required to understand a given subject in terms of multiple traditional disciplines. For example, the subject of land use may appear differently when examined by different disciplines, for instance, biology, chemistry, economics, geography, and politics.    Development  Although interdisciplinary and interdisciplinarity are frequently viewed as twentieth century terms, the concept has historical antecedents, most notably Greek philosophy. Julie Thompson Klein attests that \"the roots of the concepts lie in a number of ideas that resonate through modern discourse—the ideas of a unified science, general knowledge, synthesis and the integration of knowledge,\" while Giles Gunn says that Greek historians and dramatists took elements from other realms of knowledge (such as medicine or philosophy) to further understand their own material. Actually any broadminded humanist project involves interdisciplinarity, and history shows a crowd of cases, as seventeenth-century Leibniz's task to create a system of universal justice, which required linguistics, economics, management, ethics, law philosophy, politics... even sinology. Interdisciplinary programs sometimes arise from a shared conviction that the traditional disciplines are unable or unwilling to address an important problem. For example, social science disciplines such as anthropology and sociology paid little attention to the social analysis of technology throughout most of the twentieth century. As a result, many social scientists with interests in technology have joined science and technology studies programs, which are typically staffed by scholars drawn from numerous disciplines. They may also arise from new research developments, such as nanotechnology, which cannot be addressed without combining the approaches of two or more disciplines. Examples include quantum information processing, an amalgamation of quantum physics and computer science, and bioinformatics, combining molecular biology with computer science. Sustainable Development as a research area deals with problems requiring analysis and synthesis across economic, social and environmental spheres; often an integration of multiple social and natural science disciplines. Interdisciplinary research is also key to the study of health sciences, for example in studying optimal solutions to diseases. Some institutions of higher education offer accredited degree programs in Interdisciplinary Studies. At another level interdisciplinarity is seen as a remedy to the harmful effects of excessive specialization. On some views, however, interdisciplinarity is entirely indebted to those who specialize in one field of study—that is, without specialists, interdisciplinarians would have no information and no leading experts to consult. Others place the focus of interdisciplinarity on the need to transcend disciplines, viewing excessive specialization as problematic both epistemologically and politically. When interdisciplinary collaboration or research results in new solutions to problems, much information is given back to the various disciplines involved. Therefore, both disciplinarians and interdisciplinarians may be seen in complementary relation to one another.    Barriers  Because most participants in interdisciplinary ventures were trained in traditional disciplines, they must learn to appreciate differing of perspectives and methods. For example, a discipline that places more emphasis on quantitative \"rigor\" may produce practitioners who think of themselves (and their discipline) as \"more scientific\" than others; in turn, colleagues in \"softer\" disciplines may associate quantitative approaches with an inability to grasp the broader dimensions of a problem. An interdisciplinary program may not succeed if its members remain stuck in their disciplines (and in disciplinary attitudes). On the other hand, and from the disciplinary perspective, much interdisciplinary work may be seen as \"soft,\" lacking in rigor, or ideologically motivated; these beliefs place barriers in the career paths of those who choose interdisciplinary work. For example, interdisciplinary grant applications are often refereed by peer reviewers drawn from established disciplines; not surprisingly, interdisciplinary researchers may experience difficulty getting funding for their research. In addition, untenured researchers know that, when they seek promotion and tenure, it is likely that some of the evaluators will lack commitment to interdisciplinarity. They may fear that making a commitment to interdisciplinary research will increase the risk of being denied tenure. Interdisciplinary programs may fail if they are not given sufficient autonomy. For example, interdisciplinary faculty are usually recruited to a joint appointment, with responsibilities in both an interdisciplinary program (such as women's studies) and a traditional discipline (such as history). If the traditional discipline makes the tenure decisions, new interdisciplinary faculty will be hesitant to commit themselves fully to interdisciplinary work. Other barriers include the generally disciplinary orientation of most scholarly journals, leading to the perception, if not the fact, that interdisciplinary research is hard to publish. In addition, since traditional budgetary practices at most universities channel resources through the disciplines, it becomes difficult to account for a given scholar or teacher's salary and time. During periods of budgetary contraction, the natural tendency to serve the primary constituency (i.e., students majoring in the traditional discipline) makes resources scarce for teaching and research comparatively far from the center of the discipline as traditionally understood. For these same reasons, the introduction of new interdisciplinary programs is often resisted because it is perceived as a competition for diminishing funds. Due to these and other barriers, interdisciplinary research areas are strongly motivated to become disciplines themselves. If they succeed, they can establish their own research funding programs and make their own tenure and promotion decisions. In so doing, they lower the risk of entry. Examples of former interdisciplinary research areas that have become disciplines include neuroscience, cybernetics, biochemistry and biomedical engineering. These new fields are occasionally referred to as \"interdisciplines.\" On the other hand, even though interdisciplinary activities are now a focus of attention for institutions promoting learning and teaching, as well as organizational and social entities concerned with education, they are practically facing complex barriers, serious challenges and criticism. The most important obstacles and challenges faced by interdisciplinary activities in the past two decades can be divided into \"professional\", \"organizational,\" and \"cultural\" obstacles.    Interdisciplinary Studies – and Studies of Interdisciplinarity  An initial distinction should be made between interdisciplinary studies, which can be found spread across the academy today, and the study of interdisciplinarity, which involves a much smaller group of researchers. The former is instantiated in thousands of research centers across the US and the world. The latter has one US organization, the Association for Interdisciplinary Studies (founded in 1979), two international organizations, the International Network of Inter- and Transdisciplinarity (founded in 2010) and the Philosophy of/as Interdisciplinarity Network (founded in 2009), and one research institute devoted to the theory and practice of interdisciplinarity, the Center for the Study of Interdisciplinarity at the University of North Texas (founded in 2008). An Interdisciplinary study is an academic program or process seeking to synthesize broad perspectives, knowledge, skills, interconnections, and epistemology in an educational setting. Interdisciplinary programs may be founded in order to facilitate the study of subjects which have some coherence, but which cannot be adequately understood from a single disciplinary perspective (for example, women's studies or medieval studies). More rarely, and at a more advanced level, interdisciplinarity may itself become the focus of study, in a critique of institutionalized disciplines' ways of segmenting knowledge. In contrast, Studies of Interdisciplinarity raise to self-consciousness questions about how interdisciplinarity works, the nature and history of disciplinarity, and the future of knowledge in post-industrial society. Researchers at the Center for the Study of Interdisciplinarity have made the distinction between philosophy 'of' and 'as' interdisciplinarity, the former identifying a new, discrete area within philosophy that raises epistemological and metaphysical questions about the status of interdisciplinary thinking, with the latter pointing toward a philosophical practice that is sometimes called 'field philosophy'. Perhaps the most common complaint regarding interdisciplinary programs, by supporters and detractors alike, is the lack of synthesis—that is, students are provided with multiple disciplinary perspectives, but are not given effective guidance in resolving the conflicts and achieving a coherent view of the subject. Others have argued that the very idea of synthesis or integration of disciplines presupposes questionable politico-epistemic commitments. Critics of interdisciplinary programs feel that the ambition is simply unrealistic, given the knowledge and intellectual maturity of all but the exceptional undergraduate; some defenders concede the difficulty, but insist that cultivating interdisciplinarity as a habit of mind, even at that level, is both possible and essential to the education of informed and engaged citizens and leaders capable of analyzing, evaluating, and synthesizing information from multiple sources in order to render reasoned decisions. While much has been written on the philosophy and promise of interdisciplinarity in academic programs and professional practice, social scientists are increasingly interrogating academic discourses on interdisciplinarity, as well as how interdisciplinarity actually works – and does not – in practice. Some have shown, for example, that some interdisciplinary enterprises that aim to serve society can produce deleterious outcomes for which no one can be held to account. The Politics of Interdisciplinary Studies Since 1998 there has been an ascendancy in the value of the concept and practice of interdisciplinary research and teaching and a growth in the number of bachelor's degrees awarded at U.S. universities classified as multi- or interdisciplinary studies. The number of interdisciplinary bachelor's degrees awarded annually rose from 7,000 in 1973 to 30,000 a year by 2005 according to data from the National Center of Educational Statistics (NECS). In addition, educational leaders from the Boyer Commission to Carnegie's President Vartan Gregorian to Alan I. Leshner, CEO of the American Association for the Advancement of Science have advocated for interdisciplinary rather than disciplinary approaches to problem solving in the 21st century. This has been echoed by federal funding agencies, particularly the NIH under the Direction of Elias Zerhouni, who have advocated that grant proposals be framed more as interdisciplinary collaborative projects than single researcher, single discipline ones. At the same time, many thriving longstanding bachelors in interdisciplinary studies programs in existence for 30 or more years, have been closed down, in spite of healthy enrollment. Examples include Arizona International (formerly part of the University of Arizona), the School of Interdisciplinary Studies at Miami University, and the Department of Interdisciplinary Studies at Wayne State University; others such as the Department of Interdisciplinary Studies at Appalachian State University, and George Mason University's New Century College, have been cut back. Stuart Henry has seen this trend as part of the hegemony of the disciplines in their attempt to recolonize the experimental knowledge production of otherwise marginalized fields of inquiry. This is due to threat perceptions seemingly based on the ascendancy of interdisciplinary studies against traditional academia.    Historical examples  There are many examples of when a particular idea, almost on the same period, arises in different disciplines. One case is the shift from the approach of focusing on \"specialized segments of attention\" (adopting one particular perspective), to the idea of \"instant sensory awareness of the whole\", an attention to the \"total field\", a \"sense of the whole pattern, of form and function as a unity\", an \"integral idea of structure and configuration\". This has happened in painting (with cubism), physics, poetry, communication and educational theory. According to Marshall McLuhan, this paradigm shift was due to the passage from an era shaped by mechanization, which brought sequentiality, to the era shaped by the instant speed of electricity, which brought simultaneity.    Efforts To Simplify and Defend the Concept of Interdisciplinary  An article posted online by University College, London, attempts to provide a simple, common-sense, definition of interdisciplinarity, bypassing the difficulties of defining that concept and obviating the need for such related concepts as transdisciplinarity, pluridisciplinarity, and multidisciplinarity:  \"To begin with, a discipline can be conveniently defined as any comparatively self-contained and isolated domain of human experience which possesses its own community of experts. Interdisciplinarity is best seen as bringing together distinctive components of two or more disciplines. In academic discourse, interdisciplinarity typically applies to four realms: knowledge, research, education, and theory. Interdisciplinary knowledge involves familiarity with components of two or more disciplines. Interdisciplinary research combines components of two or more disciplines in the search or creation of new knowledge, operations, or artistic expressions. Interdisciplinary education merges components of two or more disciplines in a single program of instruction. Interdisciplinary theory takes interdisciplinary knowledge, research, or education as its main objects of study.\"  In turn, interdisciplinary knowledge and research are important because: \"Creativity often requires interdisciplinary knowledge. Immigrants often make important contributions to their new field. Disciplinarians often commit errors which can be best detected by people familiar with two or more disciplines. Some worthwhile topics of research fall in the interstices among the traditional disciplines. Many intellectual, social, and practical problems require interdisciplinary approaches. Interdisciplinary knowledge and research serve to remind us of the unity-of-knowledge ideal. Interdisciplinarians enjoy greater flexibility in their research. More so than narrow disciplinarians, interdisciplinarians often treat themselves to the intellectual equivalent of traveling in new lands. Interdisciplinarians may help breach communication gaps in the modern academy, thereby helping to mobilize its enormous intellectual resources in the cause of greater social rationality and justice. By bridging fragmented disciplines, interdisciplinarians might play a role in the defense of academic freedom.\"    Quotations   \"The modem mind divides, specializes, thinks in categories: the Greek instinct was the opposite, to take the widest view, to see things as an organic whole .... It was arete that the [Olympic] games were designed to test the arete of the whole man, not a merely specialized skill .... The great event was the pentathalon, if you won this, you were a man. Needless to say, the Marathon race was never heard of until modem times: the Greeks would have regarded it as a monstrosity.\"  \"Previously, men could be divided simply into the learned and the ignorant, those Previously, men could be divided simply into the learned and the ignorant, those more or less the one, and those more or less the other. But your specialist cannot be brought in under either of these two categories. He is not learned, for he is formally ignorant of all that does not enter into his specialty; but neither is he ignorant, because he is 'a scientist,' and 'knows' very well his own tiny portion of the universe. We shall have to say that he is a learned ignoramus, which is a very serious matter, as it implies that he is a person who is ignorant, not in the fashion of the ignorant man, but with all the petulance of one who is learned in his own special line.\"    See also  Commensurability (philosophy of science) Crossdisciplinarity Holism in science Integrative learning Interdiscipline Interprofessional education Multidisciplinarity Social ecological model Systems thinking Systems theory Periodic Table of Human Sciences in Tinbergen's four questions Transdisciplinarity Science of Team Science    References     Further reading  Alderman, Harold; Chiappori, Pierre Andre; Haddad, Lawrence; Hoddinott, John. \"Unitary Versus Collective Models of the Household: Time to Shift the Burden of Proof?\". World Bank Research Observer 10 (1): 1–19. doi:10.1093/wbro/10.1.1.  Augsburg, Tanya. (2005), Becoming Interdisciplinary: An Introduction to Interdisciplinary Studies (Kendall/Hunt) Association for Integrative Studies Bagchi, Amiya Kumar (1982) ‘The Political Economy of Underdevelopment’, New York, Cambridge University Press Bernstein, Henry (1973) ‘Introduction: Development and The Social Sciences’, in Henry Bernstein (ed.) Underdevelopment and Development: The Third World Today, Harmondsworth: Penguin, pp. 13–30 Center for the Study of Interdisciplinarity Centre for Interdisciplinary Research in the Arts (University of Manchester) Chambers, Robert (2001), \"Qualitative approaches: self-criticism and what can be gained from quantitative approaches\", in Kanbur, Ravi, Qual–quant: qualitative and quantitative poverty appraisal - complementaries, tensions, and the way forward (pdf), Ithaca, New York: Cornell University, pp. 22–25.  Chubin, D. E. (1976). \"The conceptualization of scientific specialties\". The Sociological Quarterly 17: 448–476. doi:10.1111/j.1533-8525.1976.tb01715.x.  College for Interdisciplinary Studies, University of British Columbia, Vancouver, British Columbia, Canada Davies. M. and Devlin, M. (2007). Interdisciplinary Higher Education: Implications for Teaching and Learning. Centre for the Study of Higher Education, The University of Melbourne. http://www.cshe.unimelb.edu.au/pdfs/InterdisciplinaryHEd.pdf Frodeman, R. and Mitcham, C. \"New Directions in Interdisciplinarity: Broad, Deep, and Critical,\" Bulletin of Science, Technology, and Society, Vol. 27 (Fall 2007) no. 6; pp. 506–514. Franks, D.; Dale, P.; Hindmarsh, R.; Fellows, C.; Buckridge, M.; Cybinski, P. (2007). \"Interdisciplinary foundations: reflecting on interdisciplinarity and three decades of teaching and research at Griffith University, Australia\". Studies in Higher Education 32 (2): 167–185. doi:10.1080/03075070701267228.  Frodeman, R., Klein, J.T., and Mitcham, C. Oxford Handbook of Interdisciplinarity. Oxford University Press, 2010. The Evergreen State College, Olympia, Washington Gram Vikas (2007) Annual Report, p. 19. Granovetter, Mark (1985). \"Economic Action and Social Structure: The Problem of Embeddedness\". The American Journal of Sociology 91 (3): 481–510. doi:10.1086/228311.  Hang Seng Centre for Cognitive Studies Harriss, John (2002). \"The Case for Cross-Disciplinary Approaches in International Development\". World Development 30 (3): 487–496. doi:10.1016/s0305-750x(01)00115-2.  Henry, Stuart (2005). \"Disciplinary hegemony meets interdisciplinary ascendancy: Can interdisciplinary/integrative studies survive, and if so how?\". Issues in Integrative Studies 23: 1–37.  Indiresan, P.V. (1990) Managing Development: Decentralisation, Geographical Socialism And Urban Replication. India: Sage Interdisciplinary Arts Department, Columbia College Chicago Interdisciplinarity and tenure Interdisciplinary Studies Project, Harvard University School of Education, Project Zero Jackson, Cecile (2002). \"Disciplining Gender?\". World Development 30 (3): 497–509. doi:10.1016/s0305-750x(01)00113-9.  Jacobs, J.A. & Frickel, S. (2009). \"Interdisciplinarity: A Critical Assessment.\" Annual Review of Sociology, 35, 43–65. Retrieved from: http://proseminarcrossnationalstudies.files.wordpress.com/2009/11/interdisciplinarity_ars_2009.pdf Johnston, R (2003). \"Integrating methodologists into teams of substantive experts\" (PDF). Studies in Intelligence 47 (1).  Kanbur, Ravi (March 2002). \"Economics, social science and development\". World Development (Elsevier) 30 (3): 477–486. doi:10.1016/S0305-750X(01)00117-6.  Kanbur, Ravi (2003), \"Q-squared?: a commentry on qualitative and quantitative poverty appraisal\", in Kanbur, Ravi, Q-squared, combining qualitative and quantitative methods in poverty appraisal, Delhi Bangalore: Permanent Black Distributed by Orient Longman, pp. 2–27, ISBN 9788178240534.  Klein, Julie Thompson (1996) Crossing Boundaries: Knowledge, Disciplinarities, and Interdisciplinarities (University Press of Virginia) Klein, Julie Thompson (2006) \"Resources for interdisciplinary studies.\" Change, (Mark/April). 52–58 Kleinberg, Ethan (2008). \"Interdisciplinary studies at the crossroads\". Liberal Education 94 (1): 6–11.  Lipton, Michael (1970). \"Interdisciplinary Studies in Less Developed Countries\". Journal of Development Studies 7 (1): 5–18. doi:10.1080/00220387008421343.  Gerhard Medicus Interdisciplinarity in Human Sciences (Documents No. 5, 6 and 7 in English) Moran, Joe. (2002). Interdisciplinarity. NYU Gallatin School of Individualized Study, New York, NY Poverty Action Lab, http://www.povertyactionlab.org/research/rand.php (accessed on 4 November 2008) Ravallion, Martin (2003), \"Can qualitative methods help quantitative poverty\", in Kanbur, Ravi, Q-squared, combining qualitative and quantitative methods in poverty appraisal, Delhi Bangalore: Permanent Black Distributed by Orient Longman, pp. 58–67, ISBN 9788178240534  Rhoten, D. (2003). A multi-method analysis of the social and technical conditions for interdisciplinary collaboration. School of Social Ecology at the University of California, Irvine Schuurman, F.J. (2000). \"Paradigms Lost, paradigms regained? Development studies in the twenty-first century\". Third World Quarterly 21 (1): 7–20. doi:10.1080/01436590013198.  Sen, Amartya (1999). Development as freedom. New York: Oxford University Press. ISBN 9780198297581.  Siskin, L.S. & Little, J.W. (1995). The Subjects in Question. Teachers College Press. about the departmental organization of high schools and efforts to change that. Stiglitz, Joseph (2002) Globalisation and its Discontents, United States of America, W.W. Norton and Company Sumner, A and M. Tribe (2008) International Development Studies: Theories and Methods in Research and Practice, London: Sage Thorbecke, Eric. (2006) \"The Evolution of the Development Doctrine, 1950–2005\". UNU-WIDER Research Paper No. 2006/155. United Nations University, World Institute for Development Economics Research Trans- & inter-disciplinary science approaches- A guide to on-line resources on integration and trans- and inter-disciplinary approaches. Truman State University's Interdisciplinary Studies Program Waldman, Amy (2003), \"Distrust Opens the Door for Polio in India\", http://query.nytimes.com/gst/fullpage.html?res=9C03E5D81430F93AA25752C0A9659C8B63&sec=health&spon=&pagewanted=all (accessed on 4 November 2008) Peter Weingart and Nico Stehr, eds. 2000. Practicing Interdisciplinarity(University of Toronto Press) Peter Weingart and Britta Padberg, eds. 2014. \"University Experiments in Interdisciplinarity - Obstacles and Opportunities\", Bielefeld: transcript Verlag White, Howard (2002). \"Combining Quantitative and Qualitative Approaches in Poverty Analysis\". World Development 30 (3): 511–522. doi:10.1016/s0305-750x(01)00114-0.     External links  National Science Foundation Workshop Report: Interdisciplinary Collaboration in Innovative Science and Engineering Fields Rethinking Interdisciplnarity online conference, organized by the Institut Nicod, CNRS, Paris [broken] Center for the Study of Interdisciplinarity at the University of North Texas Labyrinthe. Atelier interdisciplinaire, a journal (in French), with a special issue on La Fin des Disciplines? Rupkatha Journal on Interdisciplinary Studies in Humanities: An Online Open Access E-Journal, publishing articles on a number of areas Article about interdisciplinary modeling (in French with an English abstract) Wolf, Dieter. Unity of Knowledge, an interdisciplinary project Soka University of America has no disciplinary departments and emphasizes interdisciplinary concentrations in the Humanities, Social and Behavioral Sciences, International Studies, and Environmental Studies. SystemsX.ch - The Swiss Initiative in Systems Biology Interdisciplinarity at Wikispaces - creative explorations of the term interdisciplinarity and its interactions with gender studies","label":"foo"},{"text":"Biophysics is an interdisciplinary science using methods of, and theories from, physics to study biological systems. Biophysics spans all scales of biological organization, from the molecular scale to whole organisms and ecosystems. Biophysical research shares significant overlap with biochemistry, nanotechnology, bioengineering, computational biology and (complex) systems biology. It has been suggested as a bridge between biology and physics. The term \"biophysics\" was originally introduced by Karl Pearson in 1892.    OverviewEdit  Molecular biophysics typically addresses biological questions similar to those in biochemistry and molecular biology, but more quantitatively. Scientists in this field conduct research concerned with understanding the interactions between the various systems of a cell, including the interactions between DNA, RNA and protein biosynthesis, as well as how these interactions are regulated. A great variety of techniques are used to answer these questions. Fluorescent imaging techniques, as well as electron microscopy, x-ray crystallography, NMR spectroscopy, atomic force microscopy (AFM) and small-angle scattering (SAS) both with X-rays and neutrons (SAXS/SANS) are often used to visualize structures of biological significance. Protein dynamics can be observed by neutron spin echo spectroscopy. Conformational change in structure can be measured using techniques such as dual polarisation interferometry, circular dichroism, SAXS and SANS. Direct manipulation of molecules using optical tweezers or AFM, can also be used to monitor biological events where forces and distances are at the nanoscale. Molecular biophysicists often consider complex biological events as systems of interacting entities which can be understood e.g. through statistical mechanics, thermodynamics and chemical kinetics. By drawing knowledge and experimental techniques from a wide variety of disciplines, biophysicists are often able to directly observe, model or even manipulate the structures and interactions of individual molecules or complexes of molecules. In addition to traditional (i.e. molecular and cellular) biophysical topics like structural biology or enzyme kinetics, modern biophysics encompasses an extraordinarily broad range of research, from bioelectronics to quantum biology involving both experimental and theoretical tools. It is becoming increasingly common for biophysicists to apply the models and experimental techniques derived from physics, as well as mathematics and statistics (see biomathematics), to larger systems such as tissues, organs, populations and ecosystems. Biophysical models are used extensively in the study of electrical conduction in single neurons, as well as neural circuit analysis in both tissue and whole brain.    HistoryEdit  Some of the earlier studies in biophysics were conducted in the 1840s by a group known as the Berlin school of physiologists. Among its members were pioneers such as Hermann von Helmholtz, Ernst Heinrich Weber, Carl F. W. Ludwig, and Johannes Peter Müller. Biophysics might even be seen as dating back to the studies of Luigi Galvani. The popularity of the field rose when the book “What is life?” by Erwin Schrödinger was published. Since 1957 biophysicists have organized themselves into the Biophysical Society which now has about 9,000 members over the world.    Focus as a subfieldEdit  Generally, biophysics does not have university-level departments of its own, but has presence as groups across departments within the fields of molecular biology, biochemistry, chemistry, computer science, mathematics, medicine, pharmacology, physiology, physics, and neuroscience. What follows is a list of examples of how each department applies its efforts toward the study of biophysics. This list is hardly all inclusive. Nor does each subject of study belong exclusively to any particular department. Each academic institution makes its own rules and there is much overlap between departments. Biology and molecular biology - Almost all forms of biophysics efforts are included in some biology department somewhere. To include some: gene regulation, single protein dynamics, bioenergetics, patch clamping, biomechanics. Structural biology - Ångstrom-resolution structures of proteins, nucleic acids, lipids, carbohydrates, and complexes thereof. Biochemistry and chemistry - biomolecular structure, siRNA, nucleic acid structure, structure-activity relationships. Computer science - Neural networks, biomolecular and drug databases. Computational chemistry - molecular dynamics simulation, molecular docking, quantum chemistry Bioinformatics - sequence alignment, structural alignment, protein structure prediction Mathematics - graph/network theory, population modeling, dynamical systems, phylogenetics. Medicine and neuroscience - tackling neural networks experimentally (brain slicing) as well as theoretically (computer models), membrane permitivity, gene therapy, understanding tumors. Pharmacology and physiology - channelomics, biomolecular interactions, cellular membranes, polyketides. Physics - negentropy, stochastic processes, covering dynamics. Quantum biology - The field of quantum biology applies quantum mechanics to biological objects and problems decohered isomers to yield time-dependent base substitutions. These studies imply applications in quantum computing. Agronomy and agriculture Many biophysical techniques are unique to this field. Research efforts in biophysics are often initiated by scientists who were traditional physicists, chemists, and biologists by training.    See alsoEdit     ReferencesEdit     NotesEdit     External linksEdit  Biophysical Society Journal of Physiology: 2012 virtual issue Biophysics and Beyond bio-physics-wiki Link archive of learning resources for students: biophysika.de (60% English, 40% German) Journal of Medicine, Physiology and Biophysics,(IISTE), USA. Chief Editor of the journal is Ignat Ignatov. Chief editor of all IISTE journals is Alexander Decker.","label":"foo"},{"text":"Quantum chemistry is a branch of chemistry whose primary focus is the application of quantum mechanics in physical models and experiments of chemical systems. It is also called molecular quantum mechanics.    OverviewEdit  It involves heavy interplay of experimental and theoretical methods: Experimental quantum chemists rely heavily on spectroscopy, through which information regarding the quantization of energy on a molecular scale can be obtained. Common methods are infra-red (IR) spectroscopy and nuclear magnetic resonance (NMR) spectroscopy. Theoretical quantum chemistry, the workings of which also tend to fall under the category of computational chemistry, seeks to calculate the predictions of quantum theory as atoms and molecules can only have discrete energies; as this task, when applied to polyatomic species, invokes the many-body problem, these calculations are performed using computers rather than by analytical \"back of the envelope\" methods, pen recorder or computerized data station with a VDU. In these ways, quantum chemists investigate chemical phenomena. In reactions, quantum chemistry studies the ground state of individual atoms and molecules, the excited states, and the transition states that occur during chemical reactions. On the calculations: quantum chemical studies use also semi-empirical and other methods based on quantum mechanical principles, and deal with time dependent problems. Many quantum chemical studies assume the nuclei are at rest (Born–Oppenheimer approximation). Many calculations involve iterative methods that include self-consistent field methods. Major goals of quantum chemistry include increasing the accuracy of the results for small molecular systems, and increasing the size of large molecules that can be processed, which is limited by scaling considerations—the computation time increases as a power of the number of atoms.    HistoryEdit  Some view the birth of quantum chemistry in the discovery of the Schrödinger equation and its application to the hydrogen atom in 1926. However, the 1927 article of Walter Heitler and Fritz London is often recognised as the first milestone in the history of quantum chemistry. This is the first application of quantum mechanics to the diatomic hydrogen molecule, and thus to the phenomenon of the chemical bond. In the following years much progress was accomplished by Edward Teller, Robert S. Mulliken, Max Born, J. Robert Oppenheimer, Linus Pauling, Erich Hückel, Douglas Hartree, Vladimir Aleksandrovich Fock, to cite a few. The history of quantum chemistry also goes through the 1838 discovery of cathode rays by Michael Faraday, the 1859 statement of the black body radiation problem by Gustav Kirchhoff, the 1877 suggestion by Ludwig Boltzmann that the energy states of a physical system could be discrete, and the 1900 quantum hypothesis by Max Planck that any energy radiating atomic system can theoretically be divided into a number of discrete energy elements ε such that each of these energy elements is proportional to the frequency ν with which they each individually radiate energy and a numerical value called Planck’s Constant. Then, in 1905, to explain the photoelectric effect (1839), i.e., that shining light on certain materials can function to eject electrons from the material, Albert Einstein postulated, based on Planck’s quantum hypothesis, that light itself consists of individual quantum particles, which later came to be called photons (1926). In the years to follow, this theoretical basis slowly began to be applied to chemical structure, reactivity, and bonding. Probably the greatest contribution to the field was made by Linus Pauling.    Electronic structureEdit   The first step in solving a quantum chemical problem is usually solving the Schrödinger equation (or Dirac equation in relativistic quantum chemistry) with the electronic molecular Hamiltonian. This is called determining the electronic structure of the molecule. It can be said that the electronic structure of a molecule or crystal implies essentially its chemical properties. An exact solution for the Schrödinger equation can only be obtained for the hydrogen atom (though exact solutions for the bound state energies of the hydrogen molecular ion have been identified in terms of the generalized Lambert W function). Since all other atomic, or molecular systems, involve the motions of three or more \"particles\", their Schrödinger equations cannot be solved exactly and so approximate solutions must be sought.    Wave modelEdit  The foundation of quantum mechanics and quantum chemistry is the wave model, in which the atom is a small, dense, positively charged nucleus surrounded by electrons. The wave model is derived from the wavefunction, a set of possible equations derived from the time evolution of the Schrödinger equation which is applied to the wavelike probability distribution of subatomic particles. Unlike the earlier Bohr model of the atom, however, the wave model describes electrons as \"clouds\" moving in orbitals, and their positions are represented by probability distributions rather than discrete points. The strength of this model lies in its predictive power. Specifically, it predicts the pattern of chemically similar elements found in the periodic table. The wave model is so named because electrons exhibit properties (such as interference) traditionally associated with waves. See wave-particle duality. In this model, when we solve the Schrödinger Equation for an Hidrogenoid Atom, we obtain a solution that depends on some numbers, called quantum numbers, that describes the orbital, the most probable space where an electron can be. These are n, the principal quantum number, for the energy, l, or secondary quantum number, wich correlates to the angular momentum, ml, for the orientation, and ms the spin. This model can explain the new lines that appeared in the spectroscopy of atoms. For multielectron atoms we must introduce some rules as that the electrons fill orbitals in a way to minimize the energy of the atom, in order of increasing energy, the Pauli Exclusion Principle, the Hund's Rule, and the Aufbau Principle.    Valence bondEdit   Although the mathematical basis of quantum chemistry had been laid by Schrödinger in 1926, it is generally accepted that the first true calculation in quantum chemistry was that of the German physicists Walter Heitler and Fritz London on the hydrogen (H2) molecule in 1927. Heitler and London's method was extended by the American theoretical physicist John C. Slater and the American theoretical chemist Linus Pauling to become the Valence-Bond (VB) [or Heitler–London–Slater–Pauling (HLSP)] method. In this method, attention is primarily devoted to the pairwise interactions between atoms, and this method therefore correlates closely with classical chemists' drawings of bonds. It focuses on how the atomic orbitals of an atom combine to give individual chemical bonds when a molecule is formed.    Molecular orbitalEdit   An alternative approach was developed in 1929 by Friedrich Hund and Robert S. Mulliken, in which electrons are described by mathematical functions delocalized over an entire molecule. The Hund–Mulliken approach or molecular orbital (MO) method is less intuitive to chemists, but has turned out capable of predicting spectroscopic properties better than the VB method. This approach is the conceptional basis of the Hartree–Fock method and further post Hartree–Fock methods.    Density functional theoryEdit   The Thomas–Fermi model was developed independently by Thomas and Fermi in 1927. This was the first attempt to describe many-electron systems on the basis of electronic density instead of wave functions, although it was not very successful in the treatment of entire molecules. The method did provide the basis for what is now known as density functional theory. Modern day DFT uses the Kohn-Sham method, where the density functional is split into four terms; the Kohn-Sham kinetic energy, an external potential, exchange and correlation energies. A large part of the focus on developing DFT is on improving the exchange and correlation terms. Though this method is less developed than post Hartree–Fock methods, its significantly lower computational requirements (scaling typically no worse than  with respect to  basis functions, for the pure functionals) allow it to tackle larger polyatomic molecules and even macromolecules. This computational affordability and often comparable accuracy to MP2 and CCSD(T) (post-Hartree–Fock methods) has made it one of the most popular methods in computational chemistry at present.    Chemical dynamicsEdit  A further step can consist of solving the Schrödinger equation with the total molecular Hamiltonian in order to study the motion of molecules. Direct solution of the Schrödinger equation is called quantum molecular dynamics, within the semiclassical approximation semiclassical molecular dynamics, and within the classical mechanics framework molecular dynamics (MD). Statistical approaches, using for example Monte Carlo methods, are also possible.    Adiabatic chemical dynamicsEdit   In adiabatic dynamics, interatomic interactions are represented by single scalar potentials called potential energy surfaces. This is the Born–Oppenheimer approximation introduced by Born and Oppenheimer in 1927. Pioneering applications of this in chemistry were performed by Rice and Ramsperger in 1927 and Kassel in 1928, and generalized into the RRKM theory in 1952 by Marcus who took the transition state theory developed by Eyring in 1935 into account. These methods enable simple estimates of unimolecular reaction rates from a few characteristics of the potential surface.    Non-adiabatic chemical dynamicsEdit   Non-adiabatic dynamics consists of taking the interaction between several coupled potential energy surface (corresponding to different electronic quantum states of the molecule). The coupling terms are called vibronic couplings. The pioneering work in this field was done by Stueckelberg, Landau, and Zener in the 1930s, in their work on what is now known as the Landau–Zener transition. Their formula allows the transition probability between two diabatic potential curves in the neighborhood of an avoided crossing to be calculated.    See alsoEdit     ReferencesEdit   Atkins, P.W.; Friedman, R. (2005). Molecular Quantum Mechanics (4th ed.). Oxford University Press. ISBN 978-0-19-927498-7.  Atkins, P.W. Physical Chemistry. Oxford University Press. ISBN 0-19-879285-9.  Atkins, P.W.; Friedman, R. (2008). Quanta, Matter and Change: A Molecular Approach to Physical Change. ISBN 978-0-7167-6117-4.  Pullman, Bernard; Pullman, Alberte (1963). Quantum Biochemistry. New York and London: Academic Press. ISBN 90-277-1830-X.  Scerri, Eric R. (2006). The Periodic Table: Its Story and Its Significance. Oxford University Press. ISBN 0-19-530573-6.  Considers the extent to which chemistry and especially the periodic system has been reduced to quantum mechanics. Kostas Gavroglu, Ana Simões: NEITHER PHYSICS NOR CHEMISTRY.A History of Quantum Chemistry, MIT Press, 2011, ISBN 0-262-01618-4 McWeeny, R. Coulson's Valence. Oxford Science Publications. ISBN 0-19-855144-4.  Karplus M., Porter R.N. (1971). Atoms and Molecules. An introduction for students of physical chemistry, Benjamin–Cummings Publishing Company, ISBN 978-0-8053-5218-4 Szabo, Attila; Ostlund, Neil S. (1996). Modern Quantum Chemistry: Introduction to Advanced Electronic Structure Theory. Dover. ISBN 0-486-69186-1.  Landau, L.D.; Lifshitz, E.M. Quantum Mechanics:Non-relativistic Theory. Course of Theoretical Physic 3. Pergamon Press. ISBN 0-08-019012-X.  Levine, I. (2008). Physical Chemistry (6th ed.). McGraw–Hill Science. ISBN 978-0-07-253862-5.  Pauling, L. (1954). General Chemistry. Dover Publications. ISBN 0-486-65622-5.  Pauling, L.; Wilson, E. B. (1963) [1935]. Introduction to Quantum Mechanics with Applications to Chemistry. Dover Publications. ISBN 0-486-64871-0.  Simon, Z. (1976). Quantum Biochemistry and Specific Interactions. Taylor & Francis. ISBN 978-0-85626-087-2.     External linksEdit  The Sherrill Group – Notes ChemViz Curriculum Support Resources Early ideas in the history of quantum chemistry The Particle Adventure    Nobel lectures by quantum chemistsEdit  Walter Kohn's Nobel lecture Rudolph Marcus' Nobel lecture Robert Mulliken's Nobel lecture Linus Pauling's Nobel lecture John Pople's Nobel lecture","label":"foo"},{"text":"The demarcation problem in the philosophy of science is about how to distinguish between science and nonscience, including between science, pseudoscience, and other products of human activity, like art and literature, and beliefs. The debate continues after over a century of dialogue among philosophers of science and scientists in various fields, and despite broad agreement on the basics of scientific method.    Ancient Greek scienceEdit  An early attempt at demarcation can be seen in the efforts of Greek natural philosophers and medical practitioners to distinguish their methods and their accounts of nature from the mythological or mystical accounts of their predecessors and contemporaries.  Aristotle described at length what was involved in having scientific knowledge of something. To be scientific, he said, one must deal with causes, one must use logical demonstration, and one must identify the universals which 'inhere' in the particulars of sense. But above all, to have science one must have apodictic certainty. It is the last feature which, for Aristotle, most clearly distinguished the scientific way of knowing.  G. E. R. Lloyd notes that there was a sense in which the groups engaged in various forms of inquiry into nature set out to \"legitimate their own positions,\" laying \"claim to a new kind of wisdom ... that purported to yield superior enlightenment, even superior practical effectiveness.\" Medical writers in the Hippocratic tradition maintained that their discussions were based on necessary demonstrations, a theme developed by Aristotle in his Posterior Analytics. One element of this polemic for science was an insistence on a clear and unequivocal presentation of arguments, rejecting the imagery, analogy, and myth of the old wisdom. Some of their claimed naturalistic explanations of phenomena have been found to be quite fanciful, with little reliance on actual observations.    Logical positivismEdit  Logical positivism held that only statements about matters of fact or logical relations between concepts are meaningful. All other statements lack sense and are labelled 'metaphysics' (see the verifiability theory of meaning also known as verificationism). This distinction between science, which in the view of the Vienna Circle possessed empirically verifiable statements, and what they pejoratively called 'metaphysics', which lacked such statements, can be seen as representing another aspect of the demarcation problem. Logical positivism is often discussed in the context of the demarcation between science and non-science or pseudoscience. However, \"The verificationist proposals had the aim of solving a distinctly different demarcation problem, namely that between science and metaphysics.\"    FalsifiabilityEdit  Falsifiability is the demarcation criterion proposed by Karl Popper as opposed to verificationism: \"statements or systems of statements, in order to be ranked as scientific, must be capable of conflicting with possible, or conceivable observations\". Popper saw demarcation as a central problem in the philosophy of science. Unlike the Vienna Circle, Popper stated that his proposal was not a criterion of \"meaningfulness\".  Popper's demarcation criterion has been criticized both for excluding legitimate science… and for giving some pseudosciences the status of being scientific… According to Larry Laudan (1983, 121), it \"has the untoward consequence of countenancing as 'scientific' every crank claim which makes ascertainably false assertions\". Astrology, rightly taken by Popper as an unusually clear example of a pseudoscience, has in fact been tested and thoroughly refuted… Similarly, the major threats to the scientific status of psychoanalysis, another of his major targets, do not come from claims that it is untestable but from claims that it has been tested and failed the tests.  In Popper's later work, he stated that falsifiability is both a necessary and a sufficient criterion for demarcation. He described falsifiability as a property of \"the logical structure of sentences and classes of sentences,\" so that a statement's scientific or non-scientific status does not change over time. This has been summarized as a statement being falsifiable \"if and only if it logically contradicts some (empirical) sentence that describes a logically possible event that it would be logically possible to observe.\"    PostpositivismEdit  Thomas Kuhn, an American historian and philosopher of science, is often connected with what has been called postpositivism or postempiricism. In his 1962 book The Structure of Scientific Revolutions, Kuhn divided the process of doing science into two different endeavors, which he called normal science and extraordinary science (which he sometimes also called \"revolutionary science\"). \"In Kuhn's view, 'it is normal science, in which Sir Karl's sort of testing does not occur, rather than extraordinary science which most nearly distinguishes science from other enterprises'…\" That is, the utility of a scientific paradigm for puzzle-solving, which suggests solutions to new problems while continuing to satisfy all of the problems solved by the paradigm that it replaces.  Kuhn's view of demarcation is most clearly expressed in his comparison of astronomy with astrology. Since antiquity, astronomy has been a puzzle-solving activity and therefore a science. If an astronomer's prediction failed, then this was a puzzle that he could hope to solve for instance with more measurements or with adjustments of the theory. In contrast, the astrologer had no such puzzles since in that discipline \"particular failures did not give rise to research puzzles, for no man, however skilled, could make use of them in a constructive attempt to revise the astrological tradition\"… Therefore, according to Kuhn, astrology has never been a science.  Popper criticized Kuhn's demarcation criterion, saying that astrologers are engaged in puzzle solving, and that therefore Kuhn's criterion recognized astrology as a science. He stated that Kuhn's criterion leads to a \"major disaster…[the] replacement of a rational criterion of science by a sociological one\".    Feyerabend and LakatosEdit  Kuhn's work largely called into question Popper's demarcation, and emphasized the human, subjective quality of scientific change. Paul Feyerabend was concerned that the very question of demarcation was insidious: science itself had no need of a demarcation criterion, but instead some philosophers were seeking to justify a special position of authority from which science could dominate public discourse. Feyerabend argued that science does not in fact occupy a special place in terms of either its logic or method, and no claim to special authority made by scientists can be upheld. He argued that, within the history of scientific practice, no rule or method can be found that has not been violated or circumvented at some point in order to advance scientific knowledge. Both Lakatos and Feyerabend suggest that science is not an autonomous form of reasoning, but is inseparable from the larger body of human thought and inquiry.    ThagardEdit  Paul R. Thagard has proposed another set of principles to try to overcome these difficulties, and believes it is important for society to find a way of doing so. According to Thagard's method, a theory is not scientific if it satisfies two conditions:  Thagard specifies that sometimes theories will spend some time as merely \"unpromising\" before they truly deserve the title of pseudoscience. He cites astrology as an example: it was stagnant compared to advances in physics during the 17th century, and only later became \"pseudoscience\" in the advent of alternative explanations provided by psychology during the 19th century. Thagard also states that his criteria should not be interpreted so narrowly as to allow willful ignorance of alternative explanations, or so broadly as to discount our modern science compared to science of the future. His definition is a practical one, which generally seeks to distinguish pseudoscience as areas of inquiry which are stagnant and without active scientific investigation.    Some historians' perspectivesEdit  Many historians of science are concerned with the development of science from its primitive origins; consequently they define science in sufficiently broad terms to include early forms of natural knowledge. In the article on science in the eleventh edition of the Encyclopædia Britannica, the scientist and historian William Cecil Dampier Whetham defined science as \"ordered knowledge of natural phenomena and of the relations between them.\" In his study of Greek science, Marshall Clagett defined science as \"first, the orderly and systematic comprehension, description and/or explanation of natural phenomena and, secondly, the [mathematical and logical] tools necessary for the undertaking.\" A similar definition appeared more recently in David Pingree's study of early science: \"Science is a systematic explanation of perceived or imaginary phenomena, or else is based on such an explanation. Mathematics finds a place in science only as one of the symbolical languages in which scientific explanations may be expressed.\"  These definitions tend to focus more on the subject matter of science than on its method and from these perspectives, the philosophical concern to establish a line of demarcation between science and non-science becomes \"problematic, if not futile.\"    LaudanEdit  Larry Laudan concluded, after examining various historical attempts to establish a demarcation criterion, that \"philosophy has failed to deliver the goods\" in its attempts to distinguish science from non-science—to distinguish science from pseudoscience. None of the past attempts would be accepted by a majority of philosophers nor, in his view, should they be accepted by them or by anyone else. He stated that many well-founded beliefs are not scientific and, conversely, many scientific conjectures are not well-founded. He also stated that demarcation criteria were historically used as \"machines de guerre\" in polemical disputes between \"scientists\" and \"pseudo-scientists.\" Advancing a number of examples from everyday practice of football and carpentry and non-scientific scholarship such as literary criticism and philosophy, he saw the question of whether a belief is well-founded or not to be more practically and philosophically significant than whether it is scientific or not. In his judgment, the demarcation between science and non-science was a pseudo-problem that would best be replaced by focusing on the distinction between reliable and unreliable knowledge, without bothering to ask whether that knowledge is scientific or not. He would consign phrases like \"pseudo-science\" or \"unscientific\" to the rhetoric of politicians or sociologists. Others have disagreed with Laudan. Sebastian Lutz, for example, argues that demarcation does not have to be a single necessary and sufficient condition as Laudan implied. Rather, Laudan's reasoning at the most establishes that there has to be one necessary criterion and one possibly different sufficient criterion. Other critics have argued for multiple demarcation criteria suggesting that there should be one set of criteria for the natural sciences; another set of criteria for the social sciences, and claims involving the supernatural could have a set of pseudoscientific criteria. Massimo Pigliucci wrote that science generally conforms to Ludwig Wittgenstein's concept of family resemblances.    See alsoEdit  Boundary-work    ReferencesEdit","label":"foo"},{"text":"","label":"foo"},{"text":"Philosophy is the study of the general and fundamental nature of reality, existence, knowledge, values, reason, mind, and language. The Ancient Greek word φιλοσοφία (philosophia) was probably coined by Pythagoras and literally means \"love of wisdom\" or \"friend of wisdom\". Philosophy has been divided into many sub-fields. It has been divided chronologically (e.g., ancient and modern); by topic (the major topics being epistemology, logic, metaphysics, ethics, and aesthetics); and by style (e.g., analytic philosophy). As a method, philosophy is often distinguished from other ways of addressing such problems by its questioning, critical, generally systematic approach and its reliance on rational argument. As a noun, the term \"philosophy\" can refer to any body of knowledge. Historically, these bodies of knowledge were commonly divided into natural philosophy, moral philosophy, and metaphysical philosophy. In casual speech, the term can refer to any of \"the most basic beliefs, concepts, and attitudes of an individual or group,\" (e.g., \"Dr. Smith's philosophy of parenting\").    Areas of inquiryEdit  Philosophy has been divided into many sub-fields. In modern universities, these sub-fields are distinguished either by chronology or topic or style. Chronological divisions include ancient, medieval, modern, and contemporary. Topical divisions include epistemology, logic, metaphysics, ethics, and aesthetics. Divisions of style include analytic, continental, and social/political philosophy, among others. Some of the major areas of study are considered individually below.    EpistemologyEdit   Epistemology is concerned with the nature and scope of knowledge, such as the relationships between truth, belief, perception and theories of justification. Skepticism is the position which questions the possibility of completely justifying any truth. The regress argument, a fundamental problem in epistemology, occurs when, in order to completely prove any statement, its justification itself needs to be supported by another justification. This chain can do three possible options, all of which are unsatisfactory according to the Münchhausen trilemma. One option is infinitism, where this chain of justification can go on forever. Another option is foundationalism, where the chain of justifications eventually relies on basic beliefs or axioms that are left unproven. The last option, such as in coherentism, is making the chain circular so that a statement is included in its own chain of justification. Rationalism is the emphasis on reasoning as a source of knowledge. Empiricism is the emphasis on observational evidence via sensory experience over other evidence as the source of knowledge. Rationalism claims that every possible object of knowledge can be deduced from coherent premises without observation. Empiricism claims that at least some knowledge is only a matter of observation. For this, Empiricism often cites the concept of tabula rasa, where individuals are not born with mental content and that knowledge builds from experience or perception. Epistemological solipsism is the idea that the existence of the world outside the mind is an unresolvable question.  Parmenides (fl. 500 BC) argued that it is impossible to doubt that thinking actually occurs. But thinking must have an object, therefore something beyond thinking really exists. Parmenides deduced that what really exists must have certain properties—for example, that it cannot come into existence or cease to exist, that it is a coherent whole, that it remains the same eternally (in fact, exists altogether outside time). This is known as the third man argument. Plato (427–347 BC) combined rationalism with a form of realism. The philosopher's work is to consider being, and the essence (ousia) of things. But the characteristic of essences is that they are universal. The nature of a man, a triangle, a tree, applies to all men, all triangles, all trees. Plato argued that these essences are mind-independent \"forms\", that humans (but particularly philosophers) can come to know by reason, and by ignoring the distractions of sense-perception. Modern rationalism begins with Descartes. Reflection on the nature of perceptual experience, as well as scientific discoveries in physiology and optics, led Descartes (and also Locke) to the view that we are directly aware of ideas, rather than objects. This view gave rise to three questions: Is an idea a true copy of the real thing that it represents? Sensation is not a direct interaction between bodily objects and our sense, but is a physiological process involving representation (for example, an image on the retina). Locke thought that a \"secondary quality\" such as a sensation of green could in no way resemble the arrangement of particles in matter that go to produce this sensation, although he thought that \"primary qualities\" such as shape, size, number, were really in objects. How can physical objects such as chairs and tables, or even physiological processes in the brain, give rise to mental items such as ideas? This is part of what became known as the mind-body problem. If all the contents of awareness are ideas, how can we know that anything exists apart from ideas? Descartes tried to address the last problem by reason. He began, echoing Parmenides, with a principle that he thought could not coherently be denied: I think, therefore I am (often given in his original Latin: Cogito ergo sum). From this principle, Descartes went on to construct a complete system of knowledge (which involves proving the existence of God, using, among other means, a version of the ontological argument). His view that reason alone could yield substantial truths about reality strongly influenced those philosophers usually considered modern rationalists (such as Baruch Spinoza, Gottfried Leibniz, and Christian Wolff), while provoking criticism from other philosophers who have retrospectively come to be grouped together as empiricists.    LogicEdit   Logic is the study of the principles of correct reasoning. Arguments use either deductive reasoning or inductive reasoning. Deductive reasoning is when, given certain statements (called premises), other statements (called conclusions) are unavoidably implied. Rules of inference from premises include the most popular method, modus ponens, where given “A” and “If A then B”, then “B” must be concluded. A common convention for a deductive argument is the syllogism. An argument is termed valid if its conclusion does follow from its premises, whether the premises are true or not, while an argument is sound if its conclusion follows from premises that are true. Propositional logic uses premises that are propositions, which are declarations that are either true or false, while predicate logic uses more complex premises called formulae that contain variables. These can be assigned values or can be quantified as to when they apply with the universal quantifier (always apply) or the existential quantifier (applies at least once). Inductive reasoning makes conclusions or generalizations based on probabilistic reasoning. For example, if “90% of humans are right-handed” and “Joe is human” then “Joe is probably right-handed”. Fields in logic include mathematical logic (formal symbolic logic) and philosophical logic.    MetaphysicsEdit   Metaphysics is the study of the most general features of reality, such as existence, time, the relationship between mind and body, objects and their properties, wholes and their parts, events, processes, and causation. Traditional branches of metaphysics include cosmology, the study of the world in its entirety, and ontology, the study of being. Within metaphysics itself there are a wide range of differing philosophical theories. Idealism, for example, is the belief that reality is mentally constructed or otherwise immaterial while realism holds that reality, or at least some part of it, exists independently of the mind. Subjective idealism describes objects as no more than collections or \"bundles\" of sense data in the perceiver. The 18th-century philosopher George Berkeley contended that existence is fundamentally tied to perception with the phrase Esse est aut percipi aut percipere or \"To be is to be perceived or to perceive\". In addition to the aforementioned views, however, there is also an ontological dichotomy within metaphysics between the concepts of particulars and universals as well. Particulars are those objects that are said to exist in space and time, as opposed to abstract objects, such as numbers. Universals are properties held by multiple particulars, such as redness or a gender. The type of existence, if any, of universals and abstract objects is an issue of serious debate within metaphysical philosophy. Realism is the philosophical position that universals do in fact exist, while nominalism is the negation, or denial of universals, abstract objects, or both. Conceptualism holds that universals exist, but only within the mind's perception. The question of whether or not existence is a predicate has been discussed since the Early Modern period. Essence is the set of attributes that make an object what it fundamentally is and without which it loses its identity. Essence is contrasted with accident: a property that the substance has contingently, without which the substance can still retain its identity.    Ethics and political philosophyEdit   Ethics, or \"moral philosophy,\" is concerned primarily with the question of the best way to live, and secondarily, concerning the question of whether this question can be answered. The main branches of ethics are meta-ethics, normative ethics, and applied ethics. Meta-ethics concerns the nature of ethical thought, such as the origins of the words good and bad, and origins of other comparative words of various ethical systems, whether there are absolute ethical truths, and how such truths could be known. Normative ethics are more concerned with the questions of how one ought to act, and what the right course of action is. This is where most ethical theories are generated. Lastly, applied ethics go beyond theory and step into real world ethical practice, such as questions of whether or not abortion is correct. Ethics is also associated with the idea of morality, and the two are often interchangeable.  One debate that has commanded the attention of ethicists in the modern era has been between consequentialism (actions are to be morally evaluated solely by their consequences) and deontology (actions are to be morally evaluated solely by consideration of agents' duties, the rights of those whom the action concerns, or both). Jeremy Bentham and John Stuart Mill are famous for promulgating utilitarianism, which is the idea that the fundamental moral rule is to strive toward the \"greatest happiness for the greatest number\". However, in promoting this idea they also necessarily promoted the broader doctrine of consequentialism. Adopting a position opposed to consequentialism, Immanuel Kant argued that moral principles were simply products of reason. Kant believed that the incorporation of consequences into moral deliberation was a deep mistake, since it denies the necessity of practical maxims in governing the working of the will. According to Kant, reason requires that we conform our actions to the categorical imperative, which is an absolute duty. An important 20th-century deontologist, W.D. Ross, argued for weaker forms of duties called prima facie duties. More recent works have emphasized the role of character in ethics, a movement known as the aretaic turn (that is, the turn towards virtues). One strain of this movement followed the work of Bernard Williams. Williams noted that rigid forms of consequentialism and deontology demanded that people behave impartially. This, Williams argued, requires that people abandon their personal projects, and hence their personal integrity, in order to be considered moral. G.E.M. Anscombe, in an influential paper, \"Modern Moral Philosophy\" (1958), revived virtue ethics as an alternative to what was seen as the entrenched positions of Kantianism and consequentialism. Aretaic perspectives have been inspired in part by research of ancient conceptions of virtue. For example, Aristotle's ethics demands that people follow the Aristotelian mean, or balance between two vices; and Confucian ethics argues that virtue consists largely in striving for harmony with other people. Virtue ethics in general has since gained many adherents, and has been defended by such philosophers as Philippa Foot, Alasdair MacIntyre, and Rosalind Hursthouse.  Political philosophy is the study of government and the relationship of individuals (or families and clans) to communities including the state. It includes questions about justice, law, property, and the rights and obligations of the citizen. Politics and ethics are traditionally inter-linked subjects, as both discuss the question of what is good and how people should live. From ancient times, and well beyond them, the roots of justification for political authority were inescapably tied to outlooks on human nature. In The Republic, Plato presented the argument that the ideal society would be run by a council of philosopher-kings, since those best at philosophy are best able to realize the good. Even Plato, however, required philosophers to make their way in the world for many years before beginning their rule at the age of fifty. For Aristotle, humans are political animals (i.e. social animals), and governments are set up to pursue good for the community. Aristotle reasoned that, since the state (polis) was the highest form of community, it has the purpose of pursuing the highest good. Aristotle viewed political power as the result of natural inequalities in skill and virtue. Because of these differences, he favored an aristocracy of the able and virtuous. For Aristotle, the person cannot be complete unless he or she lives in a community. His The Nicomachean Ethics and The Politics are meant to be read in that order. The first book addresses virtues (or \"excellences\") in the person as a citizen; the second addresses the proper form of government to ensure that citizens will be virtuous, and therefore complete. Both books deal with the essential role of justice in civic life. Nicolas of Cusa rekindled Platonic thought in the early 15th century. He promoted democracy in Medieval Europe, both in his writings and in his organization of the Council of Florence. Unlike Aristotle and the Hobbesian tradition to follow, Cusa saw human beings as equal and divine (that is, made in God's image), so democracy would be the only just form of government. Cusa's views are credited by some as sparking the Italian Renaissance, which gave rise to the notion of \"Nation-States\".  Later, Niccolò Machiavelli rejected the views of Aristotle and Thomas Aquinas as unrealistic. The ideal sovereign is not the embodiment of the moral virtues; rather the sovereign does whatever is successful and necessary, rather than what is morally praiseworthy. Thomas Hobbes also contested many elements of Aristotle's views. For Hobbes, human nature is essentially anti-social: people are essentially egoistic, and this egoism makes life difficult in the natural state of things. Moreover, Hobbes argued, though people may have natural inequalities, these are trivial, since no particular talents or virtues that people may have will make them safe from harm inflicted by others. For these reasons, Hobbes concluded that the state arises from a common agreement to raise the community out of the state of nature. This can only be done by the establishment of a sovereign, in which (or whom) is vested complete control over the community, and is able to inspire awe and terror in its subjects.  Many in the Enlightenment were unsatisfied with existing doctrines in political philosophy, which seemed to marginalize or neglect the possibility of a democratic state. Jean-Jacques Rousseau was among those who attempted to overturn these doctrines: he responded to Hobbes by claiming that a human is by nature a kind of \"noble savage\", and that society and social contracts corrupt this nature. Another critic was John Locke. In Second Treatise on Government he agreed with Hobbes that the nation-state was an efficient tool for raising humanity out of a deplorable state, but he argued that the sovereign might become an abominable institution compared to the relatively benign unmodulated state of nature. Following the doctrine of the fact-value distinction, due in part to the influence of David Hume and his student Adam Smith, appeals to human nature for political justification were weakened. Nevertheless, many political philosophers, especially moral realists, still make use of some essential human nature as a basis for their arguments. Marxism is derived from the work of Karl Marx and Friedrich Engels. Their idea that capitalism is based on exploitation of workers and causes alienation of people from their human nature, the historical materialism, their view of social classes, etc., have influenced many fields of study, such as sociology, economics, and politics. Marxism inspired the Marxist school of communism, which brought a huge impact on the history of the 20th century.    AestheticsEdit   Aesthetics deals with beauty, art, enjoyment, sensory-emotional values, perception, and matters of taste and sentiment. It is a branch of philosophy dealing with the nature of art, beauty, and taste, with the creation and appreciation of beauty. It is more scientifically defined as the study of sensory or sensori-emotional values, sometimes called judgments of sentiment and taste. More broadly, scholars in the field define aesthetics as \"critical reflection on art, culture and nature.\" More specific aesthetic theory, often with practical implications, relating to a particular branch of the arts is divided into areas of aesthetics such as art theory, literary theory, film theory and music theory. An example from art theory is aesthetic theory as a set of principles underlying the work of a particular artist or artistic movement: such as the Cubist aesthetic.    Specialized branchesEdit  Philosophy of history refers to the theoretical aspect of history. Philosophy of language explores the nature, the origins, and the use of language. Philosophy of law (often called jurisprudence) explores the varying theories explaining the nature and the interpretations of law. Philosophy of mind explores the nature of the mind, and its relationship to the body, and is typified by disputes between dualism and materialism. In recent years there has been increasing similarity between this branch of philosophy and cognitive science. Philosophy of religion explores questions that often arise in connection with one or several religions, including the soul, the afterlife, God, religious experiences, analysis of religious vocabulary and texts, and the relationship of religion and science. Philosophy of science explores the foundations, methods, history, implications, and purpose of science. Feminist philosophy explores questions surrounding gender, sexuality, and the body including the nature of feminism itself as a social and philosophical movement. Philosophy of film analyzes films and filmmakers for their philosophical content and style explores film (images, cinema, etc.) as a medium for philosophical reflection and expression. Metaphilosophy explores the aims of philosophy, its boundaries, and its methods. Many academic disciplines have also generated philosophical inquiry. These include history, logic, and mathematics.    HistoryEdit   Many societies have considered philosophical questions and built philosophical traditions based upon each other's works. Eastern philosophy is organized by the chronological periods of each region. Historians of western philosophy usually divide the subject into three or more periods, the most important being ancient philosophy, medieval philosophy, and modern philosophy.    Ancient philosophyEdit   In Western philosophy, the spread of Christianity through the Roman Empire marked the ending of Hellenistic philosophy and ushered in the beginnings of Medieval philosophy, whereas in Eastern philosophy, the spread of Islam through the Arab Empire marked the end of Old Iranian philosophy and ushered in the beginnings of early Islamic philosophy. Genuinely philosophical thought, depending upon original individual insights, arose in many cultures roughly contemporaneously. Karl Jaspers termed the intense period of philosophical development beginning around the 7th century and concluding around the 3rd century BCE an Axial Age in human thought.   = Egypt and BabylonEdit =  There are authors who date the philosophical maxims of Ptahhotep before the 25th century. For instance, Pulitzer Prize–winning historian Will Durant dates these writings as early as 2880 BCE within The Story of Civilization: Our Oriental History. Durant claims that Ptahhotep could be considered the very first philosopher in virtue of having the earliest and surviving fragments of moral philosophy (i.e., \"The Maxims of Ptah-Hotep\"). Ptahhotep's grandson, Ptahhotep Tshefi, is traditionally credited with being the author of the collection of wise sayings known as The Maxims of Ptahhotep, whose opening lines attribute authorship to the vizier Ptahhotep: Instruction of the Mayor of the city, the Vizier Ptahhotep, under the Majesty of King Isesi. The origins of Babylonian philosophy can be traced back to the wisdom of early Mesopotamia, which embodied certain philosophies of life, particularly ethics, in the forms of dialectic, dialogues, epic poetry, folklore, hymns, lyrics, prose, and proverbs. The reasoning and rationality of the Babylonians developed beyond empirical observation. The Babylonian text Dialog of Pessimism contains similarities to the agnostic thought of the sophists, the Heraclitean doctrine of contrasts, and the dialogues of Plato, as well as a precursor to the maieutic Socratic method of Socrates and Plato. The Milesian philosopher Thales is also traditionally said to have studied philosophy in Mesopotamia.   = Ancient ChineseEdit =  Philosophy has had a tremendous effect on Chinese civilization, and throughout East Asia. The majority of Chinese philosophy originates in the Spring and Autumn and Warring States era, during a period known as the \"Hundred Schools of Thought\", which was characterized by significant intellectual and cultural developments. It was during this era that the major philosophies of China, Confucianism, Mohism, Legalism, and Taoism, arose, along with philosophies that later fell into obscurity, like Agriculturalism, Chinese Naturalism, and the Logicians. Of the many philosophical schools of China, only Confucianism and Taoism existed after the Qin Dynasty suppressed any Chinese philosophy that was opposed to Legalism. Confucianism is humanistic, philosophy that believes that human beings are teachable, improvable and perfectible through personal and communal endeavour especially including self-cultivation and self-creation. Confucianism focuses on the cultivation of virtue and maintenance of ethics, the most basic of which are ren, yi, and li. Ren is an obligation of altruism and humaneness for other individuals within a community, yi is the upholding of righteousness and the moral disposition to do good, and li is a system of norms and propriety that determines how a person should properly act within a community. Taoism focuses on establishing harmony with the Tao, which is origin of and the totality of everything that exists. The word \"Tao\" (or \"Dao\", depending on the romanization scheme) is usually translated as \"way\", \"path\" or \"principle\". Taoist propriety and ethics emphasize the Three Jewels of the Tao: compassion, moderation, and humility, while Taoist thought generally focuses on nature, the relationship between humanity and the cosmos (天人相应); health and longevity; and wu wei, action through inaction. Harmony with the Universe, or the origin of it through the Tao, is the intended result of many Taoist rules and practices.   = Ancient Graeco-RomanEdit =  Ancient Graeco-Roman philosophy is a period of Western philosophy, starting in the 6th century [c. 585] BC to the 6th century AD. It is usually divided into three periods: the pre-Socratic period, the Ancient Classical Greek period of Plato and Aristotle, and the post-Aristotelian (or Hellenistic) period. A fourth period that is sometimes added includes the Neoplatonic and Christian philosophers of Late Antiquity. The most important of the ancient philosophers (in terms of subsequent influence) are Plato and Aristotle. Plato specifically, is credited as the founder of Western philosophy. The philosopher Alfred North Whitehead said of Plato: \"The safest general characterization of the European philosophical tradition is that it consists of a series of footnotes to Plato. I do not mean the systematic scheme of thought which scholars have doubtfully extracted from his writings. I allude to the wealth of general ideas scattered through them.\" It was said in Roman Ancient history that Pythagoras was the first man to call himself a philosopher, or lover of wisdom, and Pythagorean ideas exercised a marked influence on Plato, and through him, all of Western philosophy. Plato and Aristotle, the first Classical Greek philosophers, did refer critically to other simple \"wise men\", which were called in Greek \"sophists,\" and which were common before Pythagoras' time. From their critique it appears that a distinction was then established in their own Classical period between the more elevated and pure \"lovers of wisdom\" (the true Philosophers), and these other earlier and more common traveling teachers, who often also earned money from their craft. The main subjects of ancient philosophy are: understanding the fundamental causes and principles of the universe; explaining it in an economical way; the epistemological problem of reconciling the diversity and change of the natural universe, with the possibility of obtaining fixed and certain knowledge about it; questions about things that cannot be perceived by the senses, such as numbers, elements, universals, and gods. Socrates is said to have been the initiator of more focused study upon the human things including the analysis of patterns of reasoning and argument and the nature of the good life and the importance of understanding and knowledge in order to pursue it; the explication of the concept of justice, and its relation to various political systems. In this period the crucial features of the Western philosophical method were established: a critical approach to received or established views, and the appeal to reason and argumentation. This includes Socrates' dialectic method of inquiry, known as the Socratic method or method of \"elenchus\", which he largely applied to the examination of key moral concepts such as the Good and Justice. To solve a problem, it would be broken down into a series of questions, the answers to which gradually distill the answer a person would seek. The influence of this approach is most strongly felt today in the use of the scientific method, in which hypothesis is the first stage.   = Ancient IndianEdit =  The term Indian philosophy (Sanskrit: Darshanas), refers to any of several schools of philosophical thought that originated in the Indian subcontinent, including Hindu philosophy, Buddhist philosophy, and Jain philosophy. Having the same or rather intertwined origins, all of these philosophies have a common underlying themes of Dharma and Karma, and similarly attempt to explain the attainment of emancipation. They have been formalized and promulgated chiefly between 1000 BC to a few centuries AD. India's philosophical tradition dates back to the composition of the Upanisads in the later Vedic period (c. 1000-500 BCE). Subsequent schools (Skt: Darshanas) of Indian philosophy were identified as orthodox (Skt: astika) or non-orthodox (Skt: nastika), depending on whether or not they regarded the Vedas as an infallible source of knowledge. By some classifications, there are six schools of orthodox Hindu philosophy and three heterodox schools. The orthodox are Nyaya, Vaisesika, Samkhya, Yoga, Purva mimamsa and Vedanta. The Heterodox are Jain, Buddhist and materialist (Cārvāka). Other classifications also include Pashupata, Saiva, Raseśvara and Pāṇini Darśana with the other orthodox schools. Competition and integration between the various schools was intense during their formative years, especially between 500 BC to 200 AD. Some like the Jain, Buddhist, Shaiva and Vedanta schools survived, while others like Samkhya and Ajivika did not, either being assimilated or going extinct. The Sanskrit term for \"philosopher\" is dārśanika, one who is familiar with the systems of philosophy, or darśanas. In the history of the Indian subcontinent, following the establishment of a Vedic culture, the development of philosophical and religious thought over a period of two millennia gave rise to what came to be called the six schools of astika, or orthodox, Indian or Hindu philosophy. These schools have come to be synonymous with the greater religion of Hinduism, which was a development of the early Vedic religion.   = Ancient PersianEdit =  Persian philosophy can be traced back as far as Old Iranian philosophical traditions and thoughts, with their ancient Indo-Iranian roots. These were considerably influenced by Zarathustra's teachings. Throughout Iranian history and due to remarkable political and social influences such as the Macedonian, the Arab, and the Mongol invasions of Persia, a wide spectrum of schools of thought arose. These espoused a variety of views on philosophical questions, extending from Old Iranian and mainly Zoroastrianism-influenced traditions to schools appearing in the late pre-Islamic era, such as Manicheism and Mazdakism, as well as various post-Islamic schools. Iranian philosophy after Arab invasion of Persia is characterized by different interactions with the old Iranian philosophy, the Greek philosophy and with the development of Islamic philosophy. Illuminationism and the transcendent theosophy are regarded as two of the main philosophical traditions of that era in Persia. Zoroastrianism has been identified as one of the key early events in the development of philosophy.    5th–16th centuriesEdit    = EuropeEdit =    MedievalEdit   Medieval philosophy is the philosophy of Western Europe and the Middle East during the Middle Ages, roughly extending from the Christianization of the Roman Empire until the Renaissance. Medieval philosophy is defined partly by the rediscovery and further development of classical Greek and Hellenistic philosophy, and partly by the need to address theological problems and to integrate the then widespread sacred doctrines of Abrahamic religion (Islam, Judaism, and Christianity) with secular learning. The history of western European medieval philosophy is traditionally divided into two main periods: the period in the Latin West following the Early Middle Ages until the 12th century, when the works of Aristotle and Plato were preserved and cultivated; and the \"golden age\" of the 12th, 13th and 14th centuries in the Latin West, which witnessed the culmination of the recovery of ancient philosophy, and significant developments in the field of philosophy of religion, logic and metaphysics. The medieval era was disparagingly treated by the Renaissance humanists, who saw it as a barbaric \"middle\" period between the classical age of Greek and Roman culture, and the \"rebirth\" or renaissance of classical culture. Yet this period of nearly a thousand years was the longest period of philosophical development in Europe, and possibly the richest. Jorge Gracia has argued that \"in intensity, sophistication, and achievement, the philosophical flowering in the thirteenth century could be rightly said to rival the golden age of Greek philosophy in the fourth century B.C.\" Some problems discussed throughout this period are the relation of faith to reason, the existence and unity of God, the object of theology and metaphysics, the problems of knowledge, of universals, and of individuation. Philosophers from the Middle Ages include the Christian philosophers Augustine of Hippo, Boethius, Anselm, Gilbert of Poitiers, Peter Abelard, Roger Bacon, Bonaventure, Thomas Aquinas, Duns Scotus, William of Ockham and Jean Buridan; the Jewish philosophers Maimonides and Gersonides; and the Muslim philosophers Alkindus, Alfarabi, Alhazen, Avicenna, Algazel, Avempace, Abubacer, Ibn Khaldūn, and Averroes. The medieval tradition of Scholasticism continued to flourish as late as the 17th century, in figures such as Francisco Suarez and John of St. Thomas. Aquinas, father of Thomism, was immensely influential in Catholic Europe, placed a great emphasis on reason and argumentation, and was one of the first to use the new translation of Aristotle's metaphysical and epistemological writing. His work was a significant departure from the Neoplatonic and Augustinian thinking that had dominated much of early Scholasticism.    RenaissanceEdit   The Renaissance (\"rebirth\") was a period of transition between the Middle Ages and modern thought, in which the recovery of classical texts helped shift philosophical interests away from technical studies in logic, metaphysics, and theology towards eclectic inquiries into morality, philology, and mysticism. The study of the classics and the humane arts generally, such as history and literature, enjoyed a scholarly interest hitherto unknown in Christendom, a tendency referred to as humanism. Displacing the medieval interest in metaphysics and logic, the humanists followed Petrarch in making man and his virtues the focus of philosophy. The study of classical philosophy also developed in two new ways. On the one hand, the study of Aristotle was changed through the influence of Averroism. The disagreements between these Averroist Aristotelians, and more orthodox catholic Aristotelians such as Albertus Magnus and Thomas Aquinas eventually contributed to the development of a \"humanist Aristotelianism\" developed in the Renaissance, as exemplified in the thought of Pietro Pomponazzi and Giacomo Zabarella. Secondly, as an alternative to Aristotle, the study of Plato and the Neoplatonists became common. This was assisted by the rediscovery of works which had not been well known previously in Western Europe. Notable Renaissance Platonists include Nicholas of Cusa, and later Marsilio Ficino and Giovanni Pico della Mirandola. The Renaissance also renewed interest in anti-Aristotelian theories of nature considered as an organic, living whole comprehensible independently of theology, as in the work of Nicholas of Cusa, Nicholas Copernicus, Giordano Bruno, Telesius, and Tommaso Campanella. Such movements in natural philosophy dovetailed with a revival of interest in occultism, magic, hermeticism, and astrology, which were thought to yield hidden ways of knowing and mastering nature (e.g., in Marsilio Ficino and Giovanni Pico della Mirandola). These new movements in philosophy developed contemporaneously with larger religious and political transformations in Europe: the Reformation and the decline of feudalism. Though the theologians of the Protestant Reformation showed little direct interest in philosophy, their destruction of the traditional foundations of theological and intellectual authority harmonized with a revival of fideism and skepticism in thinkers such as Erasmus, Montaigne, and Francisco Sanches. Meanwhile, the gradual centralization of political power in nation-states was echoed by the emergence of secular political philosophies, as in the works of Niccolò Machiavelli (often described as the first modern political thinker, or a key turning point towards modern political thinking), Thomas More, Erasmus, Justus Lipsius, Jean Bodin, and Hugo Grotius.   = East AsiaEdit =  Mid-Imperial Chinese philosophy is primarily defined by the development of Neo-Confucianism. During the Tang Dynasty, Buddhism from Nepal also became a prominent philosophical and religious discipline. (It should be noted that philosophy and religion were clearly distinguished in the West, whilst these concepts were more continuous in the East due to, for example, the philosophical concepts of Buddhism.) Neo-Confucianism is a philosophical movement that advocated a more rationalist and secular form of Confucianism by rejecting superstitious and mystical elements of Daoism and Buddhism that had influenced Confucianism during and after the Han Dynasty. Although the Neo-Confucianists were critical of Daoism and Buddhism, the two did have an influence on the philosophy, and the Neo-Confucianists borrowed terms and concepts from both. However, unlike the Buddhists and Daoists, who saw metaphysics as a catalyst for spiritual development, religious enlightenment, and immortality, the Neo-Confucianists used metaphysics as a guide for developing a rationalist ethical philosophy. Neo-Confucianism has its origins in the Tang Dynasty; the Confucianist scholars Han Yu and Li Ao are seen as forbears of the Neo-Confucianists of the Song Dynasty. The Song Dynasty philosopher Zhou Dunyi is seen as the first true \"pioneer\" of Neo-Confucianism, using Daoist metaphysics as a framework for his ethical philosophy. Elsewhere in East Asia, Japanese Philosophy began to develop as indigenous Shinto beliefs fused with Buddhism, Confucianism and other schools of Chinese philosophy and Indian philosophy. Similar to Japan, in Korean philosophy the emotional content of Shamanism was integrated into the Neo-Confucianism imported from China. Vietnamese philosophy was also influenced heavily by Confucianism in this period.   = IndiaEdit =  The period between 5th and 9th centuries CE was the most brilliant epoch in the development of Indian philosophy as Hindu and Buddhist philosophies flourished side by side. Of these various schools of thought the non-dualistic Advaita Vedanta emerged as the most influential and most dominant school of philosophy. The major philosophers of this school were Gaudapada, Adi Shankara and Vidyaranya. Advaita Vedanta rejects theism and dualism by insisting that “Brahman [ultimate reality] is without parts or attributes...one without a second.” Since Brahman has no properties, contains no internal diversity and is identical with the whole reality, it cannot be understood as God. Brahman though being indescribable is at best described as Satchidananda (merging \"Sat\" + \"Chit\" + \"Ananda\", i.e., Existence, Consciousness and Bliss) by Shankara. Advaita ushered a new era in Indian philosophy and as a result, many new schools of thought arose in the medieval period. Some of them were Visishtadvaita (qualified monism), Dvaita (dualism), Dvaitadvaita (dualism-nondualism), Suddhadvaita (pure non-dualism), Achintya Bheda Abheda and Pratyabhijña (the recognitive school).   = Middle EastEdit =  In early Islamic thought, which refers to philosophy during the \"Islamic Golden Age\", traditionally dated between the 8th and 12th centuries, two main currents may be distinguished. The first is Kalam, that mainly dealt with Islamic theological questions. These include the Mu'tazili and Ash'ari. The other is Falsafa, that was founded on interpretations of Aristotelianism and Neoplatonism. There were attempts by later philosopher-theologians at harmonizing both trends, notably by Ibn Sina (Avicenna) who founded the school of Avicennism, Ibn Rushd (Averroës) who founded the school of Averroism, and others such as Ibn al-Haytham (Alhacen) and Abū Rayhān al-Bīrūnī.   = MesoamericaEdit =  Aztec philosophy was the school of philosophy developed by the Aztec Empire. The Aztecs had a well-developed school of philosophy, perhaps the most developed in the Americas and in many ways comparable to Greek philosophy, even amassing more texts than the ancient Greeks. Aztec philosophy focused on dualism, monism, and aesthetics, and Aztec philosophers attempted to answer the main Aztec philosophical question of how to gain stability and balance in an ephemeral world. Aztec philosophy saw the concept of Ometeotl as a unity that underlies the universe. Ometeotl forms, shapes, and is all things. Even things in opposition—light and dark, life and death—were seen as expressions of the same unity, Ometeotl. The belief in a unity with dualistic expressions compares with similar dialectical monist ideas in both Western and Eastern philosophies. Aztec priests had a panentheistic view of religion but the popular Aztec religion maintained polytheism. Priests saw the different gods as aspects of the singular and transcendent unity of teotl but the masses were allowed to practice polytheism without understanding the true, unified nature of the Aztec gods.   = AfricaEdit =  Ethiopian philosophy is the philosophical corpus of the territories of present-day Ethiopia and Eritrea. Besides via oral tradition, it was preserved early on in written form through Ge'ez manuscripts. This philosophy occupies a unique position within African philosophy. The character of Ethiopian philosophy is determined by the particular conditions of evolution of the Ethiopian culture. Thus, Ethiopian philosophy arises from the confluence of Greek and Patristic philosophy with traditional Ethiopian modes of thought. Because of the early isolation from its sources of Abrahamic spirituality – Byzantium and Alexandria – Ethiopia received some of its philosophical heritage through Arabic versions. The sapiential literature developed under these circumstances is the result of a twofold effort of creative assimilation: on one side, of a tuning of Orthodoxy to traditional modes of thought (never eradicated), and vice versa, and, on the other side, of absorption of Greek pagan and early Patristic thought into this developing Ethiopian-Christian synthesis. As a consequence, the moral reflection of religious inspiration is prevalent, and the use of narrative, parable, apothegm and rich imagery is preferred to the use of abstract argument. This sapiential literature consists in translations and adaptations of some Greek texts, namely of the Physiolog (cca. 5th century A.D.), The Life and Maxims of Skendes (11th century A.D.) and The Book of the Wise Philosophers (1510/22). In the 17th century, the religious beliefs of Ethiopians were challenged by King Suseynos' adoption of Catholicism, and by a subsequent presence of Jesuit missionaries. The attempt to forcefully impose Catholicism upon his constituents during Suseynos' reign inspired further development of Ethiopian philosophy during the 17th century. Zera Yacob (1599–1692) is the most important exponent of this renaissance. His treatise Hatata (1667) is a work often included in the narrow canon of universal philosophy.    17th–20th centuriesEdit    = Early modern philosophyEdit =  Chronologically, the early modern era of Western philosophy is usually identified with the 17th and 18th centuries, with the 18th century often being referred to as the Enlightenment. Modern philosophy is distinguished from its predecessors by its increasing independence from traditional authorities such as the Church, academia, and Aristotelianism; a new focus on the foundations of knowledge and metaphysical system-building; and the emergence of modern physics out of natural philosophy. Other central topics of philosophy in this period include the nature of the mind and its relation to the body, the implications of the new natural sciences for traditional theological topics such as free will and God, and the emergence of a secular basis for moral and political philosophy. These trends first distinctively coalesce in Francis Bacon's call for a new, empirical program for expanding knowledge, and soon found massively influential form in the mechanical physics and rationalist metaphysics of René Descartes. Thomas Hobbes was the first to apply this methodology systematically to political philosophy and is the originator of modern political philosophy, including the modern theory of a \"social contract\". The academic canon of early modern philosophy generally includes Descartes, Spinoza, Leibniz, Locke, Berkeley, Hume, and Kant, though influential contributions to philosophy were made by many thinkers in this period, such as Galileo Galilei, Pierre Gassendi, Blaise Pascal, Nicolas Malebranche, Isaac Newton, Christian Wolff, Montesquieu, Pierre Bayle, Thomas Reid, Jean d'Alembert, and Adam Smith. Jean-Jacques Rousseau was a seminal figure in initiating reaction against the Enlightenment. The approximate end of the early modern period is most often identified with Immanuel Kant's systematic attempt to limit metaphysics, justify scientific knowledge, and reconcile both of these with morality and freedom.   = 19th-century philosophyEdit =  Later modern philosophy is usually considered to begin after the philosophy of Immanuel Kant at the beginning of the 19th century. German philosophy exercised broad influence in this century, owing in part to the dominance of the German university system. German idealists, such as Johann Gottlieb Fichte, Georg Wilhelm Friedrich Hegel, and Friedrich Wilhelm Joseph Schelling, transformed the work of Kant by maintaining that the world is constituted by a rational or mind-like process, and as such is entirely knowable. Arthur Schopenhauer's identification of this world-constituting process as an irrational will to live influenced later 19th- and early 20th-century thinking, such as the work of Friedrich Nietzsche and Sigmund Freud. After Hegel's death in 1831, 19th-century philosophy largely turned against idealism in favor of varieties of philosophical naturalism, such as the positivism of Auguste Comte, the empiricism of John Stuart Mill, and the materialism of Karl Marx. Logic began a period of its most significant advances since the inception of the discipline, as increasing mathematical precision opened entire fields of inference to formalization in the work of George Boole and Gottlob Frege. Other philosophers who initiated lines of thought that would continue to shape philosophy into the 20th century include: Gottlob Frege and Henry Sidgwick, whose work in logic and ethics, respectively, provided the tools for early analytic philosophy. Charles Sanders Peirce and William James, who founded pragmatism. Søren Kierkegaard and Friedrich Nietzsche, who laid the groundwork for existentialism and post-structuralism.   = 20th-century philosophyEdit =  Within the last century, philosophy has increasingly become a professional discipline practiced within universities, like other academic disciplines. Accordingly, it has become less general and more specialized. In the view of one prominent recent historian: \"Philosophy has become a highly organized discipline, done by specialists primarily for other specialists. The number of philosophers has exploded, the volume of publication has swelled, and the subfields of serious philosophical investigation have multiplied. Not only is the broad field of philosophy today far too vast to be embraced by one mind, something similar is true even of many highly specialized subfields.\" In the English-speaking world, analytic philosophy became the dominant school for much of the 20th century. In the first half of the century, it was a cohesive school, shaped strongly by logical positivism, united by the notion that philosophical problems could and should be solved by attention to logic and language. The pioneering work of Bertrand Russell was a model for the early development of analytic philosophy, moving from a rejection of the idealism dominant in late 19th-century British philosophy to an neo-Humean empiricism, strengthened by the conceptual resources of modern mathematical logic. In the latter half of the 20th century, analytic philosophy diffused into a wide variety of disparate philosophical views, only loosely united by historical lines of influence and a self-identified commitment to clarity and rigor. The post-war transformation of the analytic program led in two broad directions: on one hand, an interest in ordinary language as a way of avoiding or redescribing traditional philosophical problems, and on the other, a more thoroughgoing naturalism that sought to dissolve the puzzles of modern philosophy via the results of the natural sciences (such as cognitive psychology and evolutionary biology). The shift in the work of Ludwig Wittgenstein, from a view congruent with logical positivism to a therapeutic dissolution of traditional philosophy as a linguistic misunderstanding of normal forms of life, was the most influential version of the first direction in analytic philosophy. The later work of Russell and the philosophy of Willard Van Orman Quine are influential exemplars of the naturalist approach dominant in the second half of the 20th century. But the diversity of analytic philosophy from the 1970s onward defies easy generalization: the naturalism of Quine and his epigoni was in some precincts superseded by a \"new metaphysics\" of possible worlds, as in the influential work of David Lewis. Recently, the experimental philosophy movement has sought to reappraise philosophical problems through social science research techniques. On continental Europe, no single school or temperament enjoyed dominance. The flight of the logical positivists from central Europe during the 1930s and 1940s, however, diminished philosophical interest in natural science, and an emphasis on the humanities, broadly construed, figures prominently in what is usually called \"continental philosophy\". 20th-century movements such as phenomenology, existentialism, modern hermeneutics, critical theory, structuralism, and poststructuralism are included within this loose category. The founder of phenomenology, Edmund Husserl, sought to study consciousness as experienced from a first-person perspective, while Martin Heidegger drew on the ideas of Kierkegaard, Nietzsche, and Husserl to propose an unconventional existential approach to ontology. In the Arabic-speaking world, Arab nationalist philosophy became the dominant school of thought, involving philosophers such as Michel Aflaq, Zaki al-Arsuzi, Salah al-Din al-Bitar of Ba'athism and Sati' al-Husri. These people disregarded much of Marx's research, and were mostly concerned with the unity of the Arab world.    Major traditionsEdit     German idealismEdit   Forms of idealism were prevalent in philosophy from the 18th century to the early 20th century. Transcendental idealism, advocated by Immanuel Kant, is the view that there are limits on what can be understood, since there is much that cannot be brought under the conditions of objective judgment. Kant wrote his Critique of Pure Reason (1781–1787) in an attempt to reconcile the conflicting approaches of rationalism and empiricism, and to establish a new groundwork for studying metaphysics. Kant's intention with this work was to look at what we know and then consider what must be true about it, as a logical consequence of the way we know it. One major theme was that there are fundamental features of reality that escape our direct knowledge because of the natural limits of the human faculties. Although Kant held that objective knowledge of the world required the mind to impose a conceptual or categorical framework on the stream of pure sensory data—a framework including space and time themselves—he maintained that things-in-themselves existed independently of our perceptions and judgments; he was therefore not an idealist in any simple sense. Kant's account of things-in-themselves is both controversial and highly complex. Continuing his work, Johann Gottlieb Fichte and Friedrich Schelling dispensed with belief in the independent existence of the world, and created a thoroughgoing idealist philosophy. The most notable work of this German idealism was G. W. F. Hegel's Phenomenology of Spirit, of 1807. Hegel admitted his ideas were not new, but that all the previous philosophies had been incomplete. His goal was to correctly finish their job. Hegel asserts that the twin aims of philosophy are to account for the contradictions apparent in human experience (which arise, for instance, out of the supposed contradictions between \"being\" and \"not being\"), and also simultaneously to resolve and preserve these contradictions by showing their compatibility at a higher level of examination (\"being\" and \"not being\" are resolved with \"becoming\"). This program of acceptance and reconciliation of contradictions is known as the \"Hegelian dialectic\". Philosophers influenced by Hegel include Ludwig Andreas Feuerbach, who coined the term projection as pertaining to our inability to recognize anything in the external world without projecting qualities of ourselves upon those things; Karl Marx; Friedrich Engels; and the British idealists, notably T. H. Green, J. M. E. McTaggart and F. H. Bradley. Few 20th-century philosophers have embraced idealism. However, quite a few have embraced Hegelian dialectic. Immanuel Kant's \"Copernican Turn\" also remains an important philosophical concept today.    PragmatismEdit   Pragmatism was founded in the spirit of finding a scientific concept of truth that does not depend on personal insight (revelation) or reference to some metaphysical realm. The meaning or purport of a statement should be judged by the effect its acceptance would have on practice. Truth is that opinion which inquiry taken far enough would ultimately reach. For Charles Sanders Peirce these were principles of the inquirer's self-regulation, implied by the idea and hope that inquiry is not generally fruitless. The details of how these principles should be interpreted have been subject to discussion since Peirce first conceived them. Peirce's maxim of pragmatism is as follows: \"Consider what effects, which might conceivably have practical bearings, we conceive the object of our conception to have. Then, our conception of these effects is the whole of our conception of the object.\" Like postmodern neo-pragmatist Richard Rorty, many are convinced that pragmatism asserts that the truth of beliefs does not consist in their correspondence with reality, but in their usefulness and efficacy. The late 19th-century American philosophers Charles Sanders Peirce and William James were its co-founders, and it was later developed by John Dewey as instrumentalism. Since the usefulness of any belief at any time might be contingent on circumstance, Peirce and James conceptualised final truth as something only established by the future, final settlement of all opinion. Critics have accused pragmatism falling victim to a simple fallacy: because something that is true proves useful, that usefulness is the basis for its truth. Thinkers in the pragmatist tradition have included John Dewey, George Santayana, Quine and C. I. Lewis. Pragmatism has more recently been taken in new directions by Richard Rorty, John Lachs, Donald Davidson, Susan Haack, and Hilary Putnam.    PhenomenologyEdit   Edmund Husserl's phenomenology was an ambitious attempt to lay the foundations for an account of the structure of conscious experience in general. An important part of Husserl's phenomenological project was to show that all conscious acts are directed at or about objective content, a feature that Husserl called intentionality. In the first part of his two-volume work, the Logical Investigations (1901), he launched an extended attack on psychologism. In the second part, he began to develop the technique of descriptive phenomenology, with the aim of showing how objective judgments are grounded in conscious experience—not, however, in the first-person experience of particular individuals, but in the properties essential to any experiences of the kind in question. He also attempted to identify the essential properties of any act of meaning. He developed the method further in Ideas (1913) as transcendental phenomenology, proposing to ground actual experience, and thus all fields of human knowledge, in the structure of consciousness of an ideal, or transcendental, ego. Later, he attempted to reconcile his transcendental standpoint with an acknowledgement of the intersubjective life-world in which real individual subjects interact. Husserl published only a few works in his lifetime, which treat phenomenology mainly in abstract methodological terms; but he left an enormous quantity of unpublished concrete analyses. Husserl's work was immediately influential in Germany, with the foundation of phenomenological schools in Munich and Göttingen. Phenomenology later achieved international fame through the work of such philosophers as Martin Heidegger (formerly Husserl's research assistant), Maurice Merleau-Ponty, and Jean-Paul Sartre. Through the work of Heidegger and Sartre, Husserl's focus on subjective experience influenced aspects of existentialism.    ExistentialismEdit   Existentialism is a term applied to the work of a number of late 19th- and 20th-century philosophers who, despite profound doctrinal differences, shared the belief that philosophical thinking begins with the human subject—not merely the thinking subject, but the acting, feeling, living human individual. In existentialism, the individual's starting point is characterized by what has been called \"the existential attitude\", or a sense of disorientation and confusion in the face of an apparently meaningless or absurd world. Many existentialists have also regarded traditional systematic or academic philosophy, in both style and content, as too abstract and remote from concrete human experience.  Although they did not use the term, the 19th-century philosophers Søren Kierkegaard and Friedrich Nietzsche are widely regarded as the fathers of existentialism. Their influence, however, has extended beyond existentialist thought. The main target of Kierkegaard's writings was the idealist philosophical system of Hegel which, he thought, ignored or excluded the inner subjective life of living human beings. Kierkegaard, conversely, held that \"truth is subjectivity\", arguing that what is most important to an actual human being are questions dealing with an individual's inner relationship to existence. In particular, Kierkegaard, a Christian, believed that the truth of religious faith was a subjective question, and one to be wrestled with passionately. Although Kierkegaard and Nietzsche were among his influences, the extent to which the German philosopher Martin Heidegger should be considered an existentialist is debatable. In Being and Time he presented a method of rooting philosophical explanations in human existence (Dasein) to be analysed in terms of existential categories (existentiale); and this has led many commentators to treat him as an important figure in the existentialist movement. However, in The Letter on Humanism, Heidegger explicitly rejected the existentialism of Jean-Paul Sartre. Sartre became the best-known proponent of existentialism, exploring it not only in theoretical works such as Being and Nothingness, but also in plays and novels. Sartre, along with Simone de Beauvoir, represented an avowedly atheistic branch of existentialism, which is now more closely associated with their ideas of nausea, contingency, bad faith, and the absurd than with Kierkegaard's spiritual angst. Nevertheless, the focus on the individual human being, responsible before the universe for the authenticity of his or her existence, is common to all these thinkers.    Structuralism and post-structuralismEdit   Inaugurated by the linguist Ferdinand de Saussure, structuralism sought to clarify systems of signs through analyzing the discourses they both limit and make possible. Saussure conceived of the sign as being delimited by all the other signs in the system, and ideas as being incapable of existence prior to linguistic structure, which articulates thought. This led continental thought away from humanism, and toward what was termed the decentering of man: language is no longer spoken by man to express a true inner self, but language speaks man. Structuralism sought the province of a hard science, but its positivism soon came under fire by poststructuralism, a wide field of thinkers, some of whom were once themselves structuralists, but later came to criticize it. Structuralists believed they could analyze systems from an external, objective standing, for example, but the poststructuralists argued that this is incorrect, that one cannot transcend structures and thus analysis is itself determined by what it examines, while the distinction between the signifier and signified was treated as crystalline by structuralists, poststructuralists asserted that every attempt to grasp the signified results in more signifiers, so meaning is always in a state of being deferred, making an ultimate interpretation impossible. Structuralism came to dominate continental philosophy throughout the 1960s and early 1970s, encompassing thinkers as diverse as Claude Lévi-Strauss, Roland Barthes and Jacques Lacan. Post-structuralism came to predominate over the 1970s onwards, including thinkers such as Michel Foucault, Jacques Derrida, Gilles Deleuze and even Roland Barthes; it incorporated a critique of structuralism's limitations.    ThomismEdit   Largely Aristotelian in its approach and content, Thomism is a philosophical tradition that follows the writings of Thomas Aquinas. His work has been read, studied, and disputed since the 13th century, especially by Roman Catholics. However, Aquinas has enjoyed a revived interest since the late 19th century, among both atheists (like Philippa Foot) and theists (like Elizabeth Anscombe). Thomist philosophers tend to be rationalists in epistemology, as well as metaphysical realists, and virtue ethicists. Human beings are rational animals whose good can be known by reason and pursued by the will. With regard to the soul, Thomists (like Aristotle) argue that soul or psyche is real and immaterial but inseparable from matter in organisms. Soul is the form of the body. Thomists accept all four of Aristotle's causes as natural, including teleological or final causes. In this way, though Aquinas argued that whatever is in the intellect begins in the senses, natural teleology can be discerned with the senses and abstracted from nature through induction. Contemporary Thomism contains a diversity of philosophical styles, from Neo-Scholasticism to Existential Thomism. The so-called new natural lawyers like Germain Grisez and Robert P. George have applied Thomistic legal principles to contemporary ethical debates, while cognitive neuroscientist Walter Freeman proposes that Thomism is the philosophical system explaining cognition that is most compatible with neurodynamics, in a 2008 article in the journal Mind and Matter entitled \"Nonlinear Brain Dynamics and Intention According to Aquinas.\" So-called Analytical Thomism of John Haldane and others encourages dialogue between analytic philosophy and broadly Aristotelian philosophy of mind, psychology, and hylomorphic metaphysics. Other modern or contemporary Thomists include Eleonore Stump, Alasdair MacIntyre, and John Finnis.    The analytic traditionEdit   The term analytic philosophy roughly designates a group of philosophical methods that stress detailed argumentation, attention to semantics, use of classical logic and non-classical logics and clarity of meaning above all other criteria. Some have held that philosophical problems arise through misuse of language or because of misunderstandings of the logic of our language, while some maintain that there are genuine philosophical problems and that philosophy is continuous with science. Michael Dummett in his Origins of Analytical Philosophy makes the case for counting Gottlob Frege's The Foundations of Arithmetic as the first analytic work, on the grounds that in that book Frege took the linguistic turn, analyzing philosophical problems through language. Bertrand Russell and G.E. Moore are also often counted as founders of analytic philosophy, beginning with their rejection of British idealism, their defense of realism and the emphasis they laid on the legitimacy of analysis. Russell's classic works The Principles of Mathematics, On Denoting and Principia Mathematica with Alfred North Whitehead, aside from greatly promoting the use of mathematical logic in philosophy, set the ground for much of the research program in the early stages of the analytic tradition, emphasizing such problems as: the reference of proper names, whether 'existence' is a property, the nature of propositions, the analysis of definite descriptions, the discussions on the foundations of mathematics; as well as exploring issues of ontological commitment and even metaphysical problems regarding time, the nature of matter, mind, persistence and change, which Russell tackled often with the aid of mathematical logic. Russell and Moore's philosophy, in the beginning of the 20th century, developed as a critique of Hegel and his British followers in particular, and of grand systems of speculative philosophy in general, though by no means all analytic philosophers reject the philosophy of Hegel (see Charles Taylor) nor speculative philosophy. Some schools in the group include logical positivism, and ordinary language both markedly influenced by Russell and Wittgenstein's development of Logical Atomism the former positively and the latter negatively. In 1921, Ludwig Wittgenstein, who studied under Russell at Cambridge, published his Tractatus Logico-Philosophicus, which gave a rigidly \"logical\" account of linguistic and philosophical issues. At the time, he understood most of the problems of philosophy as mere puzzles of language, which could be solved by investigating and then minding the logical structure of language. Years later, he reversed a number of the positions he set out in the Tractatus, in for example his second major work, Philosophical Investigations (1953). Investigations was influential in the development of \"ordinary language philosophy,\" which was promoted by Gilbert Ryle, J.L. Austin, and a few others. In the United States, meanwhile, the philosophy of Quine was having a major influence, with the paper Two Dogmas of Empiricism. In that paper Quine criticizes the distinction between analytic and synthetic statements, arguing that a clear conception of analyticity is unattainable. He argued for holism, the thesis that language, including scientific language, is a set of interconnected sentences, none of which can be verified on its own, rather, the sentences in the language depend on each other for their meaning and truth conditions. A consequence of Quine's approach is that language as a whole has only a thin relation to experience. Some sentences that refer directly to experience might be modified by sense impressions, but as the whole of language is theory-laden, for the whole language to be modified, more than this is required. However, most of the linguistic structure can in principle be revised, even logic, in order to better model the world. Notable students of Quine include Donald Davidson and Daniel Dennett. The former devised a program for giving a semantics to natural language and thereby answer the philosophical conundrum \"what is meaning?\". A crucial part of the program was the use of Alfred Tarski's semantic theory of truth. Dummett, among others, argued that truth conditions should be dispensed within the theory of meaning, and replaced by assertability conditions. Some propositions, on this view, are neither true nor false, and thus such a theory of meaning entails a rejection of the law of the excluded middle. This, for Dummett, entails antirealism, as Russell himself pointed out in his An Inquiry into Meaning and Truth. By the 1970s there was a renewed interest in many traditional philosophical problems by the younger generations of analytic philosophers. David Lewis, Saul Kripke, Derek Parfit and others took an interest in traditional metaphysical problems, which they began exploring by the use of logic and philosophy of language. Among those problems some distinguished ones were: free will, essentialism, the nature of personal identity, identity over time, the nature of the mind, the nature of causal laws, space-time, the properties of material beings, modality, etc. In those universities where analytic philosophy has spread, these problems are still being discussed passionately. Analytic philosophers are also interested in the methodology of analytic philosophy itself, with Timothy Williamson, Wykeham Professor of Logic at Oxford, publishing recently a book entitled The Philosophy of Philosophy. Some influential figures in contemporary analytic philosophy are: Timothy Williamson, David Lewis, John Searle, Thomas Nagel, Hilary Putnam, Michael Dummett, Peter van Inwagen and Saul Kripke. Analytic philosophy has sometimes been accused of not contributing to the political debate or to traditional questions in aesthetics. However, with the appearance of A Theory of Justice by John Rawls and Anarchy, State and Utopia by Robert Nozick, analytic political philosophy acquired respectability. Analytic philosophers have also shown depth in their investigations of aesthetics, with Roger Scruton, Nelson Goodman, Arthur Danto and others developing the subject to its current shape.    Applied philosophyEdit  The ideas conceived by a society have profound repercussions on what actions the society performs. As Richard Weaver has argued, \"ideas have consequences\". The study of philosophy yields applications such as those in ethics—applied ethics in particular—and political philosophy. The political and economic philosophies of Confucius, Sun Zi, Chanakya, Ibn Khaldun, Ibn Rushd, Ibn Taimiyyah, Niccolò Machiavelli, Gottfried Wilhelm Leibniz, John Locke, Jean-Jacques Rousseau, Adam Smith, Karl Marx, John Stuart Mill, Leo Tolstoy, Mahatma Gandhi, Martin Luther King Jr., and others—all of these have been used to shape and justify governments and their actions. In the field of philosophy of education, progressive education as championed by John Dewey has had a profound impact on educational practices in the United States in the 20th century. Descendants of this movement include the current efforts in philosophy for children, which are part of philosophy education. Carl von Clausewitz's political philosophy of war has had a profound effect on statecraft, international politics, and military strategy in the 20th century, especially in the years around World War II. Logic has become crucially important in mathematics, linguistics, psychology, computer science, and computer engineering. Other important applications can be found in epistemology, which aid in understanding the requisites for knowledge, sound evidence, and justified belief (important in law, economics, decision theory, and a number of other disciplines). The philosophy of science discusses the underpinnings of the scientific method and has affected the nature of scientific investigation and argumentation. As such, philosophy has fundamental implications for science as a whole. For example, the strictly empirical approach of Skinner's behaviorism affected for decades the approach of the American psychological establishment. Deep ecology and animal rights examine the moral situation of humans as occupants of a world that has non-human occupants to consider also. Aesthetics can help to interpret discussions of music, literature, the plastic arts, and the whole artistic dimension of life. In general, the various philosophies strive to provide practical activities with a deeper understanding of the theoretical or conceptual underpinnings of their fields. Often philosophy is seen as an investigation into an area not sufficiently well understood to be its own branch of knowledge. For example, what were once philosophical pursuits have evolved into the modern day fields such as psychology, sociology, linguistics, and economics.    See alsoEdit   List of important publications in philosophy List of years in philosophy List of philosophy journals List of unsolved problems in philosophy Lists of philosophers Social theory    ReferencesEdit     Further readingEdit  Bullock, Alan, R. B. Woodings, and John Cumming, eds. The Fontana Dictionary of Modern Thinkers, in series, Fontana Original[s]. Hammersmith, Eng.: Fontana Press, 1992, cop. 1983. xxv, 867 p. ISBN 978-0-00-636965-3 Bullock, Alan, and Oliver Stallybrass, jt. eds. The Harper Dictionary of Modern Thought. New York: Harper & Row, 1977. xix, 684 p. N.B.: \"First published in England under the title, The Fontana Dictionary of Modern Thought.\" ISBN 978-0-06-010578-5 Julia, Didier. Dictionnaire de la philosophie. Responsible éditorial, Emmanuel de Waresquiel; secretariat de rédaction, Joelle Narjollet. [Éd. rev.]. Paris: Larousse, 2006. 301, [1] p. + xvi p. of ill. ISBN 978-2-03-583309-9 Reese, W. L. Dictionary of Philosophy and Religion: Eastern and Western Thought. Atlantic Highlands, N.J.: Humanities Press, 1980. iv, 644 p. ISBN 978-0-391-00688-1    IntroductionsEdit     Topical introductionsEdit     AnthologiesEdit     Reference worksEdit     External linksEdit  Stanford Encyclopedia of Philosophy The Internet Encyclopedia of Philosophy Indiana Philosophy Ontology Project PhilPapers – a comprehensive directory of online philosophical articles and books by academic philosophers Philosophy Timeline Map of Western Philosophers Philosophy Magazines and Journals Philosophy at DMOZ Philosophy (review) Philosophy Documentation Center Popular Philosophy","label":"foo"},{"text":"Technology (from Greek τέχνη, techne, \"art, skill, cunning of hand\"; and -λογία, -logia) is the collection of techniques, skills, methods and processes used in the production of goods or services or in the accomplishment of objectives, such as scientific investigation. Technology can be the knowledge of techniques, processes, etc. or it can be embedded in machines, computers, devices and factories, which can be operated by individuals without detailed knowledge of the workings of such things. The human species' use of technology began with the conversion of natural resources into simple tools. The prehistoric discovery of how to control fire and the later Neolithic Revolution increased the available sources of food and the invention of the wheel helped humans to travel in and control their environment. Developments in historic times, including the printing press, the telephone, and the Internet, have lessened physical barriers to communication and allowed humans to interact freely on a global scale. The steady progress of military technology has brought weapons of ever-increasing destructive power, from clubs to nuclear weapons. Technology has many effects. It has helped develop more advanced economies (including today's global economy) and has allowed the rise of a leisure class. Many technological processes produce unwanted by-products, known as pollution, and deplete natural resources, to the detriment of Earth's environment. Various implementations of technology influence the values of a society and new technology often raises new ethical questions. Examples include the rise of the notion of efficiency in terms of human productivity, a term originally applied only to machines, and the challenge of traditional norms. Philosophical debates have arisen over the use of technology, with disagreements over whether technology improves the human condition or worsens it. Neo-Luddism, anarcho-primitivism, and similar reactionary movements criticise the pervasiveness of technology in the modern world, opining that it harms the environment and alienates people; proponents of ideologies such as transhumanism and techno-progressivism view continued technological progress as beneficial to society and the human condition. Until recently, it was believed that the development of technology was restricted only to human beings, but 21st century scientific studies indicate that other primates and certain dolphin communities have developed simple tools and passed their knowledge to other generations.    Definition and usage   The use of the term \"technology\" has changed significantly over the last 200 years. Before the 20th century, the term was uncommon in English, and usually referred to the description or study of the useful arts. The term was often connected to technical education, as in the Massachusetts Institute of Technology (chartered in 1861). The term \"technology\" rose to prominence in the 20th century in connection with the Second Industrial Revolution. The term's meanings changed in the early 20th century when American social scientists, beginning with Thorstein Veblen, translated ideas from the German concept of Technik into \"technology\". In German and other European languages, a distinction exists between technik and technologie that is absent in English, which usually translates both terms as \"technology\". By the 1930s, \"technology\" referred not only to the study of the industrial arts but to the industrial arts themselves. In 1937, the American sociologist Read Bain wrote that \"technology includes all tools, machines, utensils, weapons, instruments, housing, clothing, communicating and transporting devices and the skills by which we produce and use them.\" Bain's definition remains common among scholars today, especially social scientists. But equally prominent is the definition of technology as applied science, especially among scientists and engineers, although most social scientists who study technology reject this definition. More recently, scholars have borrowed from European philosophers of \"technique\" to extend the meaning of technology to various forms of instrumental reason, as in Foucault's work on technologies of the self (techniques de soi). Dictionaries and scholars have offered a variety of definitions. The Merriam-Webster Dictionary offers a definition of the term: \"the practical application of knowledge especially in a particular area\" and \"a capability given by the practical application of knowledge\". Ursula Franklin, in her 1989 \"Real World of Technology\" lecture, gave another definition of the concept; it is \"practice, the way we do things around here\". The term is often used to imply a specific field of technology, or to refer to high technology or just consumer electronics, rather than technology as a whole. Bernard Stiegler, in Technics and Time, 1, defines technology in two ways: as \"the pursuit of life by means other than life\", and as \"organized inorganic matter.\" Technology can be most broadly defined as the entities, both material and immaterial, created by the application of mental and physical effort in order to achieve some value. In this usage, technology refers to tools and machines that may be used to solve real-world problems. It is a far-reaching term that may include simple tools, such as a crowbar or wooden spoon, or more complex machines, such as a space station or particle accelerator. Tools and machines need not be material; virtual technology, such as computer software and business methods, fall under this definition of technology. W. Brian Arthur defines technology in a similarly broad way as \"a means to fulfill a human purpose\". The word \"technology\" can also be used to refer to a collection of techniques. In this context, it is the current state of humanity's knowledge of how to combine resources to produce desired products, to solve problems, fulfill needs, or satisfy wants; it includes technical methods, skills, processes, techniques, tools and raw materials. When combined with another term, such as \"medical technology\" or \"space technology\", it refers to the state of the respective field's knowledge and tools. \"State-of-the-art technology\" refers to the high technology available to humanity in any field.  Technology can be viewed as an activity that forms or changes culture. Additionally, technology is the application of math, science, and the arts for the benefit of life as it is known. A modern example is the rise of communication technology, which has lessened barriers to human interaction and, as a result, has helped spawn new subcultures; the rise of cyberculture has, at its basis, the development of the Internet and the computer. Not all technology enhances culture in a creative way; technology can also help facilitate political oppression and war via tools such as guns. As a cultural activity, technology predates both science and engineering, each of which formalize some aspects of technological endeavor.    Science, engineering and technology   The distinction between science, engineering and technology is not always clear. Science is the reasoned investigation or study of natural phenomena, aimed at discovering enduring principles among elements of the phenomenal world by employing formal techniques such as the scientific method. Technologies are not usually exclusively products of science, because they have to satisfy requirements such as utility, usability and safety. Engineering is the goal-oriented process of designing and making tools and systems to exploit natural phenomena for practical human means, often (but not always) using results and techniques from science. The development of technology may draw upon many fields of knowledge, including scientific, engineering, mathematical, linguistic, and historical knowledge, to achieve some practical result. Technology is often a consequence of science and engineering — although technology as a human activity precedes the two fields. For example, science might study the flow of electrons in electrical conductors, by using already-existing tools and knowledge. This new-found knowledge may then be used by engineers to create new tools and machines, such as semiconductors, computers, and other forms of advanced technology. In this sense, scientists and engineers may both be considered technologists; the three fields are often considered as one for the purposes of research and reference. The exact relations between science and technology in particular have been debated by scientists, historians, and policymakers in the late 20th century, in part because the debate can inform the funding of basic and applied science. In the immediate wake of World War II, for example, in the United States it was widely considered that technology was simply \"applied science\" and that to fund basic science was to reap technological results in due time. An articulation of this philosophy could be found explicitly in Vannevar Bush's treatise on postwar science policy, Science—The Endless Frontier: \"New products, new industries, and more jobs require continuous additions to knowledge of the laws of nature ... This essential new knowledge can be obtained only through basic scientific research.\" In the late-1960s, however, this view came under direct attack, leading towards initiatives to fund science for specific tasks (initiatives resisted by the scientific community). The issue remains contentious—though most analysts resist the model that technology simply is a result of scientific research.    History     Paleolithic (2.5 million YA – 10,000 BC)   The use of tools by early humans was partly a process of discovery and of evolution. Early humans evolved from a species of foraging hominids which were already bipedal, with a brain mass approximately one third of modern humans. Tool use remained relatively unchanged for most of early human history. Approximately 50,000 years ago, the use of tools and complex set of behaviors emerged, believed by many archaeologists to be connected to the emergence of fully modern language.   = Stone tools =  Hominids started using primitive stone tools millions of years ago. The earliest stone tools were little more than a fractured rock, but approximately 40,000 years ago, pressure flaking provided a way to make much finer work.   = Fire =  The discovery and utilization of fire, a simple energy source with many profound uses, was a turning point in the technological evolution of humankind. The exact date of its discovery is not known; evidence of burnt animal bones at the Cradle of Humankind suggests that the domestication of fire occurred before 1,000,000 BC; scholarly consensus indicates that Homo erectus had controlled fire by between 500,000 BC and 400,000 BC. Fire, fueled with wood and charcoal, allowed early humans to cook their food to increase its digestibility, improving its nutrient value and broadening the number of foods that could be eaten.   = Clothing and shelter = Other technological advances made during the Paleolithic era were clothing and shelter; the adoption of both technologies cannot be dated exactly, but they were a key to humanity's progress. As the Paleolithic era progressed, dwellings became more sophisticated and more elaborate; as early as 380,000 BC, humans were constructing temporary wood huts. Clothing, adapted from the fur and hides of hunted animals, helped humanity expand into colder regions; humans began to migrate out of Africa by 200,000 BC and into other continents, such as Eurasia.    Neolithic through classical antiquity (10,000 BC – 300 AD)   Man's technological ascent began in earnest in what is known as the Neolithic period (\"New stone age\"). The invention of polished stone axes was a major advance that allowed forest clearance on a large scale to create farms. Agriculture fed larger populations, and the transition to sedentism allowed simultaneously raising more children, as infants no longer needed to be carried, as nomadic ones must. Additionally, children could contribute labor to the raising of crops more readily than they could to the hunter-gatherer economy. With this increase in population and availability of labor came an increase in labor specialization. What triggered the progression from early Neolithic villages to the first cities, such as Uruk, and the first civilizations, such as Sumer, is not specifically known; however, the emergence of increasingly hierarchical social structures and specialized labor, of trade and war amongst adjacent cultures, and the need for collective action to overcome environmental challenges such as irrigation, are all thought to have played a role.   = Metal tools = Continuing improvements led to the furnace and bellows and provided the ability to smelt and forge native metals (naturally occurring in relatively pure form). Gold, copper, silver, and lead, were such early metals. The advantages of copper tools over stone, bone, and wooden tools were quickly apparent to early humans, and native copper was probably used from near the beginning of Neolithic times (about 8000 BC). Native copper does not naturally occur in large amounts, but copper ores are quite common and some of them produce metal easily when burned in wood or charcoal fires. Eventually, the working of metals led to the discovery of alloys such as bronze and brass (about 4000 BC). The first uses of iron alloys such as steel dates to around 1400 BC.   = Energy and transport =  Meanwhile, humans were learning to harness other forms of energy. The earliest known use of wind power is the sailboat. The earliest record of a ship under sail is shown on an Egyptian pot dating back to 3200 BC. From prehistoric times, Egyptians probably used the power of the annual flooding of the Nile to irrigate their lands, gradually learning to regulate much of it through purposely built irrigation channels and 'catch' basins. Similarly, the early peoples of Mesopotamia, the Sumerians, learned to use the Tigris and Euphrates rivers for much the same purposes. But more extensive use of wind and water (and even human) power required another invention. According to archaeologists, the wheel was invented around 4000 B.C. probably independently and nearly simultaneously in Mesopotamia (in present-day Iraq), the Northern Caucasus (Maykop culture) and Central Europe. Estimates on when this may have occurred range from 5500 to 3000 B.C., with most experts putting it closer to 4000 B.C. The oldest artifacts with drawings that depict wheeled carts date from about 3000 B.C.; however, the wheel may have been in use for millennia before these drawings were made. There is also evidence from the same period for the use of the potter's wheel. More recently, the oldest-known wooden wheel in the world was found in the Ljubljana marshes of Slovenia. The invention of the wheel revolutionized trade and war. It did not take long to discover that wheeled wagons could be used to carry heavy loads. Fast (rotary) potters' wheels enabled early mass production of pottery. But it was the use of the wheel as a transformer of energy (through water wheels, windmills, and even treadmills) that revolutionized the application of nonhuman power sources.    Medieval and modern history (300 AD – present)   Innovations continued through the Middle Ages with innovations such as silk, the horse collar and horseshoes in the first few hundred years after the fall of the Roman Empire. Medieval technology saw the use of simple machines (such as the lever, the screw, and the pulley) being combined to form more complicated tools, such as the wheelbarrow, windmills and clocks. The Renaissance brought forth many of these innovations, including the printing press (which facilitated the greater communication of knowledge), and technology became increasingly associated with science, beginning a cycle of mutual advancement. The advancements in technology in this era allowed a more steady supply of food, followed by the wider availability of consumer goods.  Starting in the United Kingdom in the 18th century, the Industrial Revolution was a period of great technological discovery, particularly in the areas of agriculture, manufacturing, mining, metallurgy and transport, driven by the discovery of steam power. Technology later took another step with the harnessing of electricity to create such innovations as the electric motor, light bulb and countless others. Scientific advancement and the discovery of new concepts later allowed for powered flight, and advancements in medicine, chemistry, physics and engineering. The rise in technology has led to the construction of skyscrapers and large cities whose inhabitants rely on automobiles or other powered transit for transportation. Communication was also greatly improved with the invention of the telegraph, telephone, radio and television. The late 19th and early 20th centuries saw a revolution in transportation with the invention of the steam-powered ship, train, airplane, and automobile.  The 20th century brought a host of innovations. In physics, the discovery of nuclear fission has led to both nuclear weapons and nuclear power. Computers were also invented and later miniaturized utilizing transistors and integrated circuits. The technology behind got called information technology, and these advancements subsequently led to the creation of the Internet, which ushered in the current Information Age. Humans have also been able to explore space with satellites (later used for telecommunication) and in manned missions going all the way to the moon. In medicine, this era brought innovations such as open-heart surgery and later stem cell therapy along with new medications and treatments. Complex manufacturing and construction techniques and organizations are needed to construct and maintain these new technologies, and entire industries have arisen to support and develop succeeding generations of increasingly more complex tools. Modern technology increasingly relies on training and education — their designers, builders, maintainers, and users often require sophisticated general and specific training. Moreover, these technologies have become so complex that entire fields have been created to support them, including engineering, medicine, and computer science, and other fields have been made more complex, such as construction, transportation and architecture.    Philosophy     Technicism  Generally, technicism is a reliance or confidence in technology as a benefactor of society. Taken to extreme, technicism is the belief that humanity will ultimately be able to control the entirety of existence using technology. In other words, human beings will someday be able to master all problems and possibly even control the future using technology. Some, such as Stephen V. Monsma, connect these ideas to the abdication of religion as a higher moral authority.    Optimism   Optimistic assumptions are made by proponents of ideologies such as transhumanism and singularitarianism, which view technological development as generally having beneficial effects for the society and the human condition. In these ideologies, technological development is morally good. Some critics see these ideologies as examples of scientism and techno-utopianism and fear the notion of human enhancement and technological singularity which they support. Some have described Karl Marx as a techno-optimist.    Skepticism and critics   On the somewhat skeptical side are certain philosophers like Herbert Marcuse and John Zerzan, who believe that technological societies are inherently flawed. They suggest that the inevitable result of such a society is to become evermore technological at the cost of freedom and psychological health. Many, such as the Luddites and prominent philosopher Martin Heidegger, hold serious, although not entirely deterministic reservations, about technology (see \"The Question Concerning Technology\"). According to Heidegger scholars Hubert Dreyfus and Charles Spinosa, \"Heidegger does not oppose technology. He hopes to reveal the essence of technology in a way that 'in no way confines us to a stultified compulsion to push on blindly with technology or, what comes to the same thing, to rebel helplessly against it.' Indeed, he promises that 'when we once open ourselves expressly to the essence of technology, we find ourselves unexpectedly taken into a freeing claim.'\" What this entails is a more complex relationship to technology than either techno-optimists or techno-pessimists tend to allow. Some of the most poignant criticisms of technology are found in what are now considered to be dystopian literary classics, for example Aldous Huxley's Brave New World and other writings, Anthony Burgess's A Clockwork Orange, and George Orwell's Nineteen Eighty-Four. And, in Faust by Goethe, Faust's selling his soul to the devil in return for power over the physical world, is also often interpreted as a metaphor for the adoption of industrial technology. More recently, modern works of science fiction, such as those by Philip K. Dick and William Gibson, and films (e.g. Blade Runner, Ghost in the Shell) project highly ambivalent or cautionary attitudes toward technology's impact on human society and identity. The late cultural critic Neil Postman distinguished tool-using societies from technological societies and, finally, what he called \"technopolies,\" that is, societies that are dominated by the ideology of technological and scientific progress, to the exclusion or harm of other cultural practices, values and world-views. Darin Barney has written about technology's impact on practices of citizenship and democratic culture, suggesting that technology can be construed as (1) an object of political debate, (2) a means or medium of discussion, and (3) a setting for democratic deliberation and citizenship. As a setting for democratic culture, Barney suggests that technology tends to make ethical questions, including the question of what a good life consists in, nearly impossible, because they already give an answer to the question: a good life is one that includes the use of more and more technology. Nikolas Kompridis has also written about the dangers of new technology, such as genetic engineering, nanotechnology, synthetic biology and robotics. He warns that these technologies introduce unprecedented new challenges to human beings, including the possibility of the permanent alteration of our biological nature. These concerns are shared by other philosophers, scientists and public intellectuals who have written about similar issues (e.g. Francis Fukuyama, Jürgen Habermas, William Joy, and Michael Sandel). Another prominent critic of technology is Hubert Dreyfus, who has published books On the Internet and What Computers Still Can't Do. Another, more infamous anti-technological treatise is Industrial Society and Its Future, written by Theodore Kaczynski (aka The Unabomber) and printed in several major newspapers (and later books) as part of an effort to end his bombing campaign of the techno-industrial infrastructure.    Appropriate technology   The notion of appropriate technology, however, was developed in the 20th century (e.g., see the work of E. F. Schumacher and of Jacques Ellul) to describe situations where it was not desirable to use very new technologies or those that required access to some centralized infrastructure or parts or skills imported from elsewhere. The eco-village movement emerged in part due to this concern.    Optimism and Skepticism in the 21st century  This article mainly focusses on American concerns even if it can reasonably be generalized to other Western countries.  In his article, Jared Bernstein, a Senior Fellow at the Center on Budget and Policy Priorities, questions the widespread idea that automation, and more broadly technological advances have mainly contributed to this growing labor market problem. His thesis appears to be a third way between Optimism and Skepticism. Basically, he stands for a neutral approach of the linkage between technology and American issues concerning unemployment and eroding wages. He uses two main arguments to defend his point. First of all, because of recent technological advances, an increasing number of workers are losing their jobs. Yet, scientific evidence fails to clearly demonstrate that technology has displaced so many workers that it has created more problems than it has solved. Indeed, automation threatens repetitive jobs but higher-end jobs are still necessary because they complement technology and manual jobs that \"requires flexibility judgment and common sense\" remain hard to be replaced by machines.Second, studies have not defined clear links between recent technology advances and the wage trends of the last decades. Therefore, according to Jared Bernstein, instead of focusing on technology and its hypothetical influences on current American increasing unemployment and eroding wages, one needs to worry more about \"bad policy that fails to offset the imbalances in demand, trade, income and opportunity.\"    Complex Technological Systems  Thomas P. Hughes pointed out that because technology has been considered as a key way to solve problems, we need to be aware of its complex and varied characters to use it more efficiently. What is the difference between a wheel or a compass and cooking machines such as an oven or a gas stove? Can we consider all of them, only a part of them or none of them as technologies? Technology is often considered too narrowly: according to Thomas P. Hughes \"Technology is a creative process involving human ingenuity”. This definition emphasizing on creativity avoids unbounded definition that may mistakenly include cooking “technologies”. But it also highlights the prominent role of humans and therefore their responsibilities for the use of complex technological systems. Yet, because technology is everywhere and has dramatically changed landscapes and societies, Hughes argued that engineers, scientists, and managers often have believed that they can use technology to shape the world as they want. They have often supposed that technology is easily controllable and this assumption has to be thoroughly questioned. For instance, Evgeny Morozov particularly challenges two concepts: “Internet-centrism” and “solutionism”. Internet-centrism refers to the idea that our society is convinced that the Internet is one of the most stable and coherent forces. Solutionism is the ideology that every social issue can be solved thanks to technology and especially thanks to the internet. In fact, technology intrinsically contains uncertainties and limitations. According to Alexis Madrigal's critique of Morozov's theory, to ignore it will lead to “unexpected consequences that could eventually cause more damage than the problems they seek to address”. Benjamin Cohen and Gwen Ottinger precisely discussed the multivalent effects of technology. Therefore, recognition of the limitations of technology and more broadly scientific knowledge is needed — especially in cases dealing with environmental justice and health issues. Gwen Ottinger continues this reasoning and argues that the ongoing recognition of the limitations of scientific knowledge goes hand in hand with scientists and engineers’ new comprehension of their role. Such an approach of technology and science \"[require] technical professionals to conceive of their roles in the process differently. [They have to consider themselves as] collaborators in research and problem solving rather than simply providers of information and technical solutions\".    Competitiveness   Technology is properly defined as any application of science to accomplish a function. The science can be leading edge or well established and the function can have high visibility or be significantly more mundane but it is all technology, and its exploitation is the foundation of all competitive advantage. Technology-based planning is what was used to build the US industrial giants before WWII (e.g., Dow, DuPont, GM) and it what was used to transform the US into a superpower. It was not economic-based planning.    Project Socrates  In 1983 Project Socrates was initiated in the US intelligence community to determine the source of declining US economic and military competitiveness. Project Socrates concluded that technology exploitation is the foundation of all competitive advantage and that declining US competitiveness was from decision-making in the private and public sectors switching from technology exploitation (technology-based planning) to money exploitation (economic-based planning) at the end of World War II. Project Socrates determined that to rebuild US competitiveness, decision making throughout the US had to readopt technology-based planning. Project Socrates also determined that countries like China and India had continued executing technology-based (while the US took its detour into economic-based) planning, and as a result had considerably advanced the process and were using it to build themselves into superpowers. To rebuild US competitiveness the US decision-makers needed to adopt a form of technology-based planning that was far more advanced than that used by China and India. Project Socrates determined that technology-based planning makes an evolutionary leap forward every few hundred years and the next evolutionary leap, the Automated Innovation Revolution, was poised to occur. In the Automated Innovation Revolution the process for determining how to acquire and utilize technology for a competitive advantage (which includes R&D) is automated so that it can be executed with unprecedented speed, efficiency and agility. Project Socrates developed the means for automated innovation so that the US could lead the Automated Innovation Revolution in order to rebuild and maintain the country's economic competitiveness for many generations.    Other animal species   The use of basic technology is also a feature of other animal species apart from humans. These include primates such as chimpanzees, some dolphin communities, and crows. Considering a more generic perspective of technology as ethology of active environmental conditioning and control, we can also refer to animal examples such as beavers and their dams, or bees and their honeycombs. The ability to make and use tools was once considered a defining characteristic of the genus Homo. However, the discovery of tool construction among chimpanzees and related primates has discarded the notion of the use of technology as unique to humans. For example, researchers have observed wild chimpanzees utilising tools for foraging: some of the tools used include leaf sponges, termite fishing probes, pestles and levers. West African chimpanzees also use stone hammers and anvils for cracking nuts, as do capuchin monkeys of Boa Vista, Brazil.    Future technology   Theories of technology often attempt to predict the future of technology based on the high technology and science of the time.    See also     References     Further reading  Ambrose, Stanley H. (2 March 2001). \"Paleolithic Technology and Human Evolution\" (PDF). Science (Science) 291 (5509): 1748–53. Bibcode:2001Sci...291.1748A. doi:10.1126/science.1059487. PMID 11249821. Retrieved 10 March 2007.  Huesemann, M.H., and J.A. Huesemann (2011). Technofix: Why Technology Won’t Save Us or the Environment, New Society Publishers, ISBN 0865717044. Kremer, Michael (1993). \"Population Growth and Technological Change: One Million B.C. to 1990\". Quarterly Journal of Economics (The MIT Press) 108 (3): 681–716. doi:10.2307/2118405. JSTOR 2118405. . Kevin Kelly. What Technology Wants. New York, Viking Press, 14 October 2010, hardcover, 416 pages. ISBN 978-0-670-02215-1 Mumford, Lewis. (2010). Technics and Civilization. University of Chicago Press, ISBN 0226550273. Rhodes, Richard. (2000). Visions of Technology: A Century of Vital Debate about Machines, Systems, and the Human World. Simon & Schuster, ISBN 0684863111. Teich, A.H. (2008). Technology and the Future. Wadsworth Publishing, 11th edition, ISBN 0495570524. Wright, R.T. (2008). Technology. Goodheart-Wilcox Company, 5th edition, ISBN 1590707184.","label":"foo"},{"text":"Electromagnetism is a branch of physics which involves the study of the electromagnetic force, a type of physical interaction that occurs between electrically charged particles. The electromagnetic force usually shows electromagnetic fields, such as electric fields, magnetic fields, and light. The electromagnetic force is one of the four fundamental interactions in nature. The other three fundamental interactions are the strong interaction, the weak interaction, and gravitation.  The word electromagnetism is a compound form of two Greek terms, ἤλεκτρον, ēlektron, \"amber\", and μαγνῆτις λίθος magnētis lithos, which means \"magnesian stone\", a type of iron ore. The science of electromagnetic phenomena is defined in terms of the electromagnetic force, sometimes called the Lorentz force, which includes both electricity and magnetism as elements of one phenomenon. The electromagnetic force plays a major role in determining the internal properties of most objects encountered in daily life. Ordinary matter takes its form as a result of intermolecular forces between individual molecules in matter. Electrons are bound by electromagnetic wave mechanics into orbitals around atomic nuclei to form atoms, which are the building blocks of molecules. This governs the processes involved in chemistry, which arise from interactions between the electrons of neighboring atoms, which are in turn determined by the interaction between electromagnetic force and the momentum of the electrons. There are numerous mathematical descriptions of the electromagnetic field. In classical electrodynamics, electric fields are described as electric potential and electric current in Ohm's law, magnetic fields are associated with electromagnetic induction and magnetism, and Maxwell's equations describe how electric and magnetic fields are generated and altered by each other and by charges and currents. The theoretical implications of electromagnetism, in particular the establishment of the speed of light based on properties of the \"medium\" of propagation (permeability and permittivity), led to the development of special relativity by Albert Einstein in 1905. Although electromagnetism is considered one of the four fundamental forces, at high energy the weak force and electromagnetism are unified. In the history of the universe, during the quark epoch, the electroweak force split into the electromagnetic and weak forces.    History of the theoryEdit   Originally electricity and magnetism were thought of as two separate forces. This view changed, however, with the publication of James Clerk Maxwell's 1873 A Treatise on Electricity and Magnetism in which the interactions of positive and negative charges were shown to be regulated by one force. There are four main effects resulting from these interactions, all of which have been clearly demonstrated by experiments: Electric charges attract or repel one another with a force inversely proportional to the square of the distance between them: unlike charges attract, like ones repel. Magnetic poles (or states of polarization at individual points) attract or repel one another in a similar way and always come in pairs: every north pole is yoked to a south pole. An electric current inside a wire creates a corresponding circular magnetic field outside the wire. Its direction (clockwise or counter-clockwise) depends on the direction of the current in the wire. A current is induced in a loop of wire when it is moved towards or away from a magnetic field, or a magnet is moved towards or away from it, the direction of current depending on that of the movement.  While preparing for an evening lecture on 21 April 1820, Hans Christian Ørsted made a surprising observation. As he was setting up his materials, he noticed a compass needle deflected away from magnetic north when the electric current from the battery he was using was switched on and off. This deflection convinced him that magnetic fields radiate from all sides of a wire carrying an electric current, just as light and heat do, and that it confirmed a direct relationship between electricity and magnetism.  At the time of discovery, Ørsted did not suggest any satisfactory explanation of the phenomenon, nor did he try to represent the phenomenon in a mathematical framework. However, three months later he began more intensive investigations. Soon thereafter he published his findings, proving that an electric current produces a magnetic field as it flows through a wire. The CGS unit of magnetic induction (oersted) is named in honor of his contributions to the field of electromagnetism.  His findings resulted in intensive research throughout the scientific community in electrodynamics. They influenced French physicist André-Marie Ampère's developments of a single mathematical form to represent the magnetic forces between current-carrying conductors. Ørsted's discovery also represented a major step toward a unified concept of energy. This unification, which was observed by Michael Faraday, extended by James Clerk Maxwell, and partially reformulated by Oliver Heaviside and Heinrich Hertz, is one of the key accomplishments of 19th century mathematical physics. It had far-reaching consequences, one of which was the understanding of the nature of light. Unlike what was proposed in Electromagnetism, light and other electromagnetic waves are at the present seen as taking the form of quantized, self-propagating oscillatory electromagnetic field disturbances which have been called photons. Different frequencies of oscillation give rise to the different forms of electromagnetic radiation, from radio waves at the lowest frequencies, to visible light at intermediate frequencies, to gamma rays at the highest frequencies. Ørsted was not the only person to examine the relationship between electricity and magnetism. In 1802, Gian Domenico Romagnosi, an Italian legal scholar, deflected a magnetic needle using electrostatic charges. Actually, no galvanic current existed in the setup and hence no electromagnetism was present. An account of the discovery was published in 1802 in an Italian newspaper, but it was largely overlooked by the contemporary scientific community.    Fundamental forcesEdit   The electromagnetic force is one of the four known fundamental forces. The other fundamental forces are: the weak nuclear force, which binds to all known particles in the Standard Model, and causes certain forms of radioactive decay. (In particle physics though, the electroweak interaction is the unified description of two of the four known fundamental interactions of nature: electromagnetism and the weak interaction); the strong nuclear force, which binds quarks to form nucleons, and binds nucleons to form nuclei the gravitational force. All other forces (e.g., friction) are derived from these four fundamental forces (including momentum which is carried by the movement of particles). The electromagnetic force is the one responsible for practically all the phenomena one encounters in daily life above the nuclear scale, with the exception of gravity. Roughly speaking, all the forces involved in interactions between atoms can be explained by the electromagnetic force acting on the electrically charged atomic nuclei and electrons inside and around the atoms, together with how these particles carry momentum by their movement. This includes the forces we experience in \"pushing\" or \"pulling\" ordinary material objects, which come from the intermolecular forces between the individual molecules in our bodies and those in the objects. It also includes all forms of chemical phenomena. A necessary part of understanding the intra-atomic to intermolecular forces is the effective force generated by the momentum of the electrons' movement, and that electrons move between interacting atoms, carrying momentum with them. As a collection of electrons becomes more confined, their minimum momentum necessarily increases due to the Pauli exclusion principle. The behaviour of matter at the molecular scale including its density is determined by the balance between the electromagnetic force and the force generated by the exchange of momentum carried by the electrons themselves.    Classical electrodynamicsEdit   The scientist William Gilbert proposed, in his De Magnete (1600), that electricity and magnetism, while both capable of causing attraction and repulsion of objects, were distinct effects. Mariners had noticed that lightning strikes had the ability to disturb a compass needle, but the link between lightning and electricity was not confirmed until Benjamin Franklin's proposed experiments in 1752. One of the first to discover and publish a link between man-made electric current and magnetism was Romagnosi, who in 1802 noticed that connecting a wire across a voltaic pile deflected a nearby compass needle. However, the effect did not become widely known until 1820, when Ørsted performed a similar experiment. Ørsted's work influenced Ampère to produce a theory of electromagnetism that set the subject on a mathematical foundation. A theory of electromagnetism, known as classical electromagnetism, was developed by various physicists over the course of the 19th century, culminating in the work of James Clerk Maxwell, who unified the preceding developments into a single theory and discovered the electromagnetic nature of light. In classical electromagnetism, the electromagnetic field obeys a set of equations known as Maxwell's equations, and the electromagnetic force is given by the Lorentz force law. One of the peculiarities of classical electromagnetism is that it is difficult to reconcile with classical mechanics, but it is compatible with special relativity. According to Maxwell's equations, the speed of light in a vacuum is a universal constant, dependent only on the electrical permittivity and magnetic permeability of free space. This violates Galilean invariance, a long-standing cornerstone of classical mechanics. One way to reconcile the two theories (electromagnetism and classical mechanics) is to assume the existence of a luminiferous aether through which the light propagates. However, subsequent experimental efforts failed to detect the presence of the aether. After important contributions of Hendrik Lorentz and Henri Poincaré, in 1905, Albert Einstein solved the problem with the introduction of special relativity, which replaces classical kinematics with a new theory of kinematics that is compatible with classical electromagnetism. (For more information, see History of special relativity.) In addition, relativity theory shows that in moving frames of reference a magnetic field transforms to a field with a nonzero electric component and vice versa; thus firmly showing that they are two sides of the same coin, and thus the term \"electromagnetism\". (For more information, see Classical electromagnetism and special relativity and Covariant formulation of classical electromagnetism.    Quantum mechanicsEdit     Photoelectric effectEdit   In another paper published in 1905, Albert Einstein undermined the very foundations of classical electromagnetism. In his theory of the photoelectric effect (for which he won the Nobel prize in physics) and inspired by the idea of Max Planck's \"quanta\", he posited that light could exist in discrete particle-like quantities as well, which later came to be known as photons. Einstein's theory of the photoelectric effect extended the insights that appeared in the solution of the ultraviolet catastrophe presented by Max Planck in 1900. In his work, Planck showed that hot objects emit electromagnetic radiation in discrete packets (\"quanta\"), which leads to a finite total energy emitted as black body radiation. Both of these results were in direct contradiction with the classical view of light as a continuous wave. Planck's and Einstein's theories were progenitors of quantum mechanics, which, when formulated in 1925, necessitated the invention of a quantum theory of electromagnetism. This theory, completed in the 1940s-1950s, is known as quantum electrodynamics (or \"QED\"), and, in situations where perturbation theory is applicable, is one of the most accurate theories known to physics.    Quantum electrodynamicsEdit   All electromagnetic phenomena are underpinned by quantum mechanics, specifically by quantum electrodynamics (which includes classical electrodynamics as a limiting case) and this accounts for almost all physical phenomena observable to the unaided human senses, including light and other electromagnetic radiation, all of chemistry, most of mechanics (excepting gravitation), and, of course, magnetism and electricity.    Electroweak interactionEdit   The electroweak interaction is the unified description of two of the four known fundamental interactions of nature: electromagnetism and the weak interaction. Although these two forces appear very different at everyday low energies, the theory models them as two different aspects of the same force. Above the unification energy, on the order of 100 GeV, they would merge into a single electroweak force. Thus if the universe is hot enough (approximately 1015 K, a temperature exceeded until shortly after the Big Bang) then the electromagnetic force and weak force merge into a combined electroweak force. During the electroweak epoch, the electroweak force separated from the strong force. During the quark epoch, the electroweak force split into the electromagnetic and weak force.    Quantities and unitsEdit   Electromagnetic units are part of a system of electrical units based primarily upon the magnetic properties of electric currents, the fundamental SI unit being the ampere. The units are:  In the electromagnetic cgs system, electric current is a fundamental quantity defined via Ampère's law and takes the permeability as a dimensionless quantity (relative permeability) whose value in a vacuum is unity. As a consequence, the square of the speed of light appears explicitly in some of the equations interrelating quantities in this system. Formulas for physical laws of electromagnetism (such as Maxwell's equations) need to be adjusted depending on what system of units one uses. This is because there is no one-to-one correspondence between electromagnetic units in SI and those in CGS, as is the case for mechanical units. Furthermore, within CGS, there are several plausible choices of electromagnetic units, leading to different unit \"sub-systems\", including Gaussian, \"ESU\", \"EMU\", and Heaviside–Lorentz. Among these choices, Gaussian units are the most common today, and in fact the phrase \"CGS units\" is often used to refer specifically to CGS-Gaussian units.    See alsoEdit     ReferencesEdit     Further readingEdit     Web sourcesEdit     Lecture notesEdit     TextbooksEdit     General referencesEdit     External linksEdit  Oppelt, Arnulf (2006-11-02). \"magnetic field strength\". Retrieved 2007-06-04.  \"magnetic field strength converter\". Retrieved 2007-06-04.  Electromagnetic Force - from Eric Weisstein's World of Physics Goudarzi, Sara (2006-08-15). \"Ties That Bind Atoms Weaker Than Thought\". LiveScience.com. Retrieved 2013-11-12.  Quarked Electromagnetic force - A good introduction for kids The Deflection of a Magnetic Compass Needle by a Current in a Wire (video) on YouTube Electromagnetism abridged","label":"foo"},{"text":"Nuclear physics is the field of physics that studies the constituents and interactions of atomic nuclei. The most commonly known applications of nuclear physics are nuclear power generation but the research has provided application in many fields, including those in nuclear medicine and magnetic resonance imaging, nuclear weapons, ion implantation in materials engineering, and radiocarbon dating in geology and archaeology. The field of particle physics evolved out of nuclear physics and is typically taught in close association with nuclear physics.    History  The history of nuclear physics as a discipline distinct from atomic physics starts with the discovery of radioactivity by Henri Becquerel in 1896, while investigating phosphorescence in uranium salts. The discovery of the electron by J. J. Thomson a year later was an indication that the atom had internal structure. At the beginning of the 20th century the accepted model of the atom was J. J. Thomson's plum pudding model in which the atom was a large positively charged ball with small negatively charged electrons embedded inside of it. In the years that followed, the phenomenon of radioactivity was extensively investigated, notably by the husband and wife team of Pierre Curie and Marie Curie and by Ernest Rutherford and his collaborators. By the turn of the century physicists had also discovered three types of radiation emanating from atoms, which they named alpha, beta, and gamma radiation. Experiments in 1911 by Otto Hahn, and by James Chadwick in 1914 discovered that the beta decay spectrum was continuous rather than discrete. That is, electrons were ejected from the atom with a range of energies, rather than the discrete amounts of energies that were observed in gamma and alpha decays. This was a problem for nuclear physics at the time, because it seemed to indicate that energy was not conserved in these decays. The 1903 Nobel Prize in Physics was awarded jointly to Becquerel for his discovery and to Pierre Curie and Marie Curie for their subsequent research into radioactivity. Rutherford was awarded the Nobel Prize in Chemistry in 1908 for his ‘investigations into the disintegration of the elements and the chemistry of radioactive substances’. In 1905, Albert Einstein formulated the idea of mass–energy equivalence. While the work on radioactivity by Becquerel and Marie Curie predates this, an explanation of the source of the energy of radioactivity would have to wait for the discovery that the nucleus itself was composed of smaller constituents, the nucleons.    Rutherford's team discovers the nucleus  In 1907 Ernest Rutherford published \"Radiation of the α Particle from Radium in passing through Matter.\" Hans Geiger expanded on this work in a communication to the Royal Society with experiments he and Rutherford had done, passing α particles through air, aluminum foil and gold leaf. More work was published in 1909 by Geiger and Marsden and further greatly expanded work was published in 1910 by Geiger. In 1911-1912 Rutherford went before the Royal Society to explain the experiments and propound the new theory of the atomic nucleus as we now understand it. The key experiment behind this announcement happened in 1910 at the University of Manchester, as Ernest Rutherford's team performed a remarkable experiment in which Hans Geiger and Ernest Marsden under his supervision fired alpha particles (helium nuclei) at a thin film of gold foil. The plum pudding model predicted that the alpha particles should come out of the foil with their trajectories being at most slightly bent. Rutherford had the idea to instruct his team to look for something that shocked him to actually observe: a few particles were scattered through large angles, even completely backwards, in some cases. He likened it to firing a bullet at tissue paper and having it bounce off. The discovery, beginning with Rutherford's analysis of the data in 1911, eventually led to the Rutherford model of the atom, in which the atom has a very small, very dense nucleus containing most of its mass, and consisting of heavy positively charged particles with embedded electrons in order to balance out the charge (since the neutron was unknown). As an example, in this model (which is not the modern one) nitrogen-14 consisted of a nucleus with 14 protons and 7 electrons (21 total particles), and the nucleus was surrounded by 7 more orbiting electrons. The Rutherford model worked quite well until studies of nuclear spin were carried out by Franco Rasetti at the California Institute of Technology in 1929. By 1925 it was known that protons and electrons had a spin of 1/2, and in the Rutherford model of nitrogen-14, 20 of the total 21 nuclear particles should have paired up to cancel each other's spin, and the final odd particle should have left the nucleus with a net spin of 1/2. Rasetti discovered, however, that nitrogen-14 had a spin of 1.    James Chadwick discovers the neutron  In 1932 Chadwick realized that radiation that had been observed by Walther Bothe, Herbert Becker, Irène and Frédéric Joliot-Curie was actually due to a neutral particle of about the same mass as the proton, that he called the neutron (following a suggestion about the need for such a particle, by Rutherford). In the same year Dmitri Ivanenko suggested that neutrons were in fact spin 1/2 particles and that the nucleus contained neutrons to explain the mass not due to protons, and that there were no electrons in the nucleus — only protons and neutrons. The neutron spin immediately solved the problem of the spin of nitrogen-14, as the one unpaired proton and one unpaired neutron in this model, each contribute a spin of 1/2 in the same direction, for a final total spin of 1. With the discovery of the neutron, scientists at last could calculate what fraction of binding energy each nucleus had, from comparing the nuclear mass with that of the protons and neutrons which composed it. Differences between nuclear masses were calculated in this way and—when nuclear reactions were measured—were found to agree with Einstein's calculation of the equivalence of mass and energy to high accuracy (within 1 percent as of in 1934).    Proca's equations of the massive vector boson field  Alexandru Proca was the first to develop and report the massive vector boson field equations and a theory of the mesonic field of nuclear forces. Proca's equations were known to Wolfgang Pauli who mentioned the equations in his Nobel address, and they were also known to Yukawa, Wentzel, Taketani, Sakata, Kemmer, Heitler, and Fröhlich who appreciated the content of Proca's equations for developing a theory of the atomic nuclei in Nuclear Physics.    Yukawa's meson postulated to bind nuclei  In 1935 Hideki Yukawa  proposed the first significant theory of the strong force to explain how the nucleus holds together. In the Yukawa interaction a virtual particle, later called a meson, mediated a force between all nucleons, including protons and neutrons. This force explained why nuclei did not disintegrate under the influence of proton repulsion, and it also gave an explanation of why the attractive strong force had a more limited range than the electromagnetic repulsion between protons. Later, the discovery of the pi meson showed it to have the properties of Yukawa's particle. With Yukawa's papers, the modern model of the atom was complete. The center of the atom contains a tight ball of neutrons and protons, which is held together by the strong nuclear force, unless it is too large. Unstable nuclei may undergo alpha decay, in which they emit an energetic helium nucleus, or beta decay, in which they eject an electron (or positron). After one of these decays the resultant nucleus may be left in an excited state, and in this case it decays to its ground state by emitting high energy photons (gamma decay). The study of the strong and weak nuclear forces (the latter explained by Enrico Fermi via Fermi's interaction in 1934) led physicists to collide nuclei and electrons at ever higher energies. This research became the science of particle physics, the crown jewel of which is the standard model of particle physics which describes the strong, weak, and electromagnetic forces.    Modern nuclear physics   A heavy nucleus can contain hundreds of nucleons which means that with some approximation it can be treated as a classical system, rather than a quantum-mechanical one. In the resulting liquid-drop model, the nucleus has an energy which arises partly from surface tension and partly from electrical repulsion of the protons. The liquid-drop model is able to reproduce many features of nuclei, including the general trend of binding energy with respect to mass number, as well as the phenomenon of nuclear fission. Superimposed on this classical picture, however, are quantum-mechanical effects, which can be described using the nuclear shell model, developed in large part by Maria Goeppert Mayer and J. Hans D. Jensen. Nuclei with certain numbers of neutrons and protons (the magic numbers 2, 8, 20, 28, 50, 82, 126, ...) are particularly stable, because their shells are filled. Other more complicated models for the nucleus have also been proposed, such as the interacting boson model, in which pairs of neutrons and protons interact as bosons, analogously to Cooper pairs of electrons. Much of current research in nuclear physics relates to the study of nuclei under extreme conditions such as high spin and excitation energy. Nuclei may also have extreme shapes (similar to that of Rugby balls or even pears) or extreme neutron-to-proton ratios. Experimenters can create such nuclei using artificially induced fusion or nucleon transfer reactions, employing ion beams from an accelerator. Beams with even higher energies can be used to create nuclei at very high temperatures, and there are signs that these experiments have produced a phase transition from normal nuclear matter to a new state, the quark–gluon plasma, in which the quarks mingle with one another, rather than being segregated in triplets as they are in neutrons and protons.    Nuclear decay   Eighty elements have at least one stable isotope never observed to decay, amounting to a total of about 254 stable isotopes. However, thousands of isotopes have been characterized as unstable. These radioisotopes decay over time scales ranging from fractions of a second to weeks, years, billions of years, or even trillions of years. The stability of a nucleus is highest when it falls into a certain range or balance of composition of neutrons and protons; too few or too many neutrons may cause it to decay. For example, in beta decay a nitrogen-16 atom (7 protons, 9 neutrons) is converted to an oxygen-16 atom (8 protons, 8 neutrons) within a few seconds of being created. In this decay a neutron in the nitrogen nucleus is converted into a proton and an electron and an antineutrino by the weak interaction. The element is transmuted to another element by acquiring the created proton. In alpha decay (which typically occurs in the heaviest nuclei) the radioactive element decays by emitting a helium nucleus (2 protons and 2 neutrons), giving another element, plus helium-4. In many cases this process continues through several steps of this kind, including other types of decays, until a stable element is formed. In gamma decay, a nucleus decays from an excited state into a lower energy state, by emitting a gamma ray. The element is not changed to another element in the process (no nuclear transmutation is involved). Other more exotic decays are possible (see the main article). For example, in internal conversion decay, the energy from an excited nucleus may be used to eject one of the inner orbital electrons from the atom, in a process which produces high speed electrons, but is not beta decay, and (unlike beta decay) does not transmute one element to another.    Nuclear fusion  In nuclear fusion, two low mass nuclei come into very close contact with each other, so that the strong force fuses them. It requires a large amount of energy to overcome the repulsion between the nuclei for the strong or nuclear forces to produce this effect, therefore nuclear fusion can only take place at very high temperatures or high pressures. Once the process succeeds, a very large amount of energy is released and the combined nucleus assumes a lower energy level. The binding energy per nucleon increases with mass number up until nickel-62. Stars like the Sun are powered by the fusion of four protons into a helium nucleus, two positrons, and two neutrinos. The uncontrolled fusion of hydrogen into helium is known as thermonuclear runaway. A frontier in current research at various institutions, for example the Joint European Torus (JET) and ITER, is the development of an economically viable method of using energy from a controlled fusion reaction. Natural nuclear fusion is the origin of the light and energy produced by the core of all stars including our own sun.    Nuclear fission  Nuclear fission is the reverse process of fusion. For nuclei heavier than nickel-62 the binding energy per nucleon decreases with the mass number. It is therefore possible for energy to be released if a heavy nucleus breaks apart into two lighter ones. The process of alpha decay is in essence a special type of spontaneous nuclear fission. This process produces a highly asymmetrical fission because the four particles which make up the alpha particle are especially tightly bound to each other, making production of this nucleus in fission particularly likely. For certain of the heaviest nuclei which produce neutrons on fission, and which also easily absorb neutrons to initiate fission, a self-igniting type of neutron-initiated fission can be obtained, in a so-called chain reaction. Chain reactions were known in chemistry before physics, and in fact many familiar processes like fires and chemical explosions are chemical chain reactions. The fission or \"nuclear\" chain-reaction, using fission-produced neutrons, is the source of energy for nuclear power plants and fission type nuclear bombs, such as those detonated by the United States in Hiroshima and Nagasaki, Japan, at the end of World War II. Heavy nuclei such as uranium and thorium may also undergo spontaneous fission, but they are much more likely to undergo decay by alpha decay. For a neutron-initiated chain-reaction to occur, there must be a critical mass of the element present in a certain space under certain conditions. The conditions for the smallest critical mass require the conservation of the emitted neutrons and also their slowing or moderation so there is a greater cross-section or probability of them initiating another fission. In two regions of Oklo, Gabon, Africa, natural nuclear fission reactors were active over 1.5 billion years ago. Measurements of natural neutrino emission have demonstrated that around half of the heat emanating from the Earth's core results from radioactive decay. However, it is not known if any of this results from fission chain-reactions.    Production of \"heavy\" elements (atomic number greater than five)   According to the theory, as the Universe cooled after the big bang it eventually became possible for common subatomic particles as we know them (neutrons, protons and electrons) to exist. The most common particles created in the big bang which are still easily observable to us today were protons and electrons (in equal numbers). The protons would eventually form hydrogen atoms. Almost all the neutrons created in the Big Bang were absorbed into helium-4 in the first three minutes after the Big Bang, and this helium accounts for most of the helium in the universe today (see Big Bang nucleosynthesis). Some fraction of elements beyond helium were created in the Big Bang, as the protons and neutrons collided with each other (lithium, beryllium, and perhaps some boron), but all of the \"heavier elements\" (carbon, element number 6, and elements of greater atomic number) that we see today, were created inside of stars during a series of fusion stages, such as the proton-proton chain, the CNO cycle and the triple-alpha process. Progressively heavier elements are created during the evolution of a star. Since the binding energy per nucleon peaks around iron, energy is only released in fusion processes occurring below this point. Since the creation of heavier nuclei by fusion costs energy, nature resorts to the process of neutron capture. Neutrons (due to their lack of charge) are readily absorbed by a nucleus. The heavy elements are created by either a slow neutron capture process (the so-called s process) or by the rapid, or r process. The s process occurs in thermally pulsing stars (called AGB, or asymptotic giant branch stars) and takes hundreds to thousands of years to reach the heaviest elements of lead and bismuth. The r process is thought to occur in supernova explosions because the conditions of high temperature, high neutron flux and ejected matter are present. These stellar conditions make the successive neutron captures very fast, involving very neutron-rich species which then beta-decay to heavier elements, especially at the so-called waiting points that correspond to more stable nuclides with closed neutron shells (magic numbers).    See also   Isomeric shift Neutron-degenerate matter Nuclear matter Nuclear model Nuclear reactor physics QCD matter    References     Bibliography  Nuclear Physics by Irving Kaplan 2nd edition1962 Addison-Wesley General Chemistry by Linus Pauling 1970 Dover Pub. ISBN 0-486-65622-5 Introductory Nuclear Physics by Kenneth S. Krane Pub. Wiley N.D. Cook (2010). Models of the Atomic Nucleus (2nd ed.). Springer. pp. xvi & 324. ISBN 978-3-642-14736-4.  Ahmad, D.Sc., Ishfaq; American Institute of Physics (1996). Physics of particles and nuclei. 1-3 27 (3rd ed.). University of California: American Institute of Physics Press. p. 124.     External links  Ernest Rutherford's biography at the American Institute of Physics American Physical Society Division of Nuclear Physics American Nuclear Society Boiling Water Reactor Plant, BWR Simulator Program Annotated bibliography on nuclear physics from the Alsos Digital Library for Nuclear Issues Nucleonica ..web driven nuclear science Nuclear science wiki Nuclear Data Services - IAEA","label":"foo"},{"text":"A human society is a group of people involved in persistent social interaction, or a large social grouping sharing the same geographical or social territory, typically subject to the same political authority and dominant cultural expectations. Human societies are characterized by patterns of relationships (social relations) between individuals who share a distinctive culture and institutions; a given society may be described as the sum total of such relationships among its constituent members. In the social sciences, a larger society often evinces stratification or dominance patterns in subgroups. Insofar as it is collaborative, a society can enable its members to benefit in ways that would not otherwise be possible on an individual basis; both individual and social (common) benefits can thus be distinguished, or in many cases found to overlap. A society can also consist of like-minded people governed by their own norms and values within a dominant, larger society. This is sometimes referred to as a subculture, a term used extensively within criminology. More broadly, and especially within structuralist thought, a society may be illustrated as an economic, social, industrial or cultural infrastructure, made up of, yet distinct from, a varied collection of individuals. In this regard society can mean the objective relationships people have with the material world and with other people, rather than \"other people\" beyond the individual and their familiar social environment.    Etymology and usageEdit   The term \"society\" came from the Latin word societas, which in turn was derived from the noun socius (\"comrade, friend, ally\"; adjectival form socialis) used to describe a bond or interaction between parties that are friendly, or at least civil. Without an article, the term can refer to the entirety of humanity (also: \"society in general\", \"society at large\", etc.), although those who are unfriendly or uncivil to the remainder of society in this sense may be deemed to be \"antisocial\". Adam Smith wrote that a society \"may subsist among different men, as among different merchants, from a sense of its utility without any mutual love or affection, if only they refrain from doing injury to each other.\" Used in the sense of an association, a society is a body of individuals outlined by the bounds of functional interdependence, possibly comprising characteristics such as national or cultural identity, social solidarity, language, or hierarchical structure.    ConceptionsEdit  Society, in general, addresses the fact that an individual has rather limited means as an autonomous unit. The great apes have always been more (Bonobo, Homo, Pan) or less (Gorilla, Pongo) social animals, so Robinson Crusoe-like situations are either fictions or unusual corner cases to the ubiquity of social context for humans, who fall between presocial and eusocial in the spectrum of animal ethology. Human societies are most often organized according to their primary means of subsistence. Social scientists have identified hunter-gatherer societies, nomadic pastoral societies, horticulturalist or simple farming societies, and intensive agricultural societies, also called civilizations. Some consider industrial and post-industrial societies to be qualitatively different from traditional agricultural societies. Today, anthropologists and many social scientists vigorously oppose the notion of cultural evolution and rigid \"stages\" such as these. In fact, much anthropological data has suggested that complexity (civilization, population growth and density, specialization, etc.) does not always take the form of hierarchical social organization or stratification. Cultural relativism as a widespread approach or ethic has largely replaced notions of \"primitive\", better/worse, or \"progress\" in relation to cultures (including their material culture/technology and social organization). According to anthropologist Maurice Godelier, one critical novelty in human society, in contrast to humanity's closest biological relatives (chimpanzees and bonobos), is the parental role assumed by the males, which supposedly would be absent in our nearest relatives for whom paternity is not generally determinable.    In political scienceEdit  Societies may also be structured politically. In order of increasing size and complexity, there are bands, tribes, chiefdoms, and state societies. These structures may have varying degrees of political power, depending on the cultural, geographical, and historical environments that these societies must contend with. Thus, a more isolated society with the same level of technology and culture as other societies is more likely to survive than one in closer proximity to others that may encroach on their resources. A society that is unable to offer an effective response to other societies it competes with will usually be subsumed into the culture of the competing society.    In sociologyEdit   Sociologist Gerhard Lenski differentiates societies based on their level of technology, communication, and economy: (1) hunters and gatherers, (2) simple agricultural, (3) advanced agricultural, (4) industrial, and (5) special (e.g. fishing societies or maritime societies). This is similar to the system earlier developed by anthropologists Morton H. Fried, a conflict theorist, and Elman Service, an integration theorist, who have produced a system of classification for societies in all human cultures based on the evolution of social inequality and the role of the state. This system of classification contains four categories: Hunter-gatherer bands (categorization of duties and responsibilities). Tribal societies in which there are some limited instances of social rank and prestige. Stratified structures led by chieftains. Civilizations, with complex social hierarchies and organized, institutional governments. In addition to this there are: Humanity, mankind, upon which rest all the elements of society, including society's beliefs. Virtual society, a society based on online identity, which is evolving in the information age. Over time, some cultures have progressed toward more complex forms of organization and control. This cultural evolution has a profound effect on patterns of community. Hunter-gatherer tribes settled around seasonal food stocks to become agrarian villages. Villages grew to become towns and cities. Cities turned into city-states and nation-states. Many societies distribute largess at the behest of some individual or some larger group of people. This type of generosity can be seen in all known cultures; typically, prestige accrues to the generous individual or group. Conversely, members of a society may also shun or scapegoat members of the society who violate its norms. Mechanisms such as gift-giving, joking relationships and scapegoating, which may be seen in various types of human groupings, tend to be institutionalized within a society. Social evolution as a phenomenon carries with it certain elements that could be detrimental to the population it serves. Some societies bestow status on an individual or group of people when that individual or group performs an admired or desired action. This type of recognition is bestowed in the form of a name, title, manner of dress, or monetary reward. In many societies, adult male or female status is subject to a ritual or process of this type. Altruistic action in the interests of the larger group is seen in virtually all societies. The phenomena of community action, shunning, scapegoating, generosity, shared risk, and reward are common to many forms of society.    TypesEdit  Societies are social groups that differ according to subsistence strategies, the ways that humans use technology to provide needs for themselves. Although humans have established many types of societies throughout history, anthropologists tend to classify different societies according to the degree to which different groups within a society have unequal access to advantages such as resources, prestige, or power. Virtually all societies have developed some degree of inequality among their people through the process of social stratification, the division of members of a society into levels with unequal wealth, prestige, or power. Sociologists place societies in three broad categories: pre-industrial, industrial, and postindustrial.    Pre-industrialEdit   In a pre-industrial society, food production, which is carried out through the use of human and animal labor, is the main economic activity. These societies can be subdivided according to their level of technology and their method of producing food. These subdivisions are hunting and gathering, pastoral, horticultural, agricultural, and feudal.   = Hunting and gatheringEdit =  The main form of food production in such societies is the daily collection of wild plants and the hunting of wild animals. Hunter-gatherers move around constantly in search of food. As a result, they do not build permanent villages or create a wide variety of artifacts, and usually only form small groups such as bands and tribes. However, some hunting and gathering societies in areas with abundant resources (such as people of tlingit) lived in larger groups and formed complex hierarchical social structures such as chiefdom. The need for mobility also limits the size of these societies. They generally consist of fewer than 60 people and rarely exceed 100. Statuses within the tribe are relatively equal, and decisions are reached through general agreement. The ties that bind the tribe are more complex than those of the bands. Leadership is personal—charismatic—and used for special purposes only in tribal society. There are no political offices containing real power, and a chief is merely a person of influence, a sort of adviser; therefore, tribal consolidations for collective action are not governmental. The family forms the main social unit, with most societal members being related by birth or marriage. This type of organization requires the family to carry out most social functions, including production and education.   = PastoralEdit =  Pastoralism is a slightly more efficient form of subsistence. Rather than searching for food on a daily basis, members of a pastoral society rely on domesticated herd animals to meet their food needs. Pastoralists live a nomadic life, moving their herds from one pasture to another. Because their food supply is far more reliable, pastoral societies can support larger populations. Since there are food surpluses, fewer people are needed to produce food. As a result, the division of labor (the specialization by individuals or groups in the performance of specific economic activities) becomes more complex. For example, some people become craftworkers, producing tools, weapons, and jewelry. The production of goods encourages trade. This trade helps to create inequality, as some families acquire more goods than others do. These families often gain power through their increased wealth. The passing on of property from one generation to another helps to centralize wealth and power. Over time emerge hereditary chieftainships, the typical form of government in pastoral societies.   = HorticulturalEdit =  Fruits and vegetables grown in garden plots that have been cleared from the jungle or forest provide the main source of food in a horticultural society. These societies have a level of technology and complexity similar to pastoral societies. Some horticultural groups use the slash-and-burn method to raise crops. The wild vegetation is cut and burned, and ashes are used as fertilizers. Horticulturists use human labor and simple tools to cultivate the land for one or more seasons. When the land becomes barren, horticulturists clear a new plot and leave the old plot to revert to its natural state. They may return to the original land several years later and begin the process again. By rotating their garden plots, horticulturists can stay in one area for a fairly long period of time. This allows them to build semipermanent or permanent villages. The size of a village's population depends on the amount of land available for farming; thus villages can range from as few as 30 people to as many as 2000. As with pastoral societies, surplus food leads to a more complex division of labor. Specialized roles in horticultural societies include craftspeople, shamans (religious leaders), and traders. This role specialization allows people to create a wide variety of artifacts. As in pastoral societies, surplus food can lead to inequalities in wealth and power within horticultural political systems, developed because of the settled nature of horticultural life.   = AgrarianEdit =  Agrarian societies use agricultural technological advances to cultivate crops over a large area. Sociologists use the phrase Agricultural Revolution to refer to the technological changes that occurred as long as 8,500 years ago that led to cultivating crops and raising farm animals. Increases in food supplies then led to larger populations than in earlier communities. This meant a greater surplus, which resulted in towns that became centers of trade supporting various rulers, educators, craftspeople, merchants, and religious leaders who did not have to worry about locating nourishment. Greater degrees of social stratification appeared in agrarian societies. For example, women previously had higher social status because they shared labor more equally with men. In hunting and gathering societies, women even gathered more food than men. However, as food stores improved and women took on lesser roles in providing food for the family, they increasingly became subordinate to men. As villages and towns expanded into neighboring areas, conflicts with other communities inevitably occurred. Farmers provided warriors with food in exchange for protection against invasion by enemies. A system of rulers with high social status also appeared. This nobility organized warriors to protect the society from invasion. In this way, the nobility managed to extract goods from “lesser” members of society.   = FeudalEdit =  Feudalism was a form of society based on ownership of land. Unlike today's farmers, vassals under feudalism were bound to cultivating their lord's land. In exchange for military protection, the lords exploited the peasants into providing food, crops, crafts, homage, and other services to the landowner. The estates of the realm system of feudalism was often multigenerational; the families of peasants may have cultivated their lord's land for generations.    IndustrialEdit   Between the 15th and 16th centuries, a new economic system emerged that began to replace feudalism. Capitalism is marked by open competition in a free market, in which the means of production are privately owned. Europe's exploration of the Americas served as one impetus for the development of capitalism. The introduction of foreign metals, silks, and spices stimulated great commercial activity in European societies. Industrial societies rely heavily on machines powered by fuels for the production of goods. This produced further dramatic increases in efficiency. The increased efficiency of production of the industrial revolution produced an even greater surplus than before. Now the surplus was not just agricultural goods, but also manufactured goods. This larger surplus caused all of the changes discussed earlier in the domestication revolution to become even more pronounced. Once again, the population boomed. Increased productivity made more goods available to everyone. However, inequality became even greater than before. The breakup of agricultural-based feudal societies caused many people to leave the land and seek employment in cities. This created a great surplus of labor and gave capitalists plenty of laborers who could be hired for extremely low wages.    Post-industrialEdit   Post-industrial societies are societies dominated by information, services, and high technology more than the production of goods. Advanced industrial societies are now seeing a shift toward an increase in service sectors over manufacturing and production. The United States is the first country to have over half of its work force employed in service industries. Service industries include government, research, education, health, sales, law, and banking.    Contemporary usageEdit  The term \"society\" is currently used to cover both a number of political and scientific connotations as well as a variety of associations.    WesternEdit   The development of the Western world has brought with it the emerging concepts of Western culture, politics, and ideas, often referred to simply as \"Western society\". Geographically, it covers at the very least the countries of Western Europe, North America, Australia, and New Zealand. It sometimes also includes Eastern Europe, South America, and Israel. The cultures and lifestyles of all of these stem from Western Europe. They all enjoy relatively strong economies and stable governments, allow freedom of religion, have chosen democracy as a form of governance, favor capitalism and international trade, are heavily influenced by Judeo-Christian values, and have some form of political and military alliance or cooperation.    InformationEdit   Although the concept of information society has been under discussion since the 1930s, in the modern world it is almost always applied to the manner in which information technologies have impacted society and culture. It therefore covers the effects of computers and telecommunications on the home, the workplace, schools, government, and various communities and organizations, as well as the emergence of new social forms in cyberspace. One of the European Union's areas of interest is the information society. Here policies are directed towards promoting an open and competitive digital economy, research into information and communication technologies, as well as their application to improve social inclusion, public services, and quality of life. The International Telecommunications Union's World Summit on the Information Society in Geneva and Tunis (2003 and 2005) has led to a number of policy and application areas where action is required. These include: promotion of ICTs for development; information and communication infrastructure; access to information and knowledge; capacity building; building confidence and security in the use of ICTs; enabling environment; ICT applications in the areas of government, business, learning, health, employment, environment, agriculture and science; cultural and linguistic diversity and local content; media; ethical dimensions of the information society; and international and regional cooperation.    KnowledgeEdit   As access to electronic information resources increased at the beginning of the 21st century, special attention was extended from the information society to the knowledge society. An analysis by the Irish government stated, \"The capacity to manipulate, store and transmit large quantities of information cheaply has increased at a staggering rate over recent years. The digitisation of information and the associated pervasiveness of the Internet are facilitating a new intensity in the application of knowledge to economic activity, to the extent that it has become the predominant factor in the creation of wealth. As much as 70 to 80 percent of economic growth is now said to be due to new and better knowledge.\" The Second World Summit on the Knowledge Society, held in Chania, Crete, in September 2009, gave special attention to the following topics: business and enterprise computing; technology-enhanced learning; social and humanistic computing; culture, tourism and technology; e-government and e-democracy; innovation, sustainable development, and strategic management; service science, management, and engineering; intellectual and human capital development; ICTs for ecology and the green economy; future prospects for the knowledge society; and technologies and business models for the creative industries.    Other usesEdit  People of many nations united by common political and cultural traditions, beliefs, or values are sometimes also said to form a society (such as Judeo-Christian, Eastern, and Western). When used in this context, the term is employed as a means of contrasting two or more \"societies\" whose members represent alternative conflicting and competing worldviews. Some academic, professional, and scientific associations describe themselves as societies (for example, the American Mathematical Society, the American Society of Civil Engineers, or the Royal Society). In some countries, e.g. the United States, France, and Latin America, the term \"society' is used in commerce to denote a partnership between investors or the start of a business. In the United Kingdom, partnerships are not called societies, but co-operatives or mutuals are often known as societies (such as friendly societies and building societies).    See alsoEdit     NotesEdit     Further readingEdit  Effland, R. 1998. The Cultural Evolution of Civilizations Mesa Community College. Jenkins, R. 2002. Foundations of Sociology. London: Palgrave MacMillan. ISBN 0-333-96050-5. Lenski, G. 1974. Human Societies: An Introduction to Macrosociology. New York: McGraw- Hill, Inc. Raymond Williams, \"www.flpmihai.blogspot.com\", in: Williams, Key Words: A Vocabulary of Culture and Society. Fontana, 1976. Althusser, Louis and Balibar, Étienne. Reading Capital. London: Verso, 2009. Bottomore, Tom (ed). A Dictionary of Marxist Thought, 2nd ed. Malden, MA: Blackwell Publishing, 1991. 45–48. Calhoun, Craig (ed), Dictionary of the Social Sciences Oxford University Press (2002) Hall, Stuart. \"Rethinking the Base and Superstructure Metaphor.\" Papers on Class, Hegemony and Party. Bloomfield, J., ed. London: Lawrence & Wishart, 1977. Chris Harman. \"Base and Superstructure\". International Socialism 2:32, Summer 1986, pp. 3–44. Harvey, David. A Companion to Marx's Capital. London: Verso, 2010. Larrain, Jorge. Marxism and Ideology. Atlantic Highlands, NJ: Humanities Press, 1983. Lukács, Georg. History and Class Consciousness. Cambridge, MA: MIT Press, 1972. Postone, Moishe. Time, Labour, and Social Domination: A Reinterpretation of Marx's Critical Theory. Cambridge [England]: Cambridge University Press, 1993. Williams, Raymond. Marxism and Literature. Oxford: Oxford University Press, 1977.    External linksEdit  Society at DMOZ Definition of Society from the OED. Lecture notes on \"Defining Society\" from East Carolina University. Internet Modern History Sourcebook: Industrial Revolution The Day the World Took Off Six part video series from the University of Cambridge tracing the question \"Why did the Industrial Revolution begin when and where it did.\" BBC History Home Page: Industrial Revolution National Museum of Science and Industry website: machines and personalities Industrial Revolution and the Standard of Living by Clark Nardinelli - the debate over whether standards of living rose or fell. Cliff Notes on Types of Societies Perceptions of Knowledge, Knowledge Society, and Knowledge Management","label":"foo"},{"text":"A television, commonly referred to as TV, telly or the tube, is a telecommunication medium used for transmitting sound with moving images in monochrome (black-and-white), or in colour, and in two or three dimensions. It can refer to a television set, a television program, or the medium of television transmission. Television is a mass medium, for entertainment, education, news and advertising. Television became available in crude experimental forms in the late 1920s. After World War II, an improved form became popular in the United States and Britain, and television sets became commonplace in homes, businesses, and institutions. During the 1950s, television was the primary medium for influencing public opinion. In the mid-1960s, color broadcasting was introduced in the US and most other developed countries. The availability of storage media such as VHS tape (1976), DVDs (1997), and high-definition Blu-ray Discs (2006) enabled viewers to watch recorded material such as movies. At the end of the first decade of the 2000s, digital television transmissions greatly increased in popularity. Another development was the move from standard-definition television (SDTV) (576i, with 576 interlaced lines of resolution and 480i) to high-definition television (HDTV), which provides a resolution that is substantially higher. HDTV may be transmitted in various formats: 1080p, 1080i and 720p. Since 2010, with the invention of smart television, Internet television has increased the availability of television programs and movies via the Internet through services such as Netflix, iPlayer, Hulu, Roku and Chromecast. In 2013, 79% of the world's households owned a television set. The replacement of early bulky, high-voltage cathode ray tube (CRT) screen displays with compact, energy-efficient, flat-panel alternative technologies such as plasma displays, LCDs (both fluorescent-backlit and LED), and OLED displays was a hardware revolution that began with computer monitors in the late 1990s. Most TV sets sold in the 2000s were flat-panel, mainly LEDs. Major manufacturers announced the discontinuation of CRT, DLP, plasma, and even fluorescent-backlit LCDs by the mid-2010s. LEDs are expected to be replaced gradually by OLEDs in the near future. Also, major manufacturers have announced that they will increasingly produce smart TV sets in the mid-2010s. Smart TVs with integrated Internet and Web 2.0 functions are expected to become the dominant form of television set by the late 2010s. Television signals were initially distributed only as terrestrial television using high-powered radio-frequency transmitters to broadcast the signal to individual television receivers. Alternatively television signals are distributed by co-axial cable or optical fibre, satellite systems and via the Internet. Until the early 2000s, these were transmitted as analog signals but countries started switching to digital, this transition is expected to be completed worldwide by late 2010s. A standard television set is composed of multiple internal electronic circuits, including a tuner for receiving and decoding broadcast signals. A visual display device which lacks a tuner is correctly called a video monitor rather than a television.    EtymologyEdit  The word television comes from Ancient Greek τῆλε (tèle), meaning \"far\", and Latin visio, meaning \"sight\". The slang term \"telly\" is more common in the UK. The slang term \"the tube\" refers to the bulky cathode ray tube used on most TVs until the advent of flat-screen TVs.    HistoryEdit     Mechanical televisionEdit   Facsimile transmission systems for still photographs pioneered methods of mechanical scanning of images in early 19th century. Alexander Bain introduced the facsimile machine between 1843 and 1846. Frederick Bakewell demonstrated a working laboratory version in 1851.Willoughby Smith discovered the photoconductivity of the element selenium in 1873.  As a 23-year-old German university student, Paul Julius Gottlieb Nipkow proposed and patented the Nipkow disk in 1884. This was a spinning disk with a spiral pattern of holes in it, so each hole scanned a line of the image. Although he never built a working model of the system, variations of Nipkow's spinning-disk \"image rasterizer\" became exceedingly common. Constantin Perskyi had coined the word television in a paper read to the International Electricity Congress at the International World Fair in Paris on 25 August 1900. Perskyi's paper reviewed the existing electromechanical technologies, mentioning the work of Nipkow and others. However, it was not until 1907 that developments in amplification tube technology by Lee de Forest and Arthur Korn, among others, made the design practical. The first demonstration of the instantaneous transmission of images was by Georges Rignoux and A. Fournier in Paris in 1909. A matrix of 64 selenium cells, individually wired to a mechanical commutator, served as an electronic retina. In the receiver, a type of Kerr cell modulated the light and a series of variously angled mirrors attached to the edge of a rotating disc scanned the modulated beam onto the display screen. A separate circuit regulated synchronization. The 8x8 pixel resolution in this proof-of-concept demonstration was just sufficient to clearly transmit individual letters of the alphabet. An updated image was transmitted \"several times\" each second. In 1911, Boris Rosing and his student Vladimir Zworykin created a system that used a mechanical mirror-drum scanner to transmit, in Zworykin's words, \"very crude images\" over wires to the \"Braun tube\" (cathode ray tube or \"CRT\") in the receiver. Moving images were not possible because, in the scanner: \"the sensitivity was not enough and the selenium cell was very laggy\".  By the 1920s, when amplification made television practical, Scottish inventor John Logie Baird employed the Nipkow disk in his prototype video systems. On 25 March 1925, Baird gave the first public demonstration of televised silhouette images in motion, at Selfridge's Department Store in London. Since human faces had inadequate contrast to show up on his primitive system, he televised a ventriloquist's dummy named \"Stooky Bill\" talking and moving, whose painted face had higher contrast. By 26 January 1926 he demonstrated the transmission of the image of a face in motion by radio. This is widely regarded as the first television demonstration. The subject was Baird's business partner Oliver Hutchinson. Baird's system used the Nipkow disk for both scanning the image and displaying it. A bright light shining through a spinning Nipkow disk set with lenses projected a bright spot of light which swept across the subject. Selenium photoelectric tube detected the light reflected from the subject and converted it into a proportional electrical signal. This was transmitted by AM radio waves to a receiver unit, where the video signal was applied to a neon light behind a second Nipkow disk rotating synchronized with the first. The brightness of the neon lamp was varied in proportion to the brightness of each spot on the image. As each hole in the disk passed by, one scan line of the image was reproduced. Baird's disk had 30 holes, producing an image with only 30 scan lines, just enough to recognize a human face. In 1927, Baird transmitted a signal over 438 miles (705 km) of telephone line between London and Glasgow. In 1928, Baird's company (Baird Television Development Company/Cinema Television) broadcast the first transatlantic television signal, between London and New York, and the first shore-to-ship transmission. In 1929, he became involved in the first experimental mechanical television service in Germany. In November of the same year, Baird and Bernard Natan of Pathé established France's first television company, Télévision-Baird-Natan. In 1931, he made the first outdoor remote broadcast, of the Epsom Derby. In 1932, he demonstrated ultra-short wave television. Baird's mechanical system reached a peak of 240-lines of resolution on BBC television broadcasts in 1936 though the mechanical system did not scan the televised scene directly. Instead a 17.5mm film was shot, rapidly developed and then scanned while the film was still wet. An American inventor, Charles Francis Jenkins, also pioneered the television. He published an article on \"Motion Pictures by Wireless\" in 1913, but it was not until 1923 that he transmitted moving silhouette images for witnesses; and it was on 13 June 1925 that he publicly demonstrated synchronized transmission of silhouette pictures. In 1925 Jenkins used the Nipkow disk and transmitted the silhouette image of a toy windmill in motion, over a distance of five miles, from a naval radio station in Maryland to his laboratory in Washington, D.C., using a lensed disk scanner with a 48-line resolution. He was granted the U.S. patent No. 1,544,156 (Transmitting Pictures over Wireless) on 30 June 1925 (filed 13 March 1922). Herbert E. Ives and Frank Gray of Bell Telephone Laboratories gave a dramatic demonstration of mechanical television on 7 April 1927. Their reflected-light television system included both small and large viewing screens. The small receiver had a 2-inch-wide by 2.5-inch-high screen. The large receiver had a screen 24 inches wide by 30 inches high. Both sets were capable of reproducing reasonably accurate, monochromatic moving images. Along with the pictures, the sets received synchronized sound. The system transmitted images over two paths: first, a copper wire link from Washington to New York City, then a radio link from Whippany, New Jersey. Comparing the two transmission methods, viewers noted no difference in quality. Subjects of the telecast included Secretary of Commerce Herbert Hoover. A flying-spot scanner beam illuminated these subjects. The scanner that produced the beam had a 50-aperture disk. The disc revolved at a rate of 18 frames per second, capturing one frame about every 56 milliseconds. (Today's systems typically transmit 30 or 60 frames per second, or one frame every 33.3 or 16.7 milliseconds respectively.) Television historian Albert Abramson underscored the significance of the Bell Labs demonstration: \"It was in fact the best demonstration of a mechanical television system ever made to this time. It would be several years before any other system could even begin to compare with it in picture quality.\" In 1928, WRGB then W2XB was started as world's first television station. It broadcast from the General Electric facility in Schenectady, NY. It was popularly known as \"WGY Television\". Meanwhile, in the Soviet Union, Léon Theremin had been developing a mirror drum-based television, starting with 16 lines resolution in 1925, then 32 lines and eventually 64 using interlacing in 1926, and as part of his thesis on 7 May 1926 he electrically transmitted and then projected near-simultaneous moving images on a five-foot square screen. By 1927 he achieved an image of 100 lines, a resolution that was not surpassed until 1931 by RCA, with 120 lines. On 25 December 1926, Kenjiro Takayanagi demonstrated a television system with a 40-line resolution that employed a Nipkow disk scanner and CRT display at Hamamatsu Industrial High School in Japan. This prototype is still on display at the Takayanagi Memorial Museum in Shizuoka University, Hamamatsu Campus. His research in creating a production model was halted by the US after Japan lost World War II. Because only a limited number of holes could be made in the disks, and disks beyond a certain diameter became impractical, image resolution on mechanical television broadcasts was relatively low, ranging from about 30 lines up to 120 or so. Nevertheless, the image quality of 30-line transmissions steadily improved with technical advances, and by 1933 the UK broadcasts using the Baird system were remarkably clear. A few systems ranging into the 200-line region also went on the air. Two of these were the 180-line system that Compagnie des Compteurs (CDC) installed in Paris in 1935, and the 180-line system that Peck Television Corp. started in 1935 at station VE9AK in Montreal. The advancement of all-electronic television (including image dissectors and other camera tubes and cathode ray tubes for the reproducer) marked the beginning of the end for mechanical systems as the dominant form of television. Mechanical television, despite its inferior image quality and generally smaller picture, would remain the primary television technology until the 1930s. The last mechanical television broadcasts ended in 1939 at stations run by a handful of public universities in the United States.    Electronic televisionEdit   In 1897, English physicist J. J. Thomson was able, in his three famous experiments, to deflect cathode rays, a fundamental function of the modern cathode ray tube (CRT). The earliest version of the CRT was invented by the German physicist Ferdinand Braun in 1897 and is also known as the Braun tube. It was a cold-cathode diode, a modification of the Crookes tube with a phosphor-coated screen. In 1907, Russian scientist Boris Rosing used a CRT in the receiving end of an experimental video signal to form a picture. He managed to display simple geometric shapes onto the screen, which marked the first time that CRT technology was used for what is now known as television. In 1908 Alan Archibald Campbell-Swinton, fellow of the Royal Society (UK), published a letter in the scientific journal Nature in which he described how \"distant electric vision\" could be achieved by using a cathode ray tube, or Braun tube, as both a transmitting and receiving device, He expanded on his vision in a speech given in London in 1911 and reported in The Times and the Journal of the Röntgen Society. In a letter to Nature published in October 1926, Campbell-Swinton also announced the results of some \"not very successful experiments\" he had conducted with G. M. Minchin and J. C. M. Stanton. They had attempted to generate an electrical signal by projecting an image onto a selenium-coated metal plate that was simultaneously scanned by a cathode ray beam. These experiments were conducted before March 1914, when Minchin died, but they were later repeated by two different teams in 1937, by H. Miller and J. W. Strange from EMI, and by H. Iams and A. Rose from RCA. Both teams succeeded in transmitting \"very faint\" images with the original Campbell-Swinton's selenium-coated plate. Although others had experimented with using a cathode ray tube as a receiver, the concept of using one as a transmitter was novel. The first cathode ray tube to use a hot cathode was developed by John B. Johnson (who gave his name to the term Johnson noise) and Harry Weiner Weinhart of Western Electric, and became a commercial product in 1922. In 1926, Hungarian engineer Kálmán Tihanyi designed a television system utilizing fully electronic scanning and display elements and employing the principle of \"charge storage\" within the scanning (or \"camera\") tube. The problem of low sensitivity to light resulting in low electrical output from transmitting or \"camera\" tubes would be solved with the introduction of charge-storage technology by Kálmán Tihanyi beginning in 1924. His solution was a camera tube that accumulated and stored electrical charges (\"photoelectrons\") within the tube throughout each scanning cycle. The device was first described in a patent application he filed in Hungary in March 1926 for a television system he dubbed \"Radioskop\". After further refinements included in a 1928 patent application, Tihanyi's patent was declared void in Great Britain in 1930, so he applied for patents in the United States. Although his breakthrough would be incorporated into the design of RCA's \"iconoscope\" in 1931, the U.S. patent for Tihanyi's transmitting tube would not be granted until May 1939. The patent for his receiving tube had been granted the previous October. Both patents had been purchased by RCA prior to their approval. Charge storage remains a basic principle in the design of imaging devices for television to the present day. On 25 December 1926, at Hamamatsu Industrial High School in Japan, Japanese inventor Kenjiro Takayanagi demonstrated a TV system with a 40-line resolution that employed a CRT display. This was the first working example of a fully electronic television receiver. Takayanagi did not apply for a patent. On 7 September 1927, American inventor Philo Farnsworth's image dissector camera tube transmitted its first image, a simple straight line, at his laboratory at 202 Green Street in San Francisco. By 3 September 1928, Farnsworth had developed the system sufficiently to hold a demonstration for the press. This is widely regarded as the first electronic television demonstration. In 1929, the system was further improved by the elimination of a motor generator, so that his television system now had no mechanical parts. That year, Farnsworth transmitted the first live human images with his system, including a three and a half-inch image of his wife Elma (\"Pem\") with her eyes closed (possibly due to the bright lighting required).  Meanwhile, Vladimir Zworykin was also experimenting with the cathode ray tube to create and show images. While working for Westinghouse Electric in 1923, he began to develop an electronic camera tube. But in a 1925 demonstration, the image was dim, had low contrast and poor definition, and was stationary. Zworykin's imaging tube never got beyond the laboratory stage. But RCA, which acquired the Westinghouse patent, asserted that the patent for Farnsworth's 1927 image dissector was written so broadly that it would exclude any other electronic imaging device. Thus RCA, on the basis of Zworykin's 1923 patent application, filed a patent interference suit against Farnsworth. The U.S. Patent Office examiner disagreed in a 1935 decision, finding priority of invention for Farnsworth against Zworykin. Farnsworth claimed that Zworykin's 1923 system would be unable to produce an electrical image of the type to challenge his patent. Zworykin received a patent in 1928 for a color transmission version of his 1923 patent application, he also divided his original application in 1931. Zworykin was unable or unwilling to introduce evidence of a working model of his tube that was based on his 1923 patent application. In September 1939, after losing an appeal in the courts, and determined to go forward with the commercial manufacturing of television equipment, RCA agreed to pay Farnsworth US$1 million over a ten-year period, in addition to license payments, to use his patents. In 1933, RCA introduced an improved camera tube that relied on Tihanyi's charge storage principle. Dubbed the Iconoscope by Zworykin, the new tube had a light sensitivity of about 75,000 lux, and thus was claimed to be much more sensitive than Farnsworth's image dissector. However, Farnsworth had overcome his power problems with his Image Dissector through the invention of a completely unique \"multipactor\" device that he began work on in 1930, and demonstrated in 1931. This small tube could amplify a signal reportedly to the 60th power or better and showed great promise in all fields of electronics. Unfortunately, a problem with the multipactor was that it wore out at an unsatisfactory rate. At the Berlin Radio Show in August 1931, Manfred von Ardenne gave a public demonstration of a television system using a CRT for both transmission and reception. However, Ardenne had not developed a camera tube, using the CRT instead as a flying-spot scanner to scan slides and film. Philo Farnsworth gave the world's first public demonstration of an all-electronic television system, using a live camera, at the Franklin Institute of Philadelphia on 25 August 1934, and for ten days afterwards. Mexican inventor Guillermo González Camarena also played an important role in early TV. His experiments with TV (known as telectroescopía at first) began in 1931 and led to a patent for the \"trichromatic field sequential system\" color television in 1940. In Britain the EMI engineering team led by Isaac Shoenberg applied in 1932 for a patent for a new device they dubbed \"the Emitron\", which formed the heart of the cameras they designed for the BBC. On 2 November 1936, a 405-line broadcasting service employing the Emitron began at studios in Alexandra Palace, and transmitted from a specially built mast atop one of the Victorian building's towers. It alternated for a short time with Baird's mechanical system in adjoining studios, but was more reliable and visibly superior. This was the world's first regular \"high-definition\" television service. The original American iconoscope was noisy, had a high ratio of interference to signal, and ultimately gave disappointing results, especially when compared to the high definition mechanical scanning systems then becoming available. The EMI team under the supervision of Isaac Shoenberg analyzed how the iconoscope (or Emitron) produces an electronic signal and concluded that its real efficiency was only about 5% of the theoretical maximum. They solved this problem by developing and patenting in 1934 two new camera tubes dubbed super-Emitron and CPS Emitron. The super-Emitron was between ten and fifteen times more sensitive than the original Emitron and iconoscope tubes and, in some cases, this ratio was considerably greater. It was used for an outside broadcasting by the BBC, for the first time, on Armistice Day 1937, when the general public could watch on a television set as the King laid a wreath at the Cenotaph. This was the first time that anyone could broadcast a live street scene from cameras installed on the roof of neighboring buildings, because neither Farnsworth nor RCA could do the same before the 1939 New York World's Fair.  On the other hand, in 1934, Zworykin shared some patent rights with the German licensee company Telefunken. The \"image iconoscope\" (\"Superikonoskop\" in Germany) was produced as a result of the collaboration. This tube is essentially identical to the super-Emitron. The production and commercialization of the super-Emitron and image iconoscope in Europe were not affected by the patent war between Zworykin and Farnsworth, because Dieckmann and Hell had priority in Germany for the invention of the image dissector, having submitted a patent application for their Lichtelektrische Bildzerlegerröhre für Fernseher (Photoelectric Image Dissector Tube for Television) in Germany in 1925, two years before Farnsworth did the same in the United States. The image iconoscope (Superikonoskop) became the industrial standard for public broadcasting in Europe from 1936 until 1960, when it was replaced by the vidicon and plumbicon tubes. Indeed, it was the representative of the European tradition in electronic tubes competing against the American tradition represented by the image orthicon. The German company Heimann produced the Superikonoskop for the 1936 Berlin Olympic Games, later Heimann also produced and commercialized it from 1940 to 1955; finally the Dutch company Philips produced and commercialized the image iconoscope and multicon from 1952 to 1958. American television broadcasting, at the time, consisted of a variety of markets in a wide range of sizes, each competing for programming and dominance with separate technology, until deals were made and standards agreed upon in 1941. RCA, for example, used only Iconoscopes in the New York area, but Farnsworth Image Dissectors in Philadelphia and San Francisco. In September 1939, RCA agreed to pay the Farnsworth Television and Radio Corporation royalties over the next ten years for access to Farnsworth's patents. With this historic agreement in place, RCA integrated much of what was best about the Farnsworth Technology into their systems. In 1941, the United States implemented 525-line television. The world's first 625-line television standard was designed in the Soviet Union in 1944, and became a national standard in 1946. The first broadcast in 625-line standard occurred in Moscow in 1948. The concept of 625 lines per frame was subsequently implemented in the European CCIR standard. In 1936, Kálmán Tihanyi described the principle of plasma display, the first flat panel display system.    Color televisionEdit   The basic idea of using three monochrome images to produce a color image had been experimented with almost as soon as black-and-white televisions had first been built. Although he gave no practical details, among the earliest published proposals for television was one by Maurice Le Blanc, in 1880, for a color system, including the first mentions in television literature of line and frame scanning. Polish inventor Jan Szczepanik patented a color television system in 1897, using a selenium photoelectric cell at the transmitter and an electromagnet controlling an oscillating mirror and a moving prism at the receiver. But his system contained no means of analyzing the spectrum of colors at the transmitting end, and could not have worked as he described it. Another inventor, Hovannes Adamian, also experimented with color television as early as 1907. The first color television project is claimed by him, and was patented in Germany on 31 March 1908, patent № 197183, then in Britain, on 1 April 1908, patent № 7219, in France (patent № 390326) and in Russia in 1910 (patent № 17912). Scottish inventor John Logie Baird demonstrated the world's first color transmission on 3 July 1928, using scanning discs at the transmitting and receiving ends with three spirals of apertures, each spiral with filters of a different primary color; and three light sources at the receiving end, with a commutator to alternate their illumination. Baird also made the world's first color broadcast on 4 February 1938, sending a mechanically scanned 120-line image from Baird's Crystal Palace studios to a projection screen at London's Dominion Theatre. Mechanically scanned color television was also demonstrated by Bell Laboratories in June 1929 using three complete systems of photoelectric cells, amplifiers, glow-tubes, and color filters, with a series of mirrors to superimpose the red, green, and blue images into one full color image. The first practical hybrid system was again pioneered by John Logie Baird. In 1940 he publicly demonstrated a color television combining a traditional black-and-white display with a rotating colored disk. This device was very \"deep\", but was later improved with a mirror folding the light path into an entirely practical device resembling a large conventional console. However, Baird was not happy with the design, and as early as 1944 had commented to a British government committee that a fully electronic device would be better. In 1939, Hungarian engineer Peter Carl Goldmark introduced an electro-mechanical system while at CBS, which contained an Iconoscope sensor. The CBS field-sequential color system was partly mechanical, with a disc made of red, blue, and green filters spinning inside the television camera at 1,200 rpm, and a similar disc spinning in synchronization in front of the cathode ray tube inside the receiver set. The system was first demonstrated to the Federal Communications Commission (FCC) on 29 August 1940, and shown to the press on 4 September. CBS began experimental color field tests using film as early as 28 August 1940, and live cameras by 12 November. NBC (owned by RCA) made its first field test of color television on 20 February 1941. CBS began daily color field tests on 1 June 1941. These color systems were not compatible with existing black-and-white television sets, and, as no color television sets were available to the public at this time, viewing of the color field tests was restricted to RCA and CBS engineers and the invited press. The War Production Board halted the manufacture of television and radio equipment for civilian use from 22 April 1942 to 20 August 1945, limiting any opportunity to introduce color television to the general public. As early as 1940, Baird had started work on a fully electronic system he called the \"Telechrome\". Early Telechrome devices used two electron guns aimed at either side of a phosphor plate. The phosphor was patterned so the electrons from the guns only fell on one side of the patterning or the other. Using cyan and magenta phosphors, a reasonable limited-color image could be obtained. He also demonstrated the same system using monochrome signals to produce a 3D image (called \"stereoscopic\" at the time). A demonstration on 16 August 1944 was the first example of a practical color television system. Work on the Telechrome continued and plans were made to introduce a three-gun version for full color. However, Baird's untimely death in 1946 ended development of the Telechrome system. Similar concepts were common through the 1940s and 50s, differing primarily in the way they re-combined the colors generated by the three guns. The Geer tube was similar to Baird's concept, but used small pyramids with the phosphors deposited on their outside faces, instead of Baird's 3D patterning on a flat surface. The Penetron used three layers of phosphor on top of each other and increased the power of the beam to reach the upper layers when drawing those colors. The Chromatron used a set of focusing wires to select the colored phosphors arranged in vertical stripes on the tube. One of the great technical challenges of introducing color broadcast television was the desire to conserve bandwidth, potentially three times that of the existing black-and-white standards, and not use an excessive amount of radio spectrum. In the United States, after considerable research, the National Television Systems Committee approved an all-electronic Compatible color system developed by RCA, which encoded the color information separately from the brightness information and greatly reduced the resolution of the color information in order to conserve bandwidth. The brightness image remained compatible with existing black-and-white television sets at slightly reduced resolution, while color televisions could decode the extra information in the signal and produce a limited-resolution color display. The higher resolution black-and-white and lower resolution color images combine in the brain to produce a seemingly high-resolution color image. The NTSC standard represented a major technical achievement.  Although all-electronic color was introduced in the U.S. in 1953, high prices and the scarcity of color programming greatly slowed its acceptance in the marketplace. The first national color broadcast (the 1954 Tournament of Roses Parade) occurred on 1 January 1954, but during the following ten years most network broadcasts, and nearly all local programming, continued to be in black-and-white. It was not until the mid-1960s that color sets started selling in large numbers, due in part to the color transition of 1965 in which it was announced that over half of all network prime-time programming would be broadcast in color that fall. The first all-color prime-time season came just one year later. In 1972, the last holdout among daytime network programs converted to color, resulting in the first completely all-color network season. Early color sets were either floor-standing console models or tabletop versions nearly as bulky and heavy; so in practice they remained firmly anchored in one place. The introduction of GE's relatively compact and lightweight Porta-Color set in the spring of 1966 made watching color television a more flexible and convenient proposition. In 1972, sales of color sets finally surpassed sales of black-and-white sets. Color broadcasting in Europe was not standardized on the PAL format until the 1960s, and broadcasts did not start until 1967. By this point many of the technical problems in the early sets had been worked out, and the spread of color sets in Europe was fairly rapid. By the mid-1970s, the only stations broadcasting in black-and-white were a few high-numbered UHF stations in small markets, and a handful of low-power repeater stations in even smaller markets such as vacation spots. By 1979, even the last of these had converted to color and by the early 1980s B&W sets had been pushed into niche markets, notably low-power uses, small portable sets, or for use as video monitor screens in lower-cost consumer equipment. By late 1980's even these areas switched to color sets.    Digital televisionEdit   Digital television (DTV) is the transmission of audio and video by digitally processed and multiplexed signals, in contrast to the totally analog and channel separated signals used by analog television. Digital TV can support more than one program in the same channel bandwidth. It is an innovative service that represents the first significant evolution in television technology since color television in the 1950s. Digital TV's roots have been tied very closely to the availability of inexpensive, high performance computers. It wasn't until the 1990s that digital TV became a real possibility. In the mid-1980s, as Japanese consumer electronics firms forged ahead with the development of HDTV technology, the MUSE analog format proposed by NHK, a Japanese company, was seen as a pacesetter that threatened to eclipse U.S. electronics companies. Until June 1990, the Japanese MUSE standard, based on an analog system, was the front-runner among the more than 23 different technical concepts under consideration. Then, an American company, General Instrument, demonstrated the feasibility of a digital television signal. This breakthrough was of such significance that the FCC was persuaded to delay its decision on an ATV standard until a digitally based standard could be developed. In March 1990, when it became clear that a digital standard was feasible, the FCC made a number of critical decisions. First, the Commission declared that the new ATV standard must be more than an enhanced analog signal, but be able to provide a genuine HDTV signal with at least twice the resolution of existing television images.(7) Then, to ensure that viewers who did not wish to buy a new digital television set could continue to receive conventional television broadcasts, it dictated that the new ATV standard must be capable of being \"simulcast\" on different channels.(8)The new ATV standard also allowed the new DTV signal to be based on entirely new design principles. Although incompatible with the existing NTSC standard, the new DTV standard would be able to incorporate many improvements. The final standards adopted by the FCC did not require a single standard for scanning formats, aspect ratios, or lines of resolution. This compromise resulted from a dispute between the consumer electronics industry (joined by some broadcasters) and the computer industry (joined by the film industry and some public interest groups) over which of the two scanning processes—interlaced or progressive—would be best suited for the newer digital HDTV compatible display devices. Interlaced scanning, which had been specifically designed for older analogue CRT display technologies, scans even-numbered lines first, then odd-numbered ones. In fact interlaced scanning can be looked at as the first video compression model as it was partly designed in the 1940s to double the image resolution to exceed the limitations of the television broadcast bandwidth. Another reason for its adoption was to limit the flickering on early CRT screens whose phosphor coated screens could only retain the image from the electron scanning gun for a relatively short duration. However interlaced scanning does not work as efficiently on newer display devices such as Liquid-crystal (LCD) for example which are better suited to a more frequent progressive refresh rate. Progressive scanning, the format that the computer industry had long adopted for computer display monitors, scans every line in sequence, from top to bottom. Progressive scanning in effect doubles the amount of data generated for every full screen displayed in comparison to interlaced scanning by painting the screen in one pass in 1/60 second, instead of two passes in 1/30 second. The computer industry argued that progressive scanning is superior because it does not \"flicker\" on the new standard of display devices in the manner of interlaced scanning. It also argued that progressive scanning enables easier connections with the Internet, and is more cheaply converted to interlaced formats than vice versa. The film industry also supported progressive scanning because it offers a more efficient means of converting filmed programming into digital formats. For their part, the consumer electronics industry and broadcasters argued that interlaced scanning was the only technology that could transmit the highest quality pictures then (and currently) feasible, i.e., 1,080 lines per picture and 1,920 pixels per line. Broadcasters also favored interlaced scanning because their vast archive of interlaced programming is not readily compatible with a progressive format. Digital television transition started in late 2000s. All the governments across the world set the deadline for analog shutdown by 2010s. Initially the adoption rate was low. But soon, more and more households were converting to digital televisions. The transition is expected to be completed worldwide by mid to late 2010s.    Smart televisionEdit   The advent of digital television allowed innovations like smart TVs. A smart television, sometimes referred to as connected TV or hybrid TV, is a television set or set-top box with integrated Internet and Web 2.0 features, and is an example of technological convergence between computers and television sets and set-top boxes. Besides the traditional functions of television sets and set-top boxes provided through traditional broadcasting media, these devices can also provide Internet TV, online interactive media, over-the-top content, as well as on-demand streaming media, and home networking access. These TVs come pre-loaded with an operating system. Smart TV should not to be confused with Internet TV, IPTV or with Web TV. Internet television refers to the receiving of television content over the internet instead of by traditional systems - terrestrial, cable and satellite (although internet itself is received by these methods). Internet Protocol television (IPTV) is one of the emerging Internet television technology standards for use by television broadcasters. Web television (WebTV) is a term used for programs created by a wide variety of companies and individuals for broadcast on Internet TV. A first patent was filed in 1994 (and extended the following year) for an \"intelligent\" television system, linked with data processing systems, by means of a digital or analog network. Apart from being linked to data networks, one key point is its ability to automatically download necessary software routines, according to a user's demand, and process their needs. Major TV manufacturers have announced production of smart TVs only, for middle-end and high-end TVs in 2015. Smart TVs are expected to become dominant form of television by late 2010s.    3D televisionEdit   Stereoscopic 3D television was demonstrated for the first time on 10 August 1928, by John Logie Baird in his company's premises at 133 Long Acre, London. Baird pioneered a variety of 3D television systems using electro-mechanical and cathode-ray tube techniques. The first 3D TV was produced in 1935. The advent of digital television in 2000s greatly improved 3D TVs. Although 3D TV sets are quite popular for watching 3D home media such as on Blu-ray discs, 3D programming has largely failed to make inroads with the public. Many 3D television channels which started in early 2010s were shut down by the mid 2010s.    Broadcast SystemsEdit     Terrestrial televisionEdit   Programming is broadcast by television stations, sometimes called \"channels\", as stations are licensed by their governments to broadcast only over assigned channels in the television band. At first, terrestrial broadcasting was the only way television could be widely distributed, and because bandwidth was limited, i.e., there were only a small number of channels available, government regulation was the norm. In the U.S., the Federal Communications Commission (FCC) allowed stations to broadcast advertisements beginning in July 1941, but required public service programming commitments as a requirement for a license. By contrast, the United Kingdom chose a different route, imposing a television license fee on owners of television reception equipment to fund the British Broadcasting Corporation (BBC), which had public service as part of its Royal Charter. WRGB claims to be the world's oldest television station, tracing its roots to an experimental station founded on 13 January 1928, broadcasting from the General Electric factory in Schenectady, NY, under the call letters W2XB. It was popularly known as \"WGY Television\" after its sister radio station. Later in 1928, General Electric started a second facility, this one in New York City, which had the call letters W2XBS and which today is known as WNBC. The two stations were experimental in nature and had no regular programming, as receivers were operated by engineers within the company. The image of a Felix the Cat doll rotating on a turntable was broadcast for 2 hours every day for several years as new technology was being tested by the engineers. On 2 November 1936, the BBC began transmitting the world's first public regular high-definition service from the Victorian Alexandra Palace in north London. It therefore claims to be the birthplace of TV broadcasting as we know it today. With the widespread adoption of cable across the United States in the 1970s and 80s, terrestrial television broadcasts have been in decline; in 2013 it was estimated that about 7% of US households used an antenna. A slight increase in use began around 2010 due to switchover to digital terrestrial television broadcasts, which offered pristine image quality over very large areas, and offered an alternate to CATV for cord cutters. All other countries around the world are also in the process of either shutting down analog terrestrial television or switching over to digital terrestrial television.    Cable televisionEdit   Cable television is a system of broadcasting television programming to paying subscribers via radio frequency (RF) signals transmitted through coaxial cables or light pulses through fiber-optic cables. This contrasts with traditional terrestrial television, in which the television signal is transmitted over the air by radio waves and received by a television antenna attached to the television. FM radio programming, high-speed Internet, telephone service, and similar non-television services may also be provided through these cables. The abbreviation CATV is often used for cable television. It originally stood for Community Access Television or Community Antenna Television, from cable television's origins in 1948: in areas where over-the-air reception was limited by distance from transmitters or mountainous terrain, large \"community antennas\" were constructed, and cable was run from them to individual homes. The origins of cable broadcasting are even older as radio programming was distributed by cable in some European cities as far back as 1924. Earlier cable television was analog, but since 2000s all cable operators have switched to, or are in process of switching to, digital cable television.    Satellite televisionEdit   Satellite television is a system of supplying television programming using broadcast signals relayed from communication satellites. The signals are received via an outdoor parabolic reflector antenna usually referred to as a satellite dish and a low-noise block downconverter (LNB). A satellite receiver then decodes the desired television programme for viewing on a television set. Receivers can be external set-top boxes, or a built-in television tuner. Satellite television provides a wide range of channels and services, especially to geographic areas without terrestrial television or cable television. The most common method of reception is direct-broadcast satellite television (DBSTV), also known as \"direct to home\" (DTH). In DBSTV systems, signals are relayed from a direct broadcast satellite on the Ku wavelength and are completely digital. Satellite TV systems formerly used systems known as television receive-only. These systems received analog signals transmitted in the C-band spectrum from FSS type satellites, and required the use of large dishes. Consequently, these systems were nicknamed \"big dish\" systems, and were more expensive and less popular. The direct-broadcast satellite television signals were earlier analog signals and later digital signals, both of which require a compatible receiver. Digital signals may include high-definition television (HDTV). Some transmissions and channels are free-to-air or free-to-view, while many other channels are pay television requiring a subscription. In 1945, British science fiction writer Arthur C. Clarke proposed a world-wide communications system which would function by means of three satellites equally spaced apart in earth orbit. This was published in the October 1945 issue of the Wireless World magazine and won him the Franklin Institute's Stuart Ballantine Medal in 1963. The first satellite television signals from Europe to North America were relayed via the Telstar satellite over the Atlantic ocean on 23 July 1962. The signals were received and broadcast in North American and European countries and watched by over 100 million. Launched in 1962, the Relay 1 satellite was the first satellite to transmit television signals from the US to Japan. The first geosynchronous communication satellite, Syncom 2, was launched on 26 July 1963. The world's first commercial communications satellite, called Intelsat I and nicknamed \"Early Bird\", was launched into geosynchronous orbit on 6 April 1965. The first national network of television satellites, called Orbita, was created by the Soviet Union in October 1967, and was based on the principle of using the highly elliptical Molniya satellite for rebroadcasting and delivering of television signals to ground downlink stations. The first commercial North American satellite to carry television transmissions was Canada's geostationary Anik 1, which was launched on 9 November 1972. ATS-6, the world's first experimental educational and Direct Broadcast Satellite (DBS), was launched on 30 May 1974. It transmitted at 860 MHz using wideband FM modulation and had two sound channels. The transmissions were focused on the Indian subcontinent but experimenters were able to receive the signal in Western Europe using home constructed equipment that drew on UHF television design techniques already in use. The first in a series of Soviet geostationary satellites to carry Direct-To-Home television, Ekran 1, was launched on 26 October 1976. It used a 714 MHz UHF downlink frequency so that the transmissions could be received with existing UHF television technology rather than microwave technology.    Internet televisionEdit   Internet television (Internet TV) (or online television) is the digital distribution of television content via the Internet as opposed to traditional systems like terrestrial, cable and satellite, although internet itself is received by terrestrial, cable or satellite methods. Internet television is a general term that covers the delivery of television shows and other video content over the Internet by video streaming technology, typically by major traditional television broadcasters. Internet television should not to be confused with Smart TV, IPTV or with Web TV. Smart television refers to the TV set which has an inbuilt operating system. Internet Protocol television (IPTV) is one of the emerging Internet television technology standards for use by television broadcasters. Web television is a term used for programs created by a wide variety of companies and individuals for broadcast on Internet TV.    Television setsEdit   A television set, also called a television receiver, television, TV set, TV, or telly, is a device that combines a tuner, display, and speakers for the purpose of viewing television. Introduced in late 1920's in mechanical form, television sets became a popular consumer product after World War II in electronic form, using cathode ray tubes. The addition of color to broadcast television after 1953 further increased the popularity of television sets and an outdoor antenna became a common feature of suburban homes. The ubiquitous television set became the display device for the recorded media in the 1970s, such as VHS and later DVDs and Blu-ray Discs. Major TV manufacturers announced the discontinuation of CRT, DLP, plasma and even fluorescent-backlit LCDs by mid 2010s. Televisions since 2010s mostly use LEDs.   LEDs are expected to be gradually replaced by OLEDs in near future.    Display technologiesEdit    = DiskEdit =  Earliest systems employed a spinning disk to create and reproduce images. These usually had a low resolution and screen size and never became popular with the public.   = CRTEdit =  The cathode ray tube (CRT) is a vacuum tube containing one or more electron guns (a source of electrons or electron emitter) and a fluorescent screen used to view images. It has a means to accelerate and deflect the electron beam(s) onto the screen to create the images. The images may represent electrical waveforms (oscilloscope), pictures (television, computer monitor), radar targets or others. The CRT uses an evacuated glass envelope which is large, deep (i.e. long from front screen face to rear end), fairly heavy, and relatively fragile. As a matter of safety, the face is typically made of thick lead glass so as to be highly shatter-resistant and to block most X-ray emissions, particularly if the CRT is used in a consumer product. In television sets and computer monitors, the entire front area of the tube is scanned repetitively and systematically in a fixed pattern called a raster. An image is produced by controlling the intensity of each of the three electron beams, one for each additive primary color (red, green, and blue) with a video signal as a reference. In all modern CRT monitors and televisions, the beams are bent by magnetic deflection, a varying magnetic field generated by coils and driven by electronic circuits around the neck of the tube, although electrostatic deflection is commonly used in oscilloscopes, a type of diagnostic instrument.   = DLPEdit =  Digital Light Processing (DLP) is a type of projector technology that uses a digital micromirror device. Some DLPs have a TV tuner, which makes them a type of TV display. It was originally developed in 1987 by Dr. Larry Hornbeck of Texas Instruments. While the DLP imaging device was invented by Texas Instruments, the first DLP based projector was introduced by Digital Projection Ltd in 1997. Digital Projection and Texas Instruments were both awarded Emmy Awards in 1998 for the DLP projector technology. DLP is used in a variety of display applications from traditional static displays to interactive displays and also non-traditional embedded applications including medical, security, and industrial uses. DLP technology is used in DLP front projectors (standalone projection units for classrooms and business primarily), DLP rear projection television sets, and digital signs. It is also used in about 85% of digital cinema projection, and in additive manufacturing as a power source in some printers to cure resins into solid 3D objects.   = PlasmaEdit =  A plasma display panel (PDP) is a type of flat panel display common to large TV displays 30 inches (76 cm) or larger. They are called \"plasma\" displays because the technology utilizes small cells containing electrically charged ionized gases, or what are in essence chambers more commonly known as fluorescent lamps.   = LCDEdit =  Liquid-crystal-display televisions (LCD TV) are television sets that use LCD display technology to produce images. LCD televisions are much thinner and lighter than cathode ray tube (CRTs) of similar display size, and are available in much larger sizes (e.g., 90 inch diagonal). When manufacturing costs fell, this combination of features made LCDs practical for television receivers. LCD's come in two types: those using cold cathode fluorescent lamps, simply called LCDs and those using LED as backlight called as LEDs. In 2007, LCD televisions surpassed sales of CRT-based televisions worldwide for the first time, and their sales figures relative to other technologies accelerated. LCD TVs have quickly displaced the only major competitors in the large-screen market, the plasma display panel and rear-projection television. In mid 2010s LCDs especially LEDs became, by far, the most widely produced and sold television display type. LCDs also have disadvantages. Other technologies address these weaknesses, including OLEDs, FED and SED, but as of 2014 none of these have entered widespread production.   = OLEDEdit =  An OLED (organic light-emitting diode) is a light-emitting diode (LED) in which the emissive electroluminescent layer is a film of organic compound which emits light in response to an electric current. This layer of organic semiconductor is situated between two electrodes. Generally, at least one of these electrodes is transparent. OLEDs are used to create digital displays in devices such as television screens. It is also used for computer monitors, portable systems such as mobile phones, handheld games consoles and PDAs. There are two main families of OLED: those based on small molecules and those employing polymers. Adding mobile ions to an OLED creates a light-emitting electrochemical cell or LEC, which has a slightly different mode of operation. OLED displays can use either passive-matrix (PMOLED) or active-matrix (AMOLED) addressing schemes. Active-matrix OLEDs require a thin-film transistor backplane to switch each individual pixel on or off, but allow for higher resolution and larger display sizes. An OLED display works without a backlight. Thus, it can display deep black levels and can be thinner and lighter than a liquid crystal display (LCD). In low ambient light conditions such as a dark room an OLED screen can achieve a higher contrast ratio than an LCD, whether the LCD uses cold cathode fluorescent lamps or LED backlight. OLEDs are expected to replace other forms of display in near future.    Display resolutionEdit    = LDEdit =  Low-definition television or LDTV refers to television systems that have a lower screen resolution than standard-definition television systems such 240p (320*240). It is used in handheld television.   = SDEdit =  Standard-definition television or SDTV refers to two different resolutions: 576i, with 576 interlaced lines of resolution, derived from the European-developed PAL and SECAM systems; and 480i based on the American National Television System Committee NTSC system.   = HDEdit =  High-definition television (HDTV) provides a resolution that is substantially higher than that of standard-definition television. HDTV may be transmitted in various formats: 1080p: 1920×1080p: 2,073,600 pixels (~2.07 megapixels) per frame 1080i: 1920×1080i: 1,036,800 pixels (~1.04 MP) per field or 2,073,600 pixels (~2.07 MP) per frame A non-standard CEA resolution exists in some countries such as 1440×1080i: 777,600 pixels (~0.78 MP) per field or 1,555,200 pixels (~1.56 MP) per frame  720p: 1280×720p: 921,600 pixels (~0.92 MP) per frame   = UHDEdit =  Ultra-high-definition television (also known as Super Hi-Vision, Ultra HD television, UltraHD, UHDTV, or UHD) includes 4K UHD (2160p) and 8K UHD (4320p), which are two digital video formats proposed by NHK Science & Technology Research Laboratories and defined and approved by the International Telecommunication Union (ITU). The Consumer Electronics Association announced on 17 October 2012, that \"Ultra High Definition\", or \"Ultra HD\", would be used for displays that have an aspect ratio of at least 16:9 and at least one digital input capable of carrying and presenting native video at a minimum resolution of 3840×2160 pixels.    SalesEdit  North American consumers purchase a new television set on average every seven years, and the average household owns 2.8 televisions. As of 2011, 48 million are sold each year at an average price of $460 and size of 38 in (97 cm). Note: Vendor shipments are branded shipments and exclude OEM sales for all vendors    ContentEdit     ProgrammingEdit   Getting TV programming shown to the public can happen in many different ways. After production, the next step is to market and deliver the product to whichever markets are open to using it. This typically happens on two levels: Original Run or First Run: a producer creates a program of one or multiple episodes and shows it on a station or network which has either paid for the production itself or to which a license has been granted by the television producers to do the same. Broadcast syndication: this is the terminology rather broadly used to describe secondary programming usages (beyond original run). It includes secondary runs in the country of first issue but also international usage which may not be managed by the originating producer. In many cases, other companies, TV stations, or individuals are engaged to do the syndication work, in other words, to sell the product into the markets they are allowed to sell into by contract from the copyright holders, in most cases the producers. First-run programming is increasing on subscription services outside the US, but few domestically produced programs are syndicated on domestic free-to-air (FTA) elsewhere. This practice is increasing however, generally on digital-only FTA channels or with subscriber-only first-run material appearing on FTA. Unlike the US, repeat FTA screenings of an FTA network program usually only occur on that network. Also, affiliates rarely buy or produce non-network programming that is not centered on local programming.    GenresEdit  Television genres include a broad range of programming types that entertain, inform, and educate viewers. The most expensive entertainment genres to produce are usually dramas and dramatic miniseries. However, other genres, such as historical Western genres, may also have high production costs. Popular culture entertainment genres include action-oriented shows such as police, crime, detective dramas, horror, or thriller shows. As well, there are also other variants of the drama genre, such as medical dramas and daytime soap operas. Science fiction shows can fall into either the drama or action category, depending on whether they emphasize philosophical questions or high adventure. Comedy is a popular genre which includes situation comedy (sitcom) and animated shows for the adult demographic such as South Park. The least expensive forms of entertainment programming genres are game shows, talk shows, variety shows, and reality television. Game shows feature contestants answering questions and solving puzzles to win prizes. Talk shows contain interviews with film, television, and music celebrities and public figures. Variety shows feature a range of musical performers and other entertainers, such as comedians and magicians, introduced by a host or Master of Ceremonies. There is some crossover between some talk shows and variety shows because leading talk shows often feature performances by bands, singers, comedians, and other performers in between the interview segments. Reality TV shows \"regular\" people (i.e., not actors) facing unusual challenges or experiences ranging from arrest by police officers (COPS) to weight loss (The Biggest Loser). A variant version of reality shows depicts celebrities doing mundane activities such as going about their everyday life (The Osbournes, Snoop Dogg's Father Hood) or doing manual labor (The Simple Life). Fictional television programs that some television scholars and broadcasting advocacy groups argue are \"quality television\" include series such as Twin Peaks and The Sopranos. Kristin Thompson argues that some of these television series exhibit traits also found in art films, such as psychological realism, narrative complexity, and ambiguous plotlines. Nonfiction television programs that some television scholars and broadcasting advocacy groups argue are \"quality television\" include a range of serious, noncommercial programming aimed at a niche audience, such as documentaries and public affairs shows.    FundingEdit   Around the globe, broadcast TV is financed by government, advertising, licensing (a form of tax), subscription, or any combination of these. To protect revenues, subscription TV channels are usually encrypted to ensure that only subscribers receive the decryption codes to see the signal. Unencrypted channels are known as free to air or FTA. In 2009, the global TV market represented 1,217.2 million TV households with at least one TV and total revenues of 268.9 billion EUR (declining 1.2% compared to 2008). North America had the biggest TV revenue market share with 39% followed by Europe (31%), Asia-Pacific (21%), Latin America (8%), and Africa and the Middle East (2%). Globally, the different TV revenue sources divide into 45%-50% TV advertising revenues, 40%-45% subscription fees and 10% public funding.   = AdvertisingEdit =  TV's broad reach makes it a powerful and attractive medium for advertisers. Many TV networks and stations sell blocks of broadcast time to advertisers (\"sponsors\") to fund their programming. Television advertisements (variously called a television commercial, commercial or ad in American English, and known in British English as an advert) is a span of television programming produced and paid for by an organization, which conveys a message, typically to market a product or service. Advertising revenue provides a significant portion of the funding for most privately owned television networks. The vast majority of television advertisements today consist of brief advertising spots, ranging in length from a few seconds to several minutes (as well as program-length infomercials). Advertisements of this sort have been used to promote a wide variety of goods, services and ideas since the beginning of television.  The effects of television advertising upon the viewing public (and the effects of mass media in general) have been the subject of philosophical discourse by such luminaries as Marshall McLuhan. The viewership of television programming, as measured by companies such as Nielsen Media Research, is often used as a metric for television advertisement placement, and consequently, for the rates charged to advertisers to air within a given network, television program, or time of day (called a \"daypart\"). In many countries, including the United States, television campaign advertisements are considered indispensable for a political campaign. In other countries, such as France, political advertising on television is heavily restricted, while some countries, such as Norway, completely ban political advertisements. The first official, paid television advertisement was broadcast in the United States on July 1, 1941 over New York station WNBT (now WNBC) before a baseball game between the Brooklyn Dodgers and Philadelphia Phillies. The announcement for Bulova watches, for which the company paid anywhere from $4.00 to $9.00 (reports vary), displayed a WNBT test pattern modified to look like a clock with the hands showing the time. The Bulova logo, with the phrase \"Bulova Watch Time\", was shown in the lower right-hand quadrant of the test pattern while the second hand swept around the dial for one minute. The first TV ad broadcast in the UK was on ITV on 22 September 1955, advertising Gibbs SR toothpaste. The first TV ad broadcast in Asia was on Nippon Television in Tokyo on August 28, 1953, advertising Seikosha (now Seiko), which also displayed a clock with the current time.    United StatesEdit  Since inception in the US in 1941, television commercials have become one of the most effective, persuasive, and popular methods of selling products of many sorts, especially consumer goods. During the 1940s and into the 1950s, programs were hosted by single advertisers. This, in turn, gave great creative license to the advertisers over the content of the show. Perhaps due to the quiz show scandals in the 1950s, networks shifted to the magazine concept, introducing advertising breaks with multiple advertisers. US advertising rates are determined primarily by Nielsen ratings. The time of the day and popularity of the channel determine how much a TV commercial can cost. For example, it can cost approximately $750,000 for a 30-second block of commercial time during the highly popular American Idol, while the same amount of time for the Super Bowl can cost several million dollars. Conversely, lesser-viewed time slots, such as early mornings and weekday afternoons, are often sold in bulk to producers of infomercials at far lower rates. In recent years, the paid program or infomercial has become common, usually in lengths of 30 minutes or one hour. Some drug companies and other businesses have even created \"news\" items for broadcast, known in the industry as video news releases, paying program directors to use them. Some TV programs also deliberately place products into their shows as advertisements, a practice started in feature films and known as product placement. For example, a character could be drinking a certain kind of soda, going to a particular chain restaurant, or driving a certain make of car. (This is sometimes very subtle, with shows having vehicles provided by manufacturers for low cost in exchange as a product placement). Sometimes, a specific brand or trade mark, or music from a certain artist or group, is used. (This excludes guest appearances by artists who perform on the show.)    United KingdomEdit  The TV regulator oversees TV advertising in the United Kingdom. Its restrictions have applied since the early days of commercially funded TV. Despite this, an early TV mogul, Roy Thomson, likened the broadcasting licence as being a \"licence to print money\". Restrictions mean that the big three national commercial TV channels: ITV, Channel 4, and Channel 5 can show an average of only seven minutes of advertising per hour (eight minutes in the peak period). Other broadcasters must average no more than nine minutes (twelve in the peak). This means that many imported TV shows from the US have unnatural pauses where the UK company does not utilize the narrative breaks intended for more frequent US advertising. Advertisements must not be inserted in the course of certain specific proscribed types of programs which last less than half an hour in scheduled duration; this list includes any news or current affairs programs, documentaries, and programs for children; additionally, advertisements may not be carried in a program designed and broadcast for reception in schools or in any religious broadcasting service or other devotional program or during a formal Royal ceremony or occasion. There also must be clear demarcations in time between the programs and the advertisements. The BBC, being strictly non-commercial, is not allowed to show advertisements on television in the UK, although it has many advertising-funded channels abroad. The majority of its budget comes from television license fees (see below) and broadcast syndication, the sale of content to other broadcasters.    IrelandEdit  The Broadcasting Commission of Ireland (BCI) (Irish: Coimisiún Craolacháin na hÉireann) oversees advertising on television and radio within Ireland for both private and state-owned broadcasters. There are some restrictions based on advertising, especially in relation to the advertising of alcohol. Such advertisements are prohibited until after 7 pm. Broadcasters in Ireland adhere to broadcasting legislation implemented by the Broadcasting Commission of Ireland and the European Union. Sponsorship of current affairs programming is prohibited at all times. As of 1 October 2009, the responsibilities held by the BCI are gradually being transferred to the Broadcasting Authority of Ireland.   = SubscriptionEdit = Some TV channels are partly funded from subscriptions; therefore, the signals are encrypted during broadcast to ensure that only the paying subscribers have access to the decryption codes to watch pay television or specialty channels. Most subscription services are also funded by advertising.   = Taxation or licenseEdit = Television services in some countries may be funded by a television licence or a form of taxation, which means that advertising plays a lesser role or no role at all. For example, some channels may carry no advertising at all and some very little, including: Australia (ABC) Japan (NHK) Norway (NRK) Sweden (SVT) United Kingdom (BBC) United States (PBS) Denmark (DR) The BBC carries no television advertising on its UK channels and is funded by an annual television licence paid by premises receiving live TV broadcasts. Currently, it is estimated that approximately 26.8 million UK private domestic households own televisions, with approximately 25 million TV licences in all premises in force as of 2010. This television license fee is set by the government, but the BBC is not answerable to or controlled by the government. The two main BBC TV channels are watched by almost 90% of the population each week and overall have 27% share of total viewing, despite the fact that 85% of homes are multichannel, with 42% of these having access to 200 free to air channels via satellite and another 43% having access to 30 or more channels via Freeview. The licence that funds the seven advertising-free BBC TV channels currently costs £139.50 a year (about US$215) regardless of the number of TV sets owned. When the same sporting event has been presented on both BBC and commercial channels, the BBC always attracts the lion's share of the audience, indicating that viewers prefer to watch TV uninterrupted by advertising. Other than internal promotional material, the Australian Broadcasting Corporation (ABC) carries no advertising; it is banned under the ABC Act 1983. The ABC receives its funding from the Australian government every three years. In the 2008/09 federal budget, the ABC received A$1.13 billion. The funds provide for the ABC's television, radio, online, and international outputs. The ABC also receives funds from its many ABC shops across Australia. Although funded by the Australian government, the editorial independence of the ABC is ensured through law. In France, government-funded channels carry advertisements, yet those who own television sets have to pay an annual tax (\"la redevance audiovisuelle\"). In Japan, NHK is paid for by license fees (known in Japanese as reception fee (受信料, Jushinryō)). The broadcast law that governs NHK's funding stipulates that any television equipped to receive NHK is required to pay. The fee is standardized, with discounts for office workers and students who commute, as well a general discount for residents of Okinawa prefecture.    Social aspectsEdit   Television has played a pivotal role in the socialization of the 20th and 21st centuries. There are many aspects of television that can be addressed, including negative issues such as media violence. Current research is discovering that individuals suffering from social isolation can employ television to create what is termed a parasocial or faux relationship with characters from their favorite television shows and movies as a way of deflecting feelings of loneliness and social deprivation. Several studies have found that educational television has many advantages. The Media Awareness Network, explains in its article \"The Good Things about Television\" that television can be a very powerful and effective learning tool for children if used wisely.    Environmental aspectsEdit  With high lead content in CRTs and the rapid diffusion of new flat-panel display technologies, some of which (LCDs) use lamps which contain mercury, there is growing concern about electronic waste from discarded televisions. Related occupational health concerns exist, as well, for disassemblers removing copper wiring and other materials from CRTs. Further environmental concerns related to television design and use relate to the devices' increasing electrical energy requirements.    See alsoEdit     ReferencesEdit     Further readingEdit  Albert Abramson, The History of Television, 1942 to 2000, Jefferson, NC, and London, McFarland, 2003, ISBN 0-7864-1220-8. Pierre Bourdieu, On Television, The New Press, 2001. Tim Brooks and Earle March, The Complete Guide to Prime Time Network and Cable TV Shows, 8th ed., Ballantine, 2002. Jacques Derrida and Bernard Stiegler, Echographies of Television, Polity Press, 2002. David E. Fisher and Marshall J. Fisher, Tube: the Invention of Television, Counterpoint, Washington, DC, 1996, ISBN 1-887178-17-1. Steven Johnson, Everything Bad is Good for You: How Today's Popular Culture Is Actually Making Us Smarter, New York, Riverhead (Penguin), 2005, 2006, ISBN 1-59448-194-6. Leggett, Julian (April 1941). \"Television in Color\". Popular Mechanics (Chicago). Retrieved 2014-12-07.  Jerry Mander, Four Arguments for the Elimination of Television, Perennial, 1978. Jerry Mander, In the Absence of the Sacred, Sierra Club Books, 1992, ISBN 0-87156-509-9. Neil Postman, Amusing Ourselves to Death: Public Discourse in the Age of Show Business, New York, Penguin US, 1985, ISBN 0-670-80454-1. Evan I. Schwartz, The Last Lone Inventor: A Tale of Genius, Deceit, and the Birth of Television, New York, Harper Paperbacks, 2003, ISBN 0-06-093559-6. Beretta E. Smith-Shomade, Shaded Lives: African-American Women and Television, Rutgers University Press, 2002. Alan Taylor, We, the Media: Pedagogic Intrusions into US Mainstream Film and Television News Broadcasting Rhetoric, Peter Lang, 2005, ISBN 3-631-51852-8. Amanda D. Lotz, The Television Will Be Revolutionized, New York University Press, ISBN 978-0814752203    External linksEdit  National Association of Broadcasters Association of Commercial Television in Europe The Encyclopedia of Television at the Museum of Broadcast Communications Television's History - The First 75 Years Collection Profile - Television at the Canada Science and Technology Museum The Evolution of TV, A Brief History of TV Technology in Japan - NHK (Japan Broadcasting Corporation) Worldwide Television Standards Television at DMOZ","label":"foo"},{"text":"A computer is a general-purpose device that can be programmed to carry out a set of arithmetic or logical operations automatically. Since a sequence of operations can be readily changed, the computer can solve more than one kind of problem. Conventionally, a computer consists of at least one processing element, typically a central processing unit (CPU), and some form of memory. The processing element carries out arithmetic and logic operations, and a sequencing and control unit can change the order of operations in response to stored information. Peripheral devices allow information to be retrieved from an external source, and the result of operations saved and retrieved. Mechanical analog computers started appearing in the first century and were later used in the medieval era for astronomical calculations. In World War II, mechanical analog computers were used for specialized military applications such as calculating torpedo aiming. During this time the first electronic digital computers were developed. Originally they were the size of a large room, consuming as much power as several hundred modern personal computers (PCs). Modern computers based on integrated circuits are millions to billions of times more capable than the early machines, and occupy a fraction of the space. Computers are small enough to fit into mobile devices, and mobile computers can be powered by small batteries. Personal computers in their various forms are icons of the Information Age and are generally considered as \"computers\". However, the embedded computers found in many devices from MP3 players to fighter aircraft and from electronic toys to industrial robots are the most numerous.    Etymology  The first known use of the word \"computer\" was in 1613 in a book called The Yong Mans Gleanings by English writer Richard Braithwait: \"I haue read the truest computer of Times, and the best Arithmetician that euer breathed, and he reduceth thy dayes into a short number.\" It referred to a person who carried out calculations, or computations. The word continued with the same meaning until the middle of the 20th century. From the end of the 19th century the word began to take on its more familiar meaning, a machine that carries out computations.    History     Pre-twentieth century   Devices have been used to aid computation for thousands of years, mostly using one-to-one correspondence with fingers. The earliest counting device was probably a form of tally stick. Later record keeping aids throughout the Fertile Crescent included calculi (clay spheres, cones, etc.) which represented counts of items, probably livestock or grains, sealed in hollow unbaked clay containers. The use of counting rods is one example.  The abacus was initially used for arithmetic tasks. The Roman abacus was used in Babylonia as early as 2400 BC. Since then, many other forms of reckoning boards or tables have been invented. In a medieval European counting house, a checkered cloth would be placed on a table, and markers moved around on it according to certain rules, as an aid to calculating sums of money.  The Antikythera mechanism is believed to be the earliest mechanical analog \"computer\", according to Derek J. de Solla Price. It was designed to calculate astronomical positions. It was discovered in 1901 in the Antikythera wreck off the Greek island of Antikythera, between Kythera and Crete, and has been dated to circa 100 BC. Devices of a level of complexity comparable to that of the Antikythera mechanism would not reappear until a thousand years later. Many mechanical aids to calculation and measurement were constructed for astronomical and navigation use. The planisphere was a star chart invented by Abū Rayhān al-Bīrūnī in the early 11th century. The astrolabe was invented in the Hellenistic world in either the 1st or 2nd centuries BC and is often attributed to Hipparchus. A combination of the planisphere and dioptra, the astrolabe was effectively an analog computer capable of working out several different kinds of problems in spherical astronomy. An astrolabe incorporating a mechanical calendar computer and gear-wheels was invented by Abi Bakr of Isfahan, Persia in 1235. Abū Rayhān al-Bīrūnī invented the first mechanical geared lunisolar calendar astrolabe, an early fixed-wired knowledge processing machine with a gear train and gear-wheels, circa 1000 AD. The sector, a calculating instrument used for solving problems in proportion, trigonometry, multiplication and division, and for various functions, such as squares and cube roots, was developed in the late 16th century and found application in gunnery, surveying and navigation. The planimeter was a manual instrument to calculate the area of a closed figure by tracing over it with a mechanical linkage.  The slide rule was invented around 1620–1630, shortly after the publication of the concept of the logarithm. It is a hand-operated analog computer for doing multiplication and division. As slide rule development progressed, added scales provided reciprocals, squares and square roots, cubes and cube roots, as well as transcendental functions such as logarithms and exponentials, circular and hyperbolic trigonometry and other functions. Aviation is one of the few fields where slide rules are still in widespread use, particularly for solving time–distance problems in light aircraft. To save space and for ease of reading, these are typically circular devices rather than the classic linear slide rule shape. A popular example is the E6B. In the 1770s Pierre Jaquet-Droz, a Swiss watchmaker, built a mechanical doll (automata) that could write holding a quill pen. By switching the number and order of its internal wheels different letters, and hence different messages, could be produced. In effect, it could be mechanically \"programmed\" to read instructions. Along with two other complex machines, the doll is at the Musée d'Art et d'Histoire of Neuchâtel, Switzerland, and still operates. The tide-predicting machine invented by Sir William Thomson in 1872 was of great utility to navigation in shallow waters. It used a system of pulleys and wires to automatically calculate predicted tide levels for a set period at a particular location. The differential analyser, a mechanical analog computer designed to solve differential equations by integration, used wheel-and-disc mechanisms to perform the integration. In 1876 Lord Kelvin had already discussed the possible construction of such calculators, but he had been stymied by the limited output torque of the ball-and-disk integrators. In a differential analyzer, the output of one integrator drove the input of the next integrator, or a graphing output. The torque amplifier was the advance that allowed these machines to work. Starting in the 1920s, Vannevar Bush and others developed mechanical differential analyzers.    First general-purpose computing device   Charles Babbage, an English mechanical engineer and polymath, originated the concept of a programmable computer. Considered the \"father of the computer\", he conceptualized and invented the first mechanical computer in the early 19th century. After working on his revolutionary difference engine, designed to aid in navigational calculations, in 1833 he realized that a much more general design, an Analytical Engine, was possible. The input of programs and data was to be provided to the machine via punched cards, a method being used at the time to direct mechanical looms such as the Jacquard loom. For output, the machine would have a printer, a curve plotter and a bell. The machine would also be able to punch numbers onto cards to be read in later. The Engine incorporated an arithmetic logic unit, control flow in the form of conditional branching and loops, and integrated memory, making it the first design for a general-purpose computer that could be described in modern terms as Turing-complete. The machine was about a century ahead of its time. All the parts for his machine had to be made by hand — this was a major problem for a device with thousands of parts. Eventually, the project was dissolved with the decision of the British Government to cease funding. Babbage's failure to complete the analytical engine can be chiefly attributed to difficulties not only of politics and financing, but also to his desire to develop an increasingly sophisticated computer and to move ahead faster than anyone else could follow. Nevertheless, his son, Henry Babbage, completed a simplified version of the analytical engine's computing unit (the mill) in 1888. He gave a successful demonstration of its use in computing tables in 1906.    Later Analog computers   During the first half of the 20th century, many scientific computing needs were met by increasingly sophisticated analog computers, which used a direct mechanical or electrical model of the problem as a basis for computation. However, these were not programmable and generally lacked the versatility and accuracy of modern digital computers. The first modern analog computer was a tide-predicting machine, invented by Sir William Thomson in 1872. The differential analyser, a mechanical analog computer designed to solve differential equations by integration using wheel-and-disc mechanisms, was conceptualized in 1876 by James Thomson, the brother of the more famous Lord Kelvin. The art of mechanical analog computing reached its zenith with the differential analyzer, built by H. L. Hazen and Vannevar Bush at MIT starting in 1927. This built on the mechanical integrators of James Thomson and the torque amplifiers invented by H. W. Nieman. A dozen of these devices were built before their obsolescence became obvious. By the 1950s the success of digital electronic computers had spelled the end for most analog computing machines, but analog computers remain in use in some specialized applications such as education (control systems) and aircraft (slide rule).    Digital computer development  The principle of the modern computer was first described by mathematician and pioneering computer scientist Alan Turing, who set out the idea in his seminal 1936 paper, On Computable Numbers. Turing reformulated Kurt Gödel's 1931 results on the limits of proof and computation, replacing Gödel's universal arithmetic-based formal language with the formal and simple hypothetical devices that became known as Turing machines. He proved that some such machine would be capable of performing any conceivable mathematical computation if it were representable as an algorithm. He went on to prove that there was no solution to the Entscheidungsproblem by first showing that the halting problem for Turing machines is undecidable: in general, it is not possible to decide algorithmically whether a given Turing machine will ever halt. He also introduced the notion of a 'Universal Machine' (now known as a Universal Turing machine), with the idea that such a machine could perform the tasks of any other machine, or in other words, it is provably capable of computing anything that is computable by executing a program stored on tape, allowing the machine to be programmable. Von Neumann acknowledged that the central concept of the modern computer was due to this paper. Turing machines are to this day a central object of study in theory of computation. Except for the limitations imposed by their finite memory stores, modern computers are said to be Turing-complete, which is to say, they have algorithm execution capability equivalent to a universal Turing machine.   = Electromechanical = By 1938 the United States Navy had developed an electromechanical analog computer small enough to use aboard a submarine. This was the Torpedo Data Computer, which used trigonometry to solve the problem of firing a torpedo at a moving target. During World War II similar devices were developed in other countries as well.  Early digital computers were electromechanical; electric switches drove mechanical relays to perform the calculation. These devices had a low operating speed and were eventually superseded by much faster all-electric computers, originally using vacuum tubes. The Z2, created by German engineer Konrad Zuse in 1939, was one of the earliest examples of an electromechanical relay computer. In 1941, Zuse followed his earlier machine up with the Z3, the world's first working electromechanical programmable, fully automatic digital computer. The Z3 was built with 2000 relays, implementing a 22 bit word length that operated at a clock frequency of about 5–10 Hz. Program code was supplied on punched film while data could be stored in 64 words of memory or supplied from the keyboard. It was quite similar to modern machines in some respects, pioneering numerous advances such as floating point numbers. Replacement of the hard-to-implement decimal system (used in Charles Babbage's earlier design) by the simpler binary system meant that Zuse's machines were easier to build and potentially more reliable, given the technologies available at that time. The Z3 was probably a complete Turing machine.   = Vacuum tubes and digital electronic circuits = Purely electronic circuit elements soon replaced their mechanical and electromechanical equivalents, at the same time that digital calculation replaced analog. The engineer Tommy Flowers, working at the Post Office Research Station in London in the 1930s, began to explore the possible use of electronics for the telephone exchange. Experimental equipment that he built in 1934 went into operation 5 years later, converting a portion of the telephone exchange network into an electronic data processing system, using thousands of vacuum tubes. In the US, John Vincent Atanasoff and Clifford E. Berry of Iowa State University developed and tested the Atanasoff–Berry Computer (ABC) in 1942, the first \"automatic electronic digital computer\". This design was also all-electronic and used about 300 vacuum tubes, with capacitors fixed in a mechanically rotating drum for memory.  During World War II, the British at Bletchley Park achieved a number of successes at breaking encrypted German military communications. The German encryption machine, Enigma, was first attacked with the help of the electro-mechanical bombes. To crack the more sophisticated German Lorenz SZ 40/42 machine, used for high-level Army communications, Max Newman and his colleagues commissioned Flowers to build the Colossus. He spent eleven months from early February 1943 designing and building the first Colossus. After a functional test in December 1943, Colossus was shipped to Bletchley Park, where it was delivered on 18 January 1944 and attacked its first message on 5 February. Colossus was the world's first electronic digital programmable computer. It used a large number of valves (vacuum tubes). It had paper-tape input and was capable of being configured to perform a variety of boolean logical operations on its data, but it was not Turing-complete. Nine Mk II Colossi were built (The Mk I was converted to a Mk II making ten machines in total). Colossus Mark I contained 1500 thermionic valves (tubes), but Mark II with 2400 valves, was both 5 times faster and simpler to operate than Mark 1, greatly speeding the decoding process.  The US-built ENIAC (Electronic Numerical Integrator and Computer) was the first electronic programmable computer built in the US. Although the ENIAC was similar to the Colossus it was much faster and more flexible. It was unambiguously a Turing-complete device and could compute any problem that would fit into its memory. Like the Colossus, a \"program\" on the ENIAC was defined by the states of its patch cables and switches, a far cry from the stored program electronic machines that came later. Once a program was written, it had to be mechanically set into the machine with manual resetting of plugs and switches. It combined the high speed of electronics with the ability to be programmed for many complex problems. It could add or subtract 5000 times a second, a thousand times faster than any other machine. It also had modules to multiply, divide, and square root. High speed memory was limited to 20 words (about 80 bytes). Built under the direction of John Mauchly and J. Presper Eckert at the University of Pennsylvania, ENIAC's development and construction lasted from 1943 to full operation at the end of 1945. The machine was huge, weighing 30 tons, using 200 kilowatts of electric power and contained over 18,000 vacuum tubes, 1,500 relays, and hundreds of thousands of resistors, capacitors, and inductors.   = Stored programs =  Early computing machines had fixed programs. Changing its function required the re-wiring and re-structuring of the machine. With the proposal of the stored-program computer this changed. A stored-program computer includes by design an instruction set and can store in memory a set of instructions (a program) that details the computation. The theoretical basis for the stored-program computer was laid by Alan Turing in his 1936 paper. In 1945 Turing joined the National Physical Laboratory and began work on developing an electronic stored-program digital computer. His 1945 report ‘Proposed Electronic Calculator’ was the first specification for such a device. John von Neumann at the University of Pennsylvania, also circulated his First Draft of a Report on the EDVAC in 1945.  The Manchester Small-Scale Experimental Machine, nicknamed Baby, was the world's first stored-program computer. It was built at the Victoria University of Manchester by Frederic C. Williams, Tom Kilburn and Geoff Tootill, and ran its first program on 21 June 1948. It was designed as a testbed for the Williams tube the first random-access digital storage device. Although the computer was considered \"small and primitive\" by the standards of its time, it was the first working machine to contain all of the elements essential to a modern electronic computer. As soon as the SSEM had demonstrated the feasibility of its design, a project was initiated at the university to develop it into a more usable computer, the Manchester Mark 1. The Mark 1 in turn quickly became the prototype for the Ferranti Mark 1, the world's first commercially available general-purpose computer. Built by Ferranti, it was delivered to the University of Manchester in February 1951. At least seven of these later machines were delivered between 1953 and 1957, one of them to Shell labs in Amsterdam. In October 1947, the directors of British catering company J. Lyons & Company decided to take an active role in promoting the commercial development of computers. The LEO I computer became operational in April 1951  and ran the world's first regular routine office computer job.   = Transistors =  The bipolar transistor was invented in 1947. From 1955 onwards transistors replaced vacuum tubes in computer designs, giving rise to the \"second generation\" of computers. Compared to vacuum tubes, transistors have many advantages: they are smaller, and require less power than vacuum tubes, so give off less heat. Silicon junction transistors were much more reliable than vacuum tubes and had longer, indefinite, service life. Transistorized computers could contain tens of thousands of binary logic circuits in a relatively compact space. At the University of Manchester, a team under the leadership of Tom Kilburn designed and built a machine using the newly developed transistors instead of valves. Their first transistorised computer and the first in the world, was operational by 1953, and a second version was completed there in April 1955. However, the machine did make use of valves to generate its 125 kHz clock waveforms and in the circuitry to read and write on its magnetic drum memory, so it was not the first completely transistorized computer. That distinction goes to the Harwell CADET of 1955, built by the electronics division of the Atomic Energy Research Establishment at Harwell.   = Integrated circuits = The next great advance in computing power came with the advent of the integrated circuit. The idea of the integrated circuit was first conceived by a radar scientist working for the Royal Radar Establishment of the Ministry of Defence, Geoffrey W.A. Dummer. Dummer presented the first public description of an integrated circuit at the Symposium on Progress in Quality Electronic Components in Washington, D.C. on 7 May 1952. The first practical ICs were invented by Jack Kilby at Texas Instruments and Robert Noyce at Fairchild Semiconductor. Kilby recorded his initial ideas concerning the integrated circuit in July 1958, successfully demonstrating the first working integrated example on 12 September 1958. In his patent application of 6 February 1959, Kilby described his new device as \"a body of semiconductor material ... wherein all the components of the electronic circuit are completely integrated\". Noyce also came up with his own idea of an integrated circuit half a year later than Kilby. His chip solved many practical problems that Kilby's had not. Produced at Fairchild Semiconductor, it was made of silicon, whereas Kilby's chip was made of germanium. This new development heralded an explosion in the commercial and personal use of computers and led to the invention of the microprocessor. While the subject of exactly which device was the first microprocessor is contentious, partly due to lack of agreement on the exact definition of the term \"microprocessor\", it is largely undisputed that the first single-chip microprocessor was the Intel 4004, designed and realized by Ted Hoff, Federico Faggin, and Stanley Mazor at Intel.    Mobile computers become dominant  With the continued miniaturization of computing resources, and advancements in portable battery life, portable computers grew in popularity in the 2000s. The same developments that spurred the growth of laptop computers and other portable computers allowed manufacturers to integrate computing resources into cellular phones. These so-called smartphones and tablets run on a variety of operating systems and have become the dominant computing device on the market, with manufacturers reporting having shipped an estimated 237 million devices in 2Q 2013.    Programs  The defining feature of modern computers which distinguishes them from all other machines is that they can be programmed. That is to say that some type of instructions (the program) can be given to the computer, and it will process them. Modern computers based on the von Neumann architecture often have machine code in the form of an imperative programming language. In practical terms, a computer program may be just a few instructions or extend to many millions of instructions, as do the programs for word processors and web browsers for example. A typical modern computer can execute billions of instructions per second (gigaflops) and rarely makes a mistake over many years of operation. Large computer programs consisting of several million instructions may take teams of programmers years to write, and due to the complexity of the task almost certainly contain errors.    Stored program architecture   This section applies to most common RAM machine-based computers. In most cases, computer instructions are simple: add one number to another, move some data from one location to another, send a message to some external device, etc. These instructions are read from the computer's memory and are generally carried out (executed) in the order they were given. However, there are usually specialized instructions to tell the computer to jump ahead or backwards to some other place in the program and to carry on executing from there. These are called \"jump\" instructions (or branches). Furthermore, jump instructions may be made to happen conditionally so that different sequences of instructions may be used depending on the result of some previous calculation or some external event. Many computers directly support subroutines by providing a type of jump that \"remembers\" the location it jumped from and another instruction to return to the instruction following that jump instruction. Program execution might be likened to reading a book. While a person will normally read each word and line in sequence, they may at times jump back to an earlier place in the text or skip sections that are not of interest. Similarly, a computer may sometimes go back and repeat the instructions in some section of the program over and over again until some internal condition is met. This is called the flow of control within the program and it is what allows the computer to perform tasks repeatedly without human intervention. Comparatively, a person using a pocket calculator can perform a basic arithmetic operation such as adding two numbers with just a few button presses. But to add together all of the numbers from 1 to 1,000 would take thousands of button presses and a lot of time, with a near certainty of making a mistake. On the other hand, a computer may be programmed to do this with just a few simple instructions. The following example is written in the MIPS assembly language:  Once told to run this program, the computer will perform the repetitive addition task without further human intervention. It will almost never make a mistake and a modern PC can complete the task in a fraction of a second.    Machine code  In most computers, individual instructions are stored as machine code with each instruction being given a unique number (its operation code or opcode for short). The command to add two numbers together would have one opcode; the command to multiply them would have a different opcode, and so on. The simplest computers are able to perform any of a handful of different instructions; the more complex computers have several hundred to choose from, each with a unique numerical code. Since the computer's memory is able to store numbers, it can also store the instruction codes. This leads to the important fact that entire programs (which are just lists of these instructions) can be represented as lists of numbers and can themselves be manipulated inside the computer in the same way as numeric data. The fundamental concept of storing programs in the computer's memory alongside the data they operate on is the crux of the von Neumann, or stored program, architecture. In some cases, a computer might store some or all of its program in memory that is kept separate from the data it operates on. This is called the Harvard architecture after the Harvard Mark I computer. Modern von Neumann computers display some traits of the Harvard architecture in their designs, such as in CPU caches. While it is possible to write computer programs as long lists of numbers (machine language) and while this technique was used with many early computers, it is extremely tedious and potentially error-prone to do so in practice, especially for complicated programs. Instead, each basic instruction can be given a short name that is indicative of its function and easy to remember – a mnemonic such as ADD, SUB, MULT or JUMP. These mnemonics are collectively known as a computer's assembly language. Converting programs written in assembly language into something the computer can actually understand (machine language) is usually done by a computer program called an assembler.    Programming language   Programming languages provide various ways of specifying programs for computers to run. Unlike natural languages, programming languages are designed to permit no ambiguity and to be concise. They are purely written languages and are often difficult to read aloud. They are generally either translated into machine code by a compiler or an assembler before being run, or translated directly at run time by an interpreter. Sometimes programs are executed by a hybrid method of the two techniques.   = Low-level languages =  Machine languages and the assembly languages that represent them (collectively termed low-level programming languages) tend to be unique to a particular type of computer. For instance, an ARM architecture computer (such as may be found in a PDA or a hand-held videogame) cannot understand the machine language of an Intel Pentium or the AMD Athlon 64 computer that might be in a PC.   = High-level languages/Third Generation Language =  Though considerably easier than in machine language, writing long programs in assembly language is often difficult and is also error prone. Therefore, most practical programs are written in more abstract high-level programming languages that are able to express the needs of the programmer more conveniently (and thereby help reduce programmer error). High level languages are usually \"compiled\" into machine language (or sometimes into assembly language and then into machine language) using another computer program called a compiler. High level languages are less related to the workings of the target computer than assembly language, and more related to the language and structure of the problem(s) to be solved by the final program. It is therefore often possible to use different compilers to translate the same high level language program into the machine language of many different types of computer. This is part of the means by which software like video games may be made available for different computer architectures such as personal computers and various video game consoles.    Fourth Generation Languages  These 4G languages are less procedural than 3G languages. The benefit of 4GL is that it provides ways to obtain information without requiring the direct help of a programmer. Example of 4GL is SQL.    Program design  Program design of small programs is relatively simple and involves the analysis of the problem, collection of inputs, using the programming constructs within languages, devising or using established procedures and algorithms, providing data for output devices and solutions to the problem as applicable. As problems become larger and more complex, features such as subprograms, modules, formal documentation, and new paradigms such as object-oriented programming are encountered. Large programs involving thousands of line of code and more require formal software methodologies. The task of developing large software systems presents a significant intellectual challenge. Producing software with an acceptably high reliability within a predictable schedule and budget has historically been difficult; the academic and professional discipline of software engineering concentrates specifically on this challenge.    Bugs   Errors in computer programs are called \"bugs\". They may be benign and not affect the usefulness of the program, or have only subtle effects. But in some cases, they may cause the program or the entire system to \"hang\", becoming unresponsive to input such as mouse clicks or keystrokes, to completely fail, or to crash. Otherwise benign bugs may sometimes be harnessed for malicious intent by an unscrupulous user writing an exploit, code designed to take advantage of a bug and disrupt a computer's proper execution. Bugs are usually not the fault of the computer. Since computers merely execute the instructions they are given, bugs are nearly always the result of programmer error or an oversight made in the program's design. Admiral Grace Hopper, an American computer scientist and developer of the first compiler, is credited for having first used the term \"bugs\" in computing after a dead moth was found shorting a relay in the Harvard Mark II computer in September 1947.    Components   A general purpose computer has four main components: the arithmetic logic unit (ALU), the control unit, the memory, and the input and output devices (collectively termed I/O). These parts are interconnected by buses, often made of groups of wires. Inside each of these parts are thousands to trillions of small electrical circuits which can be turned off or on by means of an electronic switch. Each circuit represents a bit (binary digit) of information so that when the circuit is on it represents a \"1\", and when off it represents a \"0\" (in positive logic representation). The circuits are arranged in logic gates so that one or more of the circuits may control the state of one or more of the other circuits.    Control unit   The control unit (often called a control system or central controller) manages the computer's various components; it reads and interprets (decodes) the program instructions, transforming them into control signals that activate other parts of the computer. Control systems in advanced computers may change the order of execution of some instructions to improve performance. A key component common to all CPUs is the program counter, a special memory cell (a register) that keeps track of which location in memory the next instruction is to be read from. The control system's function is as follows—note that this is a simplified description, and some of these steps may be performed concurrently or in a different order depending on the type of CPU: Read the code for the next instruction from the cell indicated by the program counter. Decode the numerical code for the instruction into a set of commands or signals for each of the other systems. Increment the program counter so it points to the next instruction. Read whatever data the instruction requires from cells in memory (or perhaps from an input device). The location of this required data is typically stored within the instruction code. Provide the necessary data to an ALU or register. If the instruction requires an ALU or specialized hardware to complete, instruct the hardware to perform the requested operation. Write the result from the ALU back to a memory location or to a register or perhaps an output device. Jump back to step (1). Since the program counter is (conceptually) just another set of memory cells, it can be changed by calculations done in the ALU. Adding 100 to the program counter would cause the next instruction to be read from a place 100 locations further down the program. Instructions that modify the program counter are often known as \"jumps\" and allow for loops (instructions that are repeated by the computer) and often conditional instruction execution (both examples of control flow). The sequence of operations that the control unit goes through to process an instruction is in itself like a short computer program, and indeed, in some more complex CPU designs, there is another yet smaller computer called a microsequencer, which runs a microcode program that causes all of these events to happen.    Central Processing unit (CPU)  The control unit, ALU, and registers are collectively known as a central processing unit (CPU). Early CPUs were composed of many separate components but since the mid-1970s CPUs have typically been constructed on a single integrated circuit called a microprocessor.    Arithmetic logic unit (ALU)   The ALU is capable of performing two classes of operations: arithmetic and logic. The set of arithmetic operations that a particular ALU supports may be limited to addition and subtraction, or might include multiplication, division, trigonometry functions such as sine, cosine, etc., and square roots. Some can only operate on whole numbers (integers) whilst others use floating point to represent real numbers, albeit with limited precision. However, any computer that is capable of performing just the simplest operations can be programmed to break down the more complex operations into simple steps that it can perform. Therefore, any computer can be programmed to perform any arithmetic operation—although it will take more time to do so if its ALU does not directly support the operation. An ALU may also compare numbers and return boolean truth values (true or false) depending on whether one is equal to, greater than or less than the other (\"is 64 greater than 65?\"). Logic operations involve Boolean logic: AND, OR, XOR, and NOT. These can be useful for creating complicated conditional statements and processing boolean logic. Superscalar computers may contain multiple ALUs, allowing them to process several instructions simultaneously. Graphics processors and computers with SIMD and MIMD features often contain ALUs that can perform arithmetic on vectors and matrices.    Memory   A computer's memory can be viewed as a list of cells into which numbers can be placed or read. Each cell has a numbered \"address\" and can store a single number. The computer can be instructed to \"put the number 123 into the cell numbered 1357\" or to \"add the number that is in cell 1357 to the number that is in cell 2468 and put the answer into cell 1595.\" The information stored in memory may represent practically anything. Letters, numbers, even computer instructions can be placed into memory with equal ease. Since the CPU does not differentiate between different types of information, it is the software's responsibility to give significance to what the memory sees as nothing but a series of numbers. In almost all modern computers, each memory cell is set up to store binary numbers in groups of eight bits (called a byte). Each byte is able to represent 256 different numbers (2^8 = 256); either from 0 to 255 or −128 to +127. To store larger numbers, several consecutive bytes may be used (typically, two, four or eight). When negative numbers are required, they are usually stored in two's complement notation. Other arrangements are possible, but are usually not seen outside of specialized applications or historical contexts. A computer can store any kind of information in memory if it can be represented numerically. Modern computers have billions or even trillions of bytes of memory. The CPU contains a special set of memory cells called registers that can be read and written to much more rapidly than the main memory area. There are typically between two and one hundred registers depending on the type of CPU. Registers are used for the most frequently needed data items to avoid having to access main memory every time data is needed. As data is constantly being worked on, reducing the need to access main memory (which is often slow compared to the ALU and control units) greatly increases the computer's speed. Computer main memory comes in two principal varieties: random-access memory or RAM read-only memory or ROM RAM can be read and written to anytime the CPU commands it, but ROM is preloaded with data and software that never changes, therefore the CPU can only read from it. ROM is typically used to store the computer's initial start-up instructions. In general, the contents of RAM are erased when the power to the computer is turned off, but ROM retains its data indefinitely. In a PC, the ROM contains a specialized program called the BIOS that orchestrates loading the computer's operating system from the hard disk drive into RAM whenever the computer is turned on or reset. In embedded computers, which frequently do not have disk drives, all of the required software may be stored in ROM. Software stored in ROM is often called firmware, because it is notionally more like hardware than software. Flash memory blurs the distinction between ROM and RAM, as it retains its data when turned off but is also rewritable. It is typically much slower than conventional ROM and RAM however, so its use is restricted to applications where high speed is unnecessary. In more sophisticated computers there may be one or more RAM cache memories, which are slower than registers but faster than main memory. Generally computers with this sort of cache are designed to move frequently needed data into the cache automatically, often without the need for any intervention on the programmer's part.    Input/output (I/O)   I/O is the means by which a computer exchanges information with the outside world. Devices that provide input or output to the computer are called peripherals. On a typical personal computer, peripherals include input devices like the keyboard and mouse, and output devices such as the display and printer. Hard disk drives, floppy disk drives and optical disc drives serve as both input and output devices. Computer networking is another form of I/O. I/O devices are often complex computers in their own right, with their own CPU and memory. A graphics processing unit might contain fifty or more tiny computers that perform the calculations necessary to display 3D graphics. Modern desktop computers contain many smaller computers that assist the main CPU in performing I/O.    Multitasking   While a computer may be viewed as running one gigantic program stored in its main memory, in some systems it is necessary to give the appearance of running several programs simultaneously. This is achieved by multitasking i.e. having the computer switch rapidly between running each program in turn. One means by which this is done is with a special signal called an interrupt, which can periodically cause the computer to stop executing instructions where it was and do something else instead. By remembering where it was executing prior to the interrupt, the computer can return to that task later. If several programs are running \"at the same time\". then the interrupt generator might be causing several hundred interrupts per second, causing a program switch each time. Since modern computers typically execute instructions several orders of magnitude faster than human perception, it may appear that many programs are running at the same time even though only one is ever executing in any given instant. This method of multitasking is sometimes termed \"time-sharing\" since each program is allocated a \"slice\" of time in turn. Before the era of cheap computers, the principal use for multitasking was to allow many people to share the same computer. Seemingly, multitasking would cause a computer that is switching between several programs to run more slowly, in direct proportion to the number of programs it is running, but most programs spend much of their time waiting for slow input/output devices to complete their tasks. If a program is waiting for the user to click on the mouse or press a key on the keyboard, then it will not take a \"time slice\" until the event it is waiting for has occurred. This frees up time for other programs to execute so that many programs may be run simultaneously without unacceptable speed loss.    Multiprocessing   Some computers are designed to distribute their work across several CPUs in a multiprocessing configuration, a technique once employed only in large and powerful machines such as supercomputers, mainframe computers and servers. Multiprocessor and multi-core (multiple CPUs on a single integrated circuit) personal and laptop computers are now widely available, and are being increasingly used in lower-end markets as a result. Supercomputers in particular often have highly unique architectures that differ significantly from the basic stored-program architecture and from general purpose computers. They often feature thousands of CPUs, customized high-speed interconnects, and specialized computing hardware. Such designs tend to be useful only for specialized tasks due to the large scale of program organization required to successfully utilize most of the available resources at once. Supercomputers usually see usage in large-scale simulation, graphics rendering, and cryptography applications, as well as with other so-called \"embarrassingly parallel\" tasks.    Networking and the Internet   Computers have been used to coordinate information between multiple locations since the 1950s. The U.S. military's SAGE system was the first large-scale example of such a system, which led to a number of special-purpose commercial systems such as Sabre. In the 1970s, computer engineers at research institutions throughout the United States began to link their computers together using telecommunications technology. The effort was funded by ARPA (now DARPA), and the computer network that resulted was called the ARPANET. The technologies that made the Arpanet possible spread and evolved. In time, the network spread beyond academic and military institutions and became known as the Internet. The emergence of networking involved a redefinition of the nature and boundaries of the computer. Computer operating systems and applications were modified to include the ability to define and access the resources of other computers on the network, such as peripheral devices, stored information, and the like, as extensions of the resources of an individual computer. Initially these facilities were available primarily to people working in high-tech environments, but in the 1990s the spread of applications like e-mail and the World Wide Web, combined with the development of cheap, fast networking technologies like Ethernet and ADSL saw computer networking become almost ubiquitous. In fact, the number of computers that are networked is growing phenomenally. A very large proportion of personal computers regularly connect to the Internet to communicate and receive information. \"Wireless\" networking, often utilizing mobile phone networks, has meant networking is becoming increasingly ubiquitous even in mobile computing environments.    Computer architecture paradigms  There are many types of computer architectures: Quantum computer vs. Chemical computer Scalar processor vs. Vector processor Non-Uniform Memory Access (NUMA) computers Register machine vs. Stack machine Harvard architecture vs. von Neumann architecture Cellular architecture Of all these abstract machines, a quantum computer holds the most promise for revolutionizing computing. Logic gates are a common abstraction which can apply to most of the above digital or analog paradigms. The ability to store and execute lists of instructions called programs makes computers extremely versatile, distinguishing them from calculators. The Church–Turing thesis is a mathematical statement of this versatility: any computer with a minimum capability (being Turing-complete) is, in principle, capable of performing the same tasks that any other computer can perform. Therefore, any type of computer (netbook, supercomputer, cellular automaton, etc.) is able to perform the same computational tasks, given enough time and storage capacity.    Misconceptions   A computer does not need to be electronic, nor even have a processor, nor RAM, nor even a hard disk. While popular usage of the word \"computer\" is synonymous with a personal electronic computer, the modern definition of a computer is literally: \"A device that computes, especially a programmable [usually] electronic machine that performs high-speed mathematical or logical operations or that assembles, stores, correlates, or otherwise processes information.\" Any device which processes information qualifies as a computer, especially if the processing is purposeful.    Unconventional computing   Historically, computers evolved from mechanical computers and eventually from vacuum tubes to transistors. However, conceptually computational systems as flexible as a personal computer can be built out of almost anything. For example, a computer can be made out of billiard balls (billiard ball computer); an often quoted example. More realistically, modern computers are made out of transistors made of photolithographed semiconductors.    Future  There is active research to make computers out of many promising new types of technology, such as optical computers, DNA computers, neural computers, and quantum computers. Most computers are universal, and are able to calculate any computable function, and are limited only by their memory capacity and operating speed. However different designs of computers can give very different performance for particular problems; for example quantum computers can potentially break some modern encryption algorithms (by quantum factoring) very quickly.    Further topics  Glossary of computers    Artificial intelligence  A computer will solve problems in exactly the way it is programmed to, without regard to efficiency, alternative solutions, possible shortcuts, or possible errors in the code. Computer programs that learn and adapt are part of the emerging field of artificial intelligence and machine learning.    Hardware   The term hardware covers all of those parts of a computer that are tangible objects. Circuits, displays, power supplies, cables, keyboards, printers and mice are all hardware.    History of computing hardware     Other hardware topics     Software   Software refers to parts of the computer which do not have a material form, such as programs, data, protocols, etc. When software is stored in hardware that cannot easily be modified (such as BIOS ROM in an IBM PC compatible), it is sometimes called \"firmware\".    Languages  There are thousands of different programming languages—some intended to be general purpose, others useful only for highly specialized applications.    Firmware  Firmware is the technology which has the combination of both hardware and software such as BIOS chip inside a computer. This chip (hardware) is located on the motherboard and has the BIOS set up (software) stored in it.    Liveware  At times the user working on the system are termed as Liveware.    Types of computers  Computers are typically classified based on their uses:    Based on uses  Analog computer Digital computer Hybrid computer    Based on sizes  Micro computer Personal computer Mini Computer Mainframe computer Super computer    Input Devices  When unprocessed data is sent to the computer with the help of input devices, the data is processed and sent to output devices. The input devices may be hand-operated or automated. The act of processing is mainly regulated by the CPU. Some examples of hand-operated input devices are: Overlay keyboard Trackball Joystick Digital camera Microphone Touchscreen Digital video Image scanner Graphics tablet Computer keyboard Mouse    Output Devices  The means through which computer gives output are known as output devices. Some examples of output devices are: Computer monitor Printer Projector Sound card PC speaker Video card    Professions and organizations  As the use of computers has spread throughout society, there are an increasing number of careers involving computers. The need for computers to work well together and to be able to exchange information has spawned the need for many standards organizations, clubs and societies of both a formal and informal nature.    See also     Notes     References     External links  Warhol & The Computer  Wikiversity has a quiz on this article","label":"foo"},{"text":"A major appliance, or domestic appliance, is a large machine used for routine housekeeping tasks such as cooking, washing laundry, or food preservation. An appliance is different from a plumbing fixture because it uses electricity or fuel. Major appliances differ from small appliances because they are bigger and not portable. They are often considered fixtures and part of real estate and as such they are often supplied to tenants as part of otherwise unfurnished rental properties. Major appliances may have special electrical connections, connections to gas supplies, or special plumbing and ventilation arrangements that may be permanently connected to the appliance. This limits where they can be placed in a home. Many major appliances are made of enamel-coated sheet steel which, in the middle 20th century, was usually white. The term white goods or whiteware, in contrast to brown goods, is also used, primarily where British English is spoken, although definitions for the term \"white goods\" can differ. In the United States, the term white goods more commonly refers to linens rather than appliances. Since major appliances in a home consume a significant amount of energy, they have become the objectives of programs to improve their energy efficiency in many countries. Energy efficiency improvements may require changes in construction of the appliances, or improved control systems.    Brands   In the early days of electrification, many major consumer appliances were made by the same companies that made the generation and distribution equipment. While some of these brand names persist to the present day, even if only as licensed use of old popular brand names, today many major appliances are manufactured by companies or divisions of companies that specialize particular appliances.    Types   Major appliances may be roughly divided as follows: Refrigeration equipment Freezer Refrigerator Water cooler  Stoves Cooker, also known as range, stove, oven, cooking plate, or cooktop Microwave oven  Washing equipment Washing machine Clothes dryer Drying cabinet Dishwasher  Miscellaneous Air conditioner Water heater    See also     References     External links   The dictionary definition of appliance at Wiktionary The European Committee of Domestic Equipment Manufacturers - CECED Energy Star Appliances","label":"foo"}]